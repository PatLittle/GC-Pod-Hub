<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Dev News</title><link>https://konrad.website/liveboat-github-runner/</link><description></description><item><title>Why did you choose the distro you use now?</title><link>https://www.reddit.com/r/linux/comments/1incot7/why_did_you_choose_the_distro_you_use_now/</link><author>/u/RadMarioBuddy45</author><category>dev</category><category>reddit</category><pubDate>Tue, 11 Feb 2025 23:24:17 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[I personally chose Linux Mint because most things work out of the box. All you need to do is remove the bloatware (optional), personalize everything, install all your apps, then you're all set. There's other factors involved, but they aren't significant enough to include here. Why did you choose the distro you use now?]]></content:encoded></item><item><title>I have made my first golang CLI</title><link>https://www.reddit.com/r/golang/comments/1inb0md/i_have_made_my_first_golang_cli/</link><author>/u/Halabito8</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Tue, 11 Feb 2025 22:12:05 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I built a CLI tool in Go to help me stay on top of my ClickUp tasks. I’ve got a bunch of lists, and it’s a pain to figure out what to tackle first since ClickUp doesn’t let me see everything in one place easily.This tool pulls all my to-dos and in-progress tasks, then generates two reports:A single merged list sorted by priority and due date.Tasks grouped by their respective lists.Now, I can quickly see what’s due today and what actually matters. It’s just a reporting tool for now, but it’s already making my workflow way smoother.]]></content:encoded></item><item><title>Tech&apos;s Dumbest Mistake: Why Firing Programmers for AI Will Destroy Everything</title><link>https://defragzone.substack.com/p/techs-dumbest-mistake-why-firing</link><author>/u/tapvt</author><category>dev</category><category>reddit</category><pubDate>Tue, 11 Feb 2025 21:40:44 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>NOAA&apos;s public weather data powers the local forecasts on your phone and TV</title><link>https://theconversation.com/noaas-vast-public-weather-data-powers-the-local-forecasts-on-your-phone-and-tv-a-private-company-alone-couldnt-match-it-249451</link><author>rntn</author><category>dev</category><category>hn</category><pubDate>Tue, 11 Feb 2025 21:31:43 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[When a hurricane or tornado starts to form, your local weather forecasters can quickly pull up maps tracking its movement and showing where it’s headed. But have you ever wondered where they get all that information?The forecasts can seem effortless, but behind the scenes, a vast network of satellites, airplanes, radar, computer models and weather analysts are providing access to the latest data – and warnings when necessary. This data comes from analysts at the National Oceanic and Atmospheric Administration, known as NOAA, and its National Weather Service.Atmospheric scientists Christine Wiedinmyer and Kari Bowen, who is a former National Weather Service forecaster, explained NOAA’s central role in most U.S. weather forecasts.When people see a weather report on TV, what went on at NOAA to make that forecast possible?All of that information goes into the agency’s computers, which process the data to begin defining what’s going on in different parts of the atmosphere. NOAA forecasters use computer models that simulate physics and the behavior of the atmosphere, along with their own experience and local knowledge, to start to paint a picture of the weather – what’s coming in a few minutes or hours or days. They also use that data to project seasonal conditions out over weeks or months.All of this analysis happens before the information reaches private weather apps and TV stations.No matter who you are, you can freely access that data and the analyses. In fact, a large number of private companies use NOAA data to create fancy maps and other weather products that they sell.It would be extremely difficult to do all of that without NOAA.The agency operates a fleet of 18 satellites that are packed with instruments dedicated to observing weather phenomena essential to predicting the weather, from how hot the land surface is to the water content of the atmosphere. Some are geostationary satellites which sit high above different parts of the U.S. measuring weather conditions 24/7. Others orbit the planet. Many of these are operated as part of partnerships with NASA or the Air Force.Some private companies are starting to invest in satellites, but it would take an enormous amount of money to replicate the range of instrumentation and coverage that NOAA has in place. Satellites only last so long and take time to build, so NOAA is continually planning for the future, and using its technical expertise to develop new instruments and computer algorithms to interpret the data.Maritime buoys are another measuring system that would be difficult to replicate. Over 1,300 buoys across oceans around the world measure water temperature, wind and wave height – all of which are essential for coastal warnings, as well as long-term forecasts.Weather observation has been around a long time. President Ulysses S. Grant created the first national weather service in the War Department in 1870. It became a civilian service in 1880 under the Department of Agriculture and is now in the Commerce Department. The information its scientists and technologists produce is essential for safety and also benefits people and industries in a lot of ways. Could a private company create forecasts on its own without NOAA data?It would be difficult for one company to provide comprehensive weather data in a reliable way that is also accessible to the entire public.Some companies might be able to launch their own satellite, but one satellite only gives you part of the picture. NOAA’s weather observation network has been around for a long time and collects data from points all over the U.S. and the oceans. Without that robust data, computer models and the broad network of forecasters and developers, forecasting also becomes less reliable.Analyzing that data is also complex. You’re not going to be able to take satellite data, run a model on a standard laptop and suddenly have a forecast. And there’s a question of whether a private company would want to take on the legal risk of being responsible for the nation’s forecasts and severe weather warnings. NOAA is taxpayer-funded, so it is a public good – its services provide safety and security for everyone, not just those who can pay for it.If weather data was only available at a price, one town might be able to afford the weather information necessary to protect its residents, while a smaller town or a rural area across the state might not. If you’re in a tornado-prone area or coastal zone, that information can be the difference between life or death.Is climate data and research into the changing climate important for forecasts?The Earth’s systems – its land, water and the atmosphere – are changing, and we have to be able to assess how those changes will impact weather tomorrow, in two weeks and far into the future.Rising global temperatures affect weather patterns. Dryness can fuel wildfires. Forecasts have to take the changing climate into account to be accurate, no matter who is creating the forecast.Drought is an example. The dryness of the Earth controls how much water gets exchanged with the atmosphere to form clouds and rainfall. To have an accurate weather prediction, we need to know how dry things are at the surface and how that has changed over time. That requires long-term climate information.NOAA doesn’t do all of this by itself – who else is involved?NOAA partners with private sector, academia, nonprofits and many others around the world to ensure that everyone has the best information to produce the most robust weather forecasts. Private weather companies and media also play important roles in getting those forecasts and alerts out more widely to the public.A lot of businesses rely on accuracy from NOAA’s weather data and forecasts: aviation, energy companies, insurance, even modern tractors’ precision farming equipment. The agency’s long-range forecasts are essential for managing state reservoirs to ensure enough water is saved and to avoid flooding.The government agency can be held accountable in a way private businesses are not because it answers to Congress. So, the data is trustworthy, accessible and developed with the goal to protect public safety and property for everyone. Could the same be said if only for-profit companies were producing that data?]]></content:encoded></item><item><title>The Future of KDE Themes - Introducing KDE Union and Plasma Next</title><link>https://tube.kockatoo.org/w/2rkFgbDViQVa4tjoXCrbF7</link><author>/u/Nova_496</author><category>dev</category><category>reddit</category><pubDate>Tue, 11 Feb 2025 21:06:46 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>GenAI Patterns: Query Rewriting</title><link>https://martinfowler.com/articles/gen-ai-patterns/#query-rewrite</link><author>Martin Fowler</author><category>dev</category><pubDate>Tue, 11 Feb 2025 20:58:00 +0000</pubDate><source url="https://martinfowler.com/feed.atom">Dev - Martin Fowler</source><content:encoded><![CDATA[Users often have difficulty writing the most effective queries.
       and I explain Query Rewriting:
      getting an LLM to formulate alternative queries to send to a RAG's
      retriever. ]]></content:encoded></item><item><title>Thomson Reuters wins first major AI copyright case in the US</title><link>https://www.wired.com/story/thomson-reuters-ai-copyright-lawsuit/</link><author>johnneville</author><category>dev</category><category>hn</category><pubDate>Tue, 11 Feb 2025 20:56:21 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Thomson Reuters has won the first major AI copyright case in the United States.In 2020, the media and technology conglomerate filed an unprecedented AI copyright lawsuit against the legal AI startup Ross Intelligence. In the complaint, Thomson Reuters claimed the AI firm reproduced materials from its legal research firm Westlaw. Today, a judge ruled in Thomson Reuters’ favor, finding that the company’s copyright was indeed infringed by Ross Intelligence’s actions.“None of Ross’s possible defenses holds water. I reject them all,” wrote US District Court of Delaware judge Stephanos Bibas, in a summary judgement.Thomson Reuters and Ross Intelligence did not immediately respond to requests for comment.The generative AI boom has led to a spate of additional legal fights about how AI companies can use copyrighted material, as many major AI tools were developed by training on copyrighted works including books, films, visual artwork, and websites. Right now, there are several dozen lawsuits currently winding through the US court system, as well as international challenges in China, Canada, the UK, and other countries.Notably, Judge Bibas ruled in Thomson Reuters’ favor on the question of fair use. The fair use doctrine is a key component of how AI companies are seeking to defend themselves against claims that they used copyrighted materials illegally. The idea underpinning fair use is that sometimes it’s legally permissible to use copyrighted works without permission—for example, to create parody works, or in noncommercial research or news production. When determining whether fair use applies, courts use a four-factor test, looking at the reason behind the work, the nature of the work (whether it’s poetry, nonfiction, private letters, et cetera), the amount of copyrighted work used, and how the use impacts the market value of the original. Thomson Reuters prevailed on two of the four factors, but Bibas described the fourth as the most important, and ruled that Ross “meant to compete with Westlaw by developing a market substitute.”Thomson Reuters spokesperson Jeffrey McCoy applauded the ruling in a statement emailed to WIRED. “We are pleased that the court granted summary judgment in our favor and concluded that Westlaw’s editorial content created and maintained by our attorney editors, is protected by copyright and cannot be used without our consent,” he wrote. “The copying of our content was not ‘fair use.’”Even before this ruling, Ross Intelligence had already felt the impact of the court battle: The startup shut down in 2021, citing the cost of litigation. In contrast, many of the AI companies still duking it out in court, like OpenAI and Google, are financially equipped to weather prolonged legal fights.Still, this ruling is a blow to AI companies, according to Cornell University professor of digital and internet law James Grimmelmann: “If this decision is followed elsewhere, it's really bad for the generative AI companies.” Grimmelmann believes that Bibas’ judgement suggests that much of the case law that generative AI companies are citing to argue fair use is “irrelevant.”Chris Mammen, a partner at Womble Bond Dickinson who focuses on intellectual property law, concurs that this will complicate AI companies’ fair use arguments, although it could vary from plaintiff to plaintiff. “It puts a finger on the scale towards holding that fair use doesn’t apply,” he says.Update 2/11/25 5:09 ET: This story has been updated to include additional comment from Thomson Reuters.]]></content:encoded></item><item><title>The first yearly drop in average CPU performance in its 20 years of benchmarks</title><link>https://www.tomshardware.com/pc-components/cpus/passmark-sees-the-first-yearly-drop-in-average-cpu-performance-in-its-20-years-of-benchmark-results</link><author>LorenDB</author><category>dev</category><category>hn</category><pubDate>Tue, 11 Feb 2025 20:00:13 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Benchmarking software developer PassMark publishes the average results of all Windows PC tests across the globe every two weeks in a line graph. In line with what many enthusiasts might expect, the PassMark graph has always shown a consistent increase in processor performance year-on-year. However, for the first time since the company started keeping track in 2004, the average CPU mark score for desktop and laptop processors has dropped, with laptops dropping 3.4% year-over-year.We see the biggest drop in laptop CPU performance results. PassMark recorded an average result of 14,632 across 101,316 samples last year. But, in 2025, the average score sat at an average of 14,130 points between 25,541 samples, decreasing the average score by 3.4%.The average desktop PC result in 2024 netted 26,436 points for 186,053 samples. But for 2025, the average score currently sits at 26,311 points for over 47,810 samples — a 0.5% drop from last year. While that drop is small, we should only see a continued progression of faster performance.We’ve also seen results for the top-performing CPUs, and it seems that we’ve basically reached a performance plateau, will little to no uplift in PassMark scores for the past three years for desktop chips and laptop CPUs. This happened after we received a massive uplift of 58.6% in the Top Desktop CPU benchmark scores in 2023, with the introduction of the AMD Ryzen Threadripper Pro 7995WX. The arrival of the AMD Ryzen 9 7945HX3D laptop CPU in the same year also delivered a 69.9% jump in average performance for laptops. However, a new desktop chip that will dethrone the Threadripper Pro 7995WX is yet to arrive, while Intel’s new Core Ultra 275HX has provided a measly 6.8% increase in performance points on mobile.What’s quite baffling, though, is that AMD, Intel, and even Qualcomm have just released new desktop and laptop CPUs that should have increased performance levels, according to their marketing. While it’s true that the AMD Ryzen 9000 series (except for the new X3D chips) and Intel Arrow Lake Core Ultra 200S processors were a disappointment for many enthusiasts, these should have at least delivered a modest increase in performance, particularly in productivity applications.Passmark itself mused on X (formerly Twitter) that it could be that people are switching to more affordable machines that deliver lower power and performance. Or maybe Windows 11 is depressing performance scores versus Windows 10, especially as people transition to it with the upcoming demise of the latter. We've certainly seen plenty of examples of reduced performance in gaming with some of the newer versions of Windows 11, particularly as Intel and AMD struggled to upstream needed updates into the OS.At the moment, we don’t know the cause of this drop, but it could also be that we’re just in the first quarter of 2025. As more people get newer gear and run tests for the remainder of the year, this number could climb and reflect the performance of newer chips that arrived recently, or will arrive in the coming months.PassMark also muses that bloatware could contribute to the sudden decline in performance, but that seems like a longshot. In the end, the decline could simply be due to an odd interaction between the benchmark itself and the latest versions of Windows 11.]]></content:encoded></item><item><title>DeepScaleR: Surpassing O1-Preview with a 1.5B Model by Scaling RL</title><link>https://pretty-radio-b75.notion.site/DeepScaleR-Surpassing-O1-Preview-with-a-1-5B-Model-by-Scaling-RL-19681902c1468005bed8ca303013a4e2</link><author>sijuntan</author><category>dev</category><category>hn</category><pubDate>Tue, 11 Feb 2025 19:59:00 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Introducing json_preprocessor</title><link>https://www.reddit.com/r/rust/comments/1in7afe/introducing_json_preprocessor/</link><author>/u/hajhawa</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Tue, 11 Feb 2025 19:38:54 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Do you ever feel like your json files need a bit more to make them interesting? Do you crave job security? Introducing json_preprocessor, sometimes shortened to jsonpp. It is a functional interpreted programming language that evaluates to JSON.This is not a real tool, it's a joke I spent a bit of time on, please for the love of god don't use it.]]></content:encoded></item><item><title>The subtle art of designing physical controls for cars</title><link>https://www.theturnsignalblog.com/the-subtle-art-of-designing-physical-control-for-cars/</link><author>vsdlrd</author><category>dev</category><category>hn</category><pubDate>Tue, 11 Feb 2025 19:15:02 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[I focused on the fundamental goal: making passengers comfortable with minimum interactions. I discovered thermal comfort depends on four environmental factors: air temperature, heat radiation, airflow, and humidity. More importantly, staying within a specific range of these factors creates a comfortable environment for most people. This makes it possible to rely heavily on automation.I created an automated system controlled by a temperature dial. This, in turn, determines the fan speed and seat heating. If the cabin temperature deviates significantly from the set temperature, the system will increase the fan speed and seat heating or cooling accordingly.This led me to create the first design concept that used a mixed physical/touch interface. While the automated system sets the fan speed and seat heating, the driver can always override this.For the second iteration, I decided to remove the seat heating from the automated system as this is a highly personal feature that is better controlled individually. I also introduced a dial design that would sit on top of the touch display so the touch-sensitive center is used to temporarily override the automated system:While the concept showed promise, it remained theoretical. So, it was time to pick it back up and build a prototype.My initial plan was to mount a dial onto a touch display. Having tested various implementations, I discovered this technology is not good enough. Registering accurate touch events is tricky, and it renders the space around the dial useless as it has to remain empty to prevent accidental touch interactions when rotating the dial. So, instead of bringing a dial to the display, I decided to bring the display to the dial.That's when I came across Scott Bezek's open-source Smart Knob project. He used a brushless DC motor to emulate an analog dial. By playing with the force and resistance of the motor, it's possible to create fake detents that are totally controlled by software. This means you can simulate different types of haptic feedback, like different detent strengths and hard stops. Additionally, it has a vibration motor that simulates a button press when pushing down on the dial. Combined with a small display, it creates a fully customizable physical control that can simulate almost any type of interaction with a physical dial.After gaining significant attention in the maker community, Seedlabs turned it into a pre-fabricated development kit. Included are a couple of examples that show off the capability of the device.It was the perfect starting point for my exploration, so I ordered one and got to work.A rotary dial like this may look simple from a design perspective. But there is so much to explore! The fact that you can control everything about the software and haptic feedback is a fantastic opportunity to figure out what types of interactions are possible.When you see any physical control, you automatically get certain expectations from its physical properties like size, shape, and weight. These 'affordances,' as they are called, tell you how the object can be used. For example, a round dial communicates that it can be rotated. A big dial communicates that it controls important functions requiring more precision, whereas a small one controls fewer, less essential functions. Similarly, signifiers, such as labels, can explain something about the function of the control, the number of steps, or the state.Similarly, the haptic feedback from rotating the dial is a communication layer. When a dial controls different settings, such as the media source, it often has hard detents to indicate the significance of the change. The detents are smaller if the dial controls different values within a single feature, such as volume.I did a couple of experiments to better understand these properties of rotary dials. For example, I tried implementing a simple volume control with three different haptic patterns:1. Smooth, linear resistanceThis felt lifeless and artificial, even though it was technically the most precise.2. Subtle detents at regular intervalsThis is the most common way to handle a control like volume; therefore, it felt the most natural.3. Dynamic resistance that increased in force with volume levelIncreasing the resistance proportionally to the volume is an interesting concept that is hard to create with an analog dial. It, therefore, feels unusual, especially for a typical control like volume, but it has potential in other application areas.Based on my experiments, I created a set of guidelines for designing haptic interfaces with rotary inputs:Design Guidelines for Haptic InterfacesKeep haptic patterns consistent across similar operationsMuch like a door handle should communicate whether to pull or push a door, the dial sets expectations in our minds of how it feels and how heavy the rotation is. The haptic feedback should match that, and the interaction between similar operations should not be mixed. If volume and fan speed have the same type of value range, do not change the interaction pattern. Don't use a continuous rotation for one and a stepped rotation for the other.Allow for both precise and rapid adjustmentsCertain features, like volume, allow two different behaviors. During regular usage, passengers adjust the volume in small steps to match their preferences. However, on occasion, the passengers need to mute volume quickly. Both options should be possible. Another type of function where this is common is one with an off state. In the example below, the first position of the dial is the off state. Rotating the dial fully to the left instinctively means turning the function off.Synchronize physical and visual feedbackMatching the digital interface to the physical rotation of the dial is crucial. This comes down to two things. The first is matching the total rotation of the dial to the interface. If the dial can rotate 270°, the interface must also be 270°. The second is to match the detent's position to the interface's position. This is trickier than it seems. For example, if the total range of the dial is 180° and there are 6 detents, each detent is 30°. You would expect the UI to update between each detent. However, this leads to the UI feeling out of sync. Therefore, it's better to delay updating the UI. In my experiments, the UI updates when the dial is 20% past the detent, giving a more natural feeling.Scale detent strength inversely with value rangeThe detents should be subtle if the data ranges from [0,99]. If the range is small, like [0,3], detents have to be stronger to clearly communicate the position of the dial.Place stronger detents at significant valuesTo allow for finer communication, it can help to differentiate between major and minor values through detent strength. It adds a new layer of communication to the haptic feedback that is easy to understand. For example, temperature values with a decimal value.Vary resistance and step size to indicate extreme valuesAnother way to add a layer of communication to the haptic feedback is by increasing the force for extreme values. This indicates that the consequences of this action are more intense than those of normal values. For example, in an automated climate mode, setting a temperature that varies significantly from the current cabin temperature will result in the system taking more serious action to achieve this temperature.This is especially helpful in situations where the extreme values are dynamic. In my automated climate system, the cabin temperature isn't always the same; therefore, the same set temperature can have different consequences depending on how close the cabin temperature is to the set temperature.Another way of achieving this is by increasing the number of degrees for an important step. For example, an 'off' mode can be larger than the steps for the on mode.Add subtle "preview" resistance before state changesThe curve of the force should not be linear but logarithmic. This way, resistance increases the closer you move to the detent. This makes it clear when precisely the step is triggered, and you can feel it when you get close to the next step.After establishing the design principles, I implemented the concepts I created previously. I created a fake automated system with all three functions: temperature, fan speed, and seat heating. In my previous article, I concluded that adding seat heating to the automated system wasn't suitable as it is a highly personal function. This remains true, but I wanted to include it in the dial to explore whether controlling three different functions through one dial is possible.For temperature control, I added progressive haptic resistance to communicate the magnitude of changes. The further you adjust from the current temperature, the more resistance you feel, as setting a significantly higher temperature will increase the fan speed and seat heating.Fan speed and seat heating received the same haptic profiles - fan speed uses five clear steps, whereas seat heating uses 4, with the first one being the "off" position with stronger feedback. By pressing down on the dial, you can cycle through the functions.I highlight the active function through a small paginator at the bottom of the display. However, it should also communicate the relationship between the functions in the automated system. When fan speed and seat heating change after setting a temperature, the driver should be aware of that without having to cycle through the functions. I explored this further in Figma:Being able to modify the angle, force, and detents of the dial through software is amazing and gave me a better understanding of the effect of different kinds of haptic feedback on usability. It's fun and insightful to update the different values until they are perfect.Showing three different data types in one dial is possible but definitely the maximum. When adding a fourth function, keeping track of your position in the interface without looking down becomes too difficult. One of the main challenges is the small display that needs to show a lot of information. Since I created an automated system in which all three data types are linked, it is challenging to communicate this link through a tiny display, especially if it's intended to be used while driving. It becomes a lot easier if there are only two functions — temperature and fan speed. This makes more sense conceptually, and there is now enough space in the interface to clearly communicate the status of the automated system.This setup is also my recommendation after all my experiments. A rotary dial with two functions is easy to understand and operate. Relying on an automated system keeps the number of interactions to a minimum, allowing the driver to override it easily if needed. Ideally, seat heating is also a physical control that enables passengers to set their preferred setting with one button press instead of cycling through the strength with multiple presses.Example of implementations todayI want to highlight two carmakers with similar interesting solutions. The first is Jaguar, who had a clever solution for a dial with three functions by adding a depth dimension.By default, the dial controls temperature. Pushing down on the dial activates the seat heating, and pulling up on the dial activates the fan speed. It's easy to learn and operate while keeping your eyes on the road. Unfortunately, like most carmakers, Jaguar has discontinued physical climate controls in favor of touch screens.The second carmaker is Skoda. They have an interesting concept with three 'Smart Dials' in current higher-end models. Each passenger has a dial to control the temperature, and by pressing down, they control the seat heating. The driver can configure the middle dial to control up to 6 different functions, such as volume, drive mode, fan speed, and air direction. It's a simple and superb design that deserves more praise, especially considering today's trend of touch interfaces.In my most popular article, I explain the rise of touch screens in cars. Touch screens are a necessary evil, especially for more complicated interactions such as navigation. However, frequent, simpler interactions like climate controls should not be in a touch interface.The often-cited reason for this is cost. However, surprisingly, it's often the budget brands like Skoda and Renault that offer physical controls today, showing that it's not simply a matter of cost, but of priority. Carmakers with touch-only interfaces prioritize cost and marketing over ergonomics and safety.There is an inherent satisfaction and quality in operating physical controls. For years, brands like Mercedes prided themselves on spending thousands of hours perfecting the feel of their switches and buttons. The feeling of operating a physical control gives the car an inherent quality and character. This feeling is gone with touch screens and, therefore, in most modern vehicles.I want to see more carmakers bring back physical controls and consider them a prominent part of the in-car experience. As I've demonstrated with this project, there is a lot to explore when designing physical controls, and I hope my project will inspire others to do the same. You can find the Seedlabs developer kit here and I have made my code public on my GitHub.Get notified of new postsDon't worry, I won't spam you with anything else]]></content:encoded></item><item><title>Elon Musk attempts hostile takeover of OpenAI…</title><link>https://www.youtube.com/watch?v=tPZauAYgVRQ</link><author>Fireship</author><category>dev</category><enclosure url="https://www.youtube.com/v/tPZauAYgVRQ?version=3" length="" type=""/><pubDate>Tue, 11 Feb 2025 18:58:36 +0000</pubDate><source url="https://www.youtube.com/channel/UCsBjURrPoezykLs9EqgamOA">Dev - Fireship</source><content:encoded><![CDATA[Elon Musk has launched a hostile takeover bid to take control over the non-profit assets of OpenAI. Let's look into the details of OpenAI's corporate structure and the likelihood Elon's takeover is successful. 

#tech #ai #thecodereport

💬 Chat with Me on Discord

https://discord.gg/fireship

🔗 Resources

OpenAI Stargate https://youtu.be/YrHsw4Oja7w
o1 First look https://youtu.be/Sf4WqHBCYSY

🔥 Get More Content - Upgrade to PRO

Upgrade at https://fireship.io/pro
Use code YT25 for 25% off PRO access 

🎨 My Editor Settings

- Atom One Dark 
- vscode-icons
- Fira Code Font

🔖 Topics Covered

- Why is Elon trying to buy OpenAI?
- Latest trends in artificial Intelligence
- Elon Musk vs Sam Altman Feud Explained]]></content:encoded></item><item><title>Go 1.24 is released (self.golang)</title><link>https://www.reddit.com/r/golang/comments/1in5nuw/go_124_is_released_selfgolang/</link><author>/u/rtuidrvsbrdiusbrvjdf</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Tue, 11 Feb 2025 18:33:47 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[   submitted by    /u/rtuidrvsbrdiusbrvjdf ]]></content:encoded></item><item><title>Japan can be a science heavyweight once more if it rethinks funding</title><link>https://www.nature.com/articles/d41586-025-00394-8</link><author>rntn</author><category>dev</category><category>hn</category><pubDate>Tue, 11 Feb 2025 18:30:13 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[From CRISPR gene editing to protein-structure predictions driven by artificial intelligence, great innovations stem from interdisciplinary research. Solutions to climate change, biodiversity loss, health inequities and other global crises will also rely on insights that bridge many fields.Yet interdisciplinary research is still sidelined in many countries. Even though these papers attract more citations and have more impact on research, interdisciplinary research proposals tend to be less likely to receive funding than are those with a narrower scope.Some countries have adjusted their research funding strategies accordingly. For example, between 2016 and 2018, UK research councils awarded 30% more grants to interdisciplinary investigators than they had a decade earlier, with 44% of funded projects in 2018 spanning at least two research subjects. A similar move has been made in the United States: between 2015 and 2020, university departments that submitted some interdisciplinary grant proposals received almost five times more funding — in terms of total and individual awards — from the National Institutes of Health (NIH) and National Science Foundation (NSF) than did those that submitted in only one discipline.Alas, this isn’t the case in Japan. The nation’s funding agencies still mostly support research in tight disciplinary boundaries, such as engineering or chemistry. Specialists who evaluate these grant proposals tend to favour work in fields that they are familiar with over interdisciplinary studies they do not understand well.This narrow approach is leading to substantial underfunding of interdisciplinary research in Japan. It also means that the nation is missing out on breakthroughs. Japan’s natural resources are limited, and its economy has long relied on science and technology. The decline in its research and innovation is unmistakable, as indicated by the country’s share of the world’s top 10% of most-cited research articles, which has dropped from 6% to 2% over the past two decades or so.As scientists and engineers based in Japanese research institutions, we urge the government and funding agencies to do more to support interdisciplinary research — or risk eroding the country’s global standing in science and its economy. Here we highlight five directions to foster such a shift.Fund people, not projectsWe argue that Japanese funding agencies should pivot from funding projects to supporting talented researchers. Such a model has proven to be effective in leading institutions worldwide. For example, the Howard Hughes Medical Institute’s (HHMI) Investigator Program provides outstanding scientists with flexible, long-term funding to pursue high-impact biomedical research — some US$11 million over a seven-year term, which can be renewed. This approach has led to groundbreaking discoveries, including the mechanistic understanding of circadian rhythms, the computational prediction of protein folding and the discovery of RNA interference.Similarly, in Germany, the Max Planck Society prioritizes curiosity-driven research that has long-term objectives over immediate applications. By fostering interdisciplinary collaboration and providing institute directors with generous internal funding, free from the continual need to secure external grants, this strategy has enabled breakthroughs including the development of CRISPR–Cas9 as a gene-editing tool and structural insights into ribosomes.Japan does have some programmes to support talented researchers, but these are too timid. The Japan Science and Technology Agency (JST) and the Japan Society for the Promotion of Science (JSPS), for example, dedicate some funds to basic science and some to curiosity-driven research. But the JST adopts a top-down approach, which often favours research on trending topics and can miss more-original ideas or projects . And the annual budget for the JSPS’s main grant programme, the Grants-in-Aid for Scientific Research (KAKENHI), has remained stagnant for the past decade. Adjusted for inflation and the weakening yen, the average funding per project has halved since 2013.However, one Japanese institution has adopted researcher-focused funding with great success: the Okinawa Institute of Science and Technology Graduate University (OIST). Unlike national universities, which are funded by the Ministry of Education, Culture, Sports, Science and Technology, OIST is a private university that is funded by Japan’s Cabinet Office. Launched in 2011, it has a horizontal organizational structure and a strong commitment to diversity and interdisciplinary collaboration (see ‘Steps to success’). OIST is now ranked as Japan’s leading research institute by the Nature Index, and around 20% of its publications involve contributions across more than one discipline. More Japanese institutions should follow its lead.At the Okinawa Institute of Science and Technology Graduate University (OIST), faculty members receive core funding for five years — with flexibility in what to do with it. Unlike many other universities, in which laboratories must cover the costs of core facilities and technical staff, access to these resources at OIST is provided free of charge. This fosters collaboration and enables high-risk, high-reward projects. A review of each faculty member every five years ensures accountability, with continued support granted only to those who demonstrate significant achievements.OIST also operates without conventional departments, helping scientists from different fields to collaborate. The buildings host open spaces, lounges, shared kitchens and open desk areas that encourage casual interactions. PhD students must also rotate through multiple labs during their first year to widen their horizons. These rotations facilitate the exchange of ideas and technologies, embedding interdisciplinary collaboration into the fabric of the institution.Although further progress is needed, including in terms of gender and disability, OIST fosters a greater diversity in its community than do other Japanese institutions. This is partly because it attracts scholars from around the world. In 2024, more than 60% of faculty members and 80% of students at OIST were not Japanese citizens, bringing a dynamic mix of perspectives and expertise.Embrace high-stakes projectsInterdisciplinary research is often perceived as riskier than more conventional, single-discipline work. This might be because it can be hard to learn concepts and methods, and even communicate efficiently, across disciplines. And priorities between fields can conflict.To overcome these challenges, Japanese funding agencies should consider adopting a ‘high risk, high impact’ funding model, similar to that of the US Defense Advanced Research Projects Agency (DARPA), which anticipates a success rate of just 50%. They should recognize that even ‘unsuccessful’ high-risk projects can generate valuable knowledge, contributing to broader scientific and technological advancements. The DARPA funding model has inspired others, such as the Advanced Research Projects Agency for Health (ARPA-H), launched in the NIH to drive biomedical breakthroughs.Another example is the Chan Zuckerberg Biohub in California, which supports interdisciplinary teams comprising biologists, engineers and data scientists to work on ambitious goals, such as the eradication of infectious diseases. It provides substantial funding over several years to enable teams to pivot research directions in response to emerging challenges. Success is assessed through regular milestone-based evaluations. Projects that show limited potential are promptly terminated, ensuring that resources can be reallocated to more-promising endeavours.In Japan, we are not advocating for existing funds to be redistributed. Rather, Japanese policymakers should allocate money to high-risk, high-reward projects from a separate pot, managed through dedicated DARPA-like programmes.Initiatives such as the JST’s Diversity and Inclusiveness programme aim to lessen the homogeneity of the research workforce — for example, through awards and networking opportunities for women in science and by providing support or extensions for life events as well as for childcare and caregiving commitments — which tend to fall mostly on women (see page 295). This is laudable, but these efforts should be more widespread: they should be extended across genders and also include funding agency officers, programme managers and reviewers.Japan’s funding agencies should also ensure that their own decision makers are less homogeneous in terms of their discipline, background, gender, age, nationality and culture. This would help to favour interdisciplinary research, which by nature weaves different approaches together and can benefit from being planned, conducted and assessed by a diverse community.The role of immigration in boosting diversity should also be recognized.]]></content:encoded></item><item><title>Undergraduate Upends a 40-Year-Old Data Science Conjecture</title><link>https://www.quantamagazine.org/undergraduate-upends-a-40-year-old-data-science-conjecture-20250210/</link><author>/u/Stackitu</author><category>dev</category><category>reddit</category><pubDate>Tue, 11 Feb 2025 18:26:37 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Together, Krapivin (now a graduate student at the University of Cambridge), Farach-Colton (now at New York University) and Kuszmaul demonstrated in a January 2025 paper that this new hash table can indeed find elements faster than was considered possible. ln so doing, they had disproved a conjecture long held to be true.“It’s an important paper,” said Alex Conway of Cornell Tech in New York City. “Hash tables are among the oldest data structures we have. And they’re still one of the most efficient ways to store data.” Yet open questions remain about how they work, he said. “This paper answers a couple of them in surprising ways.”Hash tables have become ubiquitous in computing, partly because of their simplicity and ease of use. They’re designed to allow users to do exactly three things: “query” (search for) an element, delete an element, or insert one into an empty slot. The first hash tables date back to the early 1950s, and computer scientists have studied and used them ever since. Among other things, researchers wanted to figure out the speed limits for some of these operations. How fast, for example, could a new search or insertion possibly be?The answer generally depends on the amount of time it takes to find an empty spot in a hash table. This, in turn, typically depends on how full the hash table is. Fullness can be described in terms of an overall percentage — this table is 50% full, that one’s 90% — but researchers often deal with much fuller tables. So instead, they may use a whole number, denoted by , to specify how close the hash table is to 100% full. If  is 100, then the table is 99% full. If  is 1,000, the table is 99.9% full. This measure of fullness offers a convenient way to evaluate how long it should take to perform actions like queries or insertions.Researchers have long known that for certain common hash tables, the expected time required to make the worst possible insertion — putting an item into, say, the last remaining open spot — is proportional to . “If your hash table is 99% full,” Kuszmaul said, “it makes sense that you would have to look at around 100 different positions to find a free slot.”In a 1985 paper, the computer scientist Andrew Yao, who would go on to win the A.M. Turing Award, asserted that among hash tables with a specific set of properties, the best way to find an individual element or an empty spot is to just go through potential spots randomly — an approach known as uniform probing. He also stated that, in the worst-case scenario, where you’re searching for the last remaining open spot, you can never do better than . for 40 years, most computer scientists assumed that Yao’s conjecture was true.Krapivin was not held back by the conventional wisdom for the simple reason that he was unaware of it. “I did this without knowing about Yao’s conjecture,”  he said. His explorations with tiny pointers led to a new kind of hash table — one that did not rely on uniform probing. And for this new hash table, the time required for worst-case queries and insertions is proportional to (log ) — far faster than . This result directly contradicted Yao’s conjecture. Farach-Colton and Kuszmaul helped Krapivin show that (log ) is the optimal, unbeatable bound for the popular class of hash tables Yao had written about.“This result is beautiful in that it addresses and solves such a classic problem,” said Guy Blelloch of Carnegie Mellon.“It’s not just that they disproved [Yao’s conjecture], they also found the best possible answer to his question,” said Sepehr Assadi of the University of Waterloo.  “We could have gone another 40 years before we knew the right answer.”]]></content:encoded></item><item><title>Reviving the joy and honor of working with your hands (2015)</title><link>https://richmond.com/holmberg-reviving-the-joy-and-honor-of-working-with-your-hands-will-strengthen-our-nation/article_d8130166-855d-53b6-94e1-cb735edcd7cc.html</link><author>skadamat</author><category>dev</category><category>hn</category><pubDate>Tue, 11 Feb 2025 18:23:03 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Go 1.24.0 tagged</title><link>https://www.reddit.com/r/golang/comments/1in559i/go_1240_tagged/</link><author>/u/khnorgaard</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Tue, 11 Feb 2025 18:12:48 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[   submitted by    /u/khnorgaard ]]></content:encoded></item><item><title>Using Terraform to deploy an ML orchestration system in EKS in minutes</title><link>https://www.reddit.com/r/kubernetes/comments/1in4ad3/using_terraform_to_deploy_an_ml_orchestration/</link><author>/u/Old-Cartographer3050</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Tue, 11 Feb 2025 17:38:21 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[If you're looking to get started or migrate to an open source ML orchestration solution that integrates natively with Kubernetes, look no further.Flyte delivers a Python SDK that abstracts away the K8s inner workings but gives users easy access to compute resources (including accelerators), Secrets, and more; enabling reproducibility, versioning, and parallelism for complex ML workflows.We developed a reference implementation for EKS that's fully automated with Terraform/OpenTofu.(Disclaimer: I'm a Flyte maintainer)]]></content:encoded></item><item><title>Launch HN: A0.dev (YC W25) – React Native App Generator</title><link>https://news.ycombinator.com/item?id=43015267</link><author>sethburster</author><category>dev</category><category>hn</category><pubDate>Tue, 11 Feb 2025 17:08:35 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Hi HN — we’re Seth and Ayo and we’re building a0.dev (https://a0.dev). a0.dev is a platform built to cut down React Native development time from weeks to just a few hours.We’ve been building mobile apps together for seven years and have had several successes. One thing that’s always bothered us though is how much harder it is to build a mobile app than a website. If you’ve built an app with React Native before you’re familiar with the pains of working with both Xcode and Android Studio, writing tons of boilerplate before you can even start making the app, setting up state management, and of course going through the dreaded app review. We decided to build a platform that would make the app development process faster.We’ve seen the success of new code-gen platforms like v0 and wanted something for React Native that goes further. We built an AI app generator that takes a user's prompt and creates a custom React Native app with an instant live preview.a0.dev is great for quickly prototyping components and screens in React Native and users are able to copy the code from our generator into their preferred development environment. Our landing page has a couple of example prompts, but we encourage you to get creative when trying it out. We’ve had success generating not only functional screens like an Instagram Feed but also 2d games like Minesweeper or Flappy Bird.Our chat has a “UI Expert” and “Advanced Logic” model users can switch between depending on the task at hand. Users can upgrade from working on a single screen to creating a full app by clicking on the “Need a Full App” button in the top right of the page. This changes the scope from a standalone chat with a single file to a full project that can include multiple chats and files. We launched an IOS app that users can download in order to preview the app on a physical device. We find that many apps look and feel better on a physical device so we recommend trying it out.Our goal is to continue to improve the app generator while adding more features to help developers get their apps to the app store and make money from those apps. The main features on our roadmap right now are a Supabase integration and a “one click submit” button to let developers publish their app to the App Store.There are a few limitations to note. We’re working on releasing our Android app, but Android users should be able to preview their app using the Expo Go App. The app is running React Native Web in the browser so any dependencies that don’t support web won’t work with the web preview but should work on the phone. There are also some dependencies that our system can’t handle because they require native modules that aren’t packaged into our app currently.We hope you guys will check it out and try making an app with a0.dev. We’re available on Discord around the clock to help developers with any problems they may face and want to guide people to actually releasing their app on the App Store. Let us know what features you’d like to see and any problems you’ve faced building apps, we’d love to hear about your experience.We dropped the need to sign up for the first message so you can just jump in and try it out.Looking forward to your thoughts!]]></content:encoded></item><item><title>LLMs can teach themselves to better predict the future</title><link>https://arxiv.org/abs/2502.05253</link><author>bturtel</author><category>dev</category><category>hn</category><pubDate>Tue, 11 Feb 2025 16:40:20 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>GNOME’s new main website has launched!</title><link>https://www.gnome.org/</link><author>/u/BrageFuglseth</author><category>dev</category><category>reddit</category><pubDate>Tue, 11 Feb 2025 16:30:04 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[A more elegant way to use your computerHit the  any time to get an overview, switch or launch apps,
         and search for anything on your computer. It's magic.]]></content:encoded></item><item><title>Intel&apos;s Battlemage Architecture</title><link>https://chipsandcheese.com/p/intels-battlemage-architecture</link><author>ksec</author><category>dev</category><category>hn</category><pubDate>Tue, 11 Feb 2025 16:00:59 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Intel’s Alchemist architecture gave the company a foot in the door to the high performance graphics segment. The Arc A770 proved to be a competent first effort, able to run many games with credible performance. Now, Intel is passing the torch to a new graphics architecture, named Battlemage.Like Alchemist, Battlemage targets the midrange segment. It doesn’t try to compete with AMD or Nvidia’s high end cards. While it’s not as flashy as Nvidia’s RTX 4090 or AMD’s RX 7900 XTX, midrange GPUs account for a much larger share of the discrete GPU market, thanks to their lower prices. Unfortunately, today’s midrange cards like the RTX 4060 and RX 7600 only come with 8 GB of VRAM, and are poor value. Intel takes advantage of this by launching the Arc B580 at $250, undercutting both competitors while offering 12 GB of VRAM.For B580 to be successful, its new Battlemage architecture has to execute well across a variety of graphics workloads. Intel has made numerous improvements over Alchemist, aiming to achieve better performance with less compute power and less memory bandwidth. I’ll be looking at the Arc B580, with comparison data from the A770 and A750, as well as scattered data I have lying around.Battlemage is organized much like its predecessor. Xe Cores continue to act as a basic building block. Four Xe Cores are grouped into a Render Slice, which also contains render backends, a rasterizer, and associated caches for those fixed function units. The entire GPU shares an 18 MB L2 cache.The Arc B580 overall is a smaller GPU than its outgoing Alchemist predecessors. B580 has five Render Slices to A770’s eight. In total, B580 has 2560 FP32 lanes to A770’s 4096.Battlemage launches with a smaller memory subsystem too. The B580 has a 192-bit GDDR6 bus running at 19 GT/s, giving it 456 GB/s of theoretical bandwidth. A770 has 560 GB/s of GDDR6 bandwidth, thanks to a 256-bit bus running at 17.5 GT/s.Even the host interface has been cut down. B580 only has a PCIe 4.0 x8 link, while A770 gets a full size x16 one. Intel’s new architecture has a lot of heavy lifting to do if it wants to beat a much larger implementation of its predecessor.Battlemage’s architectural changes start at its Xe Cores. The most substantial changes between the two generations actually debuted on Lunar Lake. Xe Cores are further split into XVEs, or Xe Vector engines. Intel merged pairs of Alchemist XVEs into ones that are twice as wide, completing a transition towards larger execution unit partitions. Xe Core throughput stays the same at 128 FP32 operations per cycle.A shared instruction cache feeds all eight XVEs in a Xe Core. Alchemist had a 96 KB instruction cache, and Battlemage almost certainly has an instruction cache at least as large. Instructions on Intel GPUs are generally 16 bytes long, with a 8 byte compacted form in some cases. A 96 KB instruction cache therefore has a nominal capacity of 6-12K instructions.XVEs form the smallest partition in Intel GPUs. Each XVE tracks up to eight threads, switching between them to hide latency and keep its execution units fed. A 64 KB register file stores thread state, giving each thread up to 8 KB of registers while maintaining maximum occupancy. Giving a register count for Intel GPUs doesn’t really work, because Intel GPU instructions can address the register file with far more flexibility than Nvidia or AMD architectures. Each instruction can specify a vector width, and access a register as small as a single scalar element.Meteor Lake articleBattlemage’s divergence behavior by comparison is intuitive and straightforward. SIMD16 achieves full utilization if groups of 16 threads go the same way. The same applies for SIMD32 and groups of 32 coherent threads. Thus Battlemage is actually more agile than its predecessor when dealing with divergent branches, while enjoying the efficiency advantage of using larger vectors.Like Alchemist, Battlemage executes most math operations down two ports (ALU0, ALU1). ALU0 handles basic FP32 and FP16 operations, while ALU1 handles integer math and less common instructions. Intel’s port layout has parallels to Nvidia’s Turing, which also splits dispatch bandwidth between 16-wide FP32 and INT32 units. A key difference is that Turing uses fixed 32-wide vectors, and keeps both units occupied by feeding them on alternate cycles. Intel can issue instructions of the same type back-to-back, and can select multiple instructions to issue per cycle to different ports.In another similarity to Turing, Battlemage carries forward Alchemist’s “XMX” matrix multiplication units. Intel claims 3-way co-issue, implying XMX is on a separate port. However, VTune only shows multiple pipe active metrics for ALU0+ALU1 and ALU0+XMX. I’ve drawn XMX as a separate port above, but the XMX units could be on ALU1.Gaming workloads tend to use more floating point operations. During compute heavy sections, ALU1 offloads other operations and keeps ALU0 free to deal with floating point math. XeSS exercises the XMX unit, with minimal co-issue alongside vector operations. A generative AI workload shows even less XMX+vector co-issue.As expected for any specialized execution unit, XMX software support is far from guaranteed. Running AI image generation or language models using other frameworks heavily exercises B580’s regular vector units, while leaving the XMX units idle.In microbenchmarks, Intel’s older A770 and A750 can often use their larger shader arrays to achieve higher compute throughput than B580. However, B580 behaves more consistently. Alchemist had trouble with FP32 FMA operations. Battlemage in contrast has no problem getting right up to its theoretical throughput. FP32+INT32 dual issue doesn’t happen perfectly on Battlemage, but it barely happened at all on A750.Each XVE also has a branch port for control flow instructions, and a “send” port that lets the XVE talk with the outside world. Load on these ports is typically low, because GPU programs don’t branch as often as CPU ones, and shared functions accessed through the “send” port won’t have enough throughput to handle all XVEs hitting it at the same time.Battlemage’s memory subsystem has a lot in common with Alchemist’s, and traces its origins to Intel’s integrated graphics architectures over the past decade. XVEs access the memory hierarchy by sending a message to the appropriate shared functional unit. At one point, the entire iGPU was basically the equivalent of a Xe Core, with XVE equivalents acting as basic building blocks. XVEs would access the iGPU’s texture units, caches, and work distribution hardware over a messaging fabric. Intel has since built larger subdivisions, but the terminology remains.Each Xe Core has eight TMUs, or texture samplers in Intel terminology. The samplers have a 32 KB texture cache, and can return 128 bytes/cycle to the XVEs. Battlemage is no different from Alchemist in this respect. But the B580 has less texture bandwidth on tap than its predecessor. Its higher clock speed isn’t enough to compensate for having far fewer Xe Cores.B580 runs at higher clock speeds, which brings down texture cache hit latency too. In clock cycle terms though, Battlemage has nearly identical texture cache hit latency to its predecessor. L2 latency has improved significantly, so missing the texture cache isn’t as bad on Battlemage.Global memory accesses are first cached in a 256 KB block, which serves double duty as Shared Local Memory (SLM). It’s larger than Alchemist and Lunar Lake’s 192 KB L1/SLM block, so Intel has found the transistor budget to keep more data closer to the execution units. Like Lunar Lake, B580 favors SLM over L1 capacity even when a compute kernel doesn’t allocate local memory.Intel may be able to split the L1/SLM block in another way, but a latency test shows exactly the same result regardless of whether I allocate local memory. Testing with Nemes’s Vulkan test suite also shows 96 KB of L1.Global memory access on Battlemage offers lower latency than texture accesses, even though the XVEs have to handle array address generation. With texture accesses, the TMUs do all the address calculations. All the XVEs do is send them a message. L1 data cache latency is similar to Alchemist in clock cycle terms, though again higher clock speeds give B580 an actual latency advantage.Battlemage gets a clock cycle latency reduction too with scalar memory accesses. Intel does not have separate scalar instructions like AMD. But Intel’s GPU ISA lets each instruction specify its SIMD width, and SIMD1 instructions are possible. Intel’s compiler has been carrying out scalar optimizations and opportunistically generating SIMD1 instructions well before Battlemage, but there was no performance difference as far as I could tell. Now there is.On B580, L1 latency for a SIMD1 (scalar) access is about 15 cycles faster than a SIMD16 access. SIMD32 accesses take one extra cycle when microbenchmarking, though that’s because the compiler generates two sets of SIMD16 instructions to calculate addresses across 32 lanes. I also got Intel’s compiler to emit scalar INT32 adds, but those didn’t see improved latency over vector ones. Therefore, the scalar latency improvements almost certainly come from an optimized memory pipeline.SIMD1 instructions also help within the XVEs. Intel doesn’t use a separate scalar register file, but can more flexibly address their vector register file than AMD or Nvidia. Instructions can access individual elements (sub-registers) and read out whatever vector width they want. Intel’s compiler could pack many “scalar registers” into the equivalent of a vector register, economizing register file capacity.L1 can deliver 512 bytes per cycleEven if the L1 can only provide 256 bytes per cycle, that still gives Intel’s Xe Core as much L1 bandwidth as an AMD RDNA WGP, and twice as much L1 bandwidth as an Nvidia Ampere SM. 512 bytes per cycle would let each XVE complete a SIMD16 load every cycle, which is kind of overkill anyway.Battlemage uses the same 256 KB block for L1 cache and SLM. SLM provides an address space local to a group of threads, and acts as a fast software managed scratchpad. In OpenCL, that’s exposed via the local memory type. Everyone likes to call it something different, but for this article I’ll use OpenCL and Intel’s term.Even though both local memory and L1 cache hits are backed by the same physical storage, SLM accesses enjoy better latency. Unlike cache hits, SLM accesses don’t need tag checks or address translation. Accessing Battlemage’s 256 KB block of memory in SLM mode brings latency down to just over 15 ns. It’s faster than doing the same on Alchemist, and is very competitive against recent GPUs from AMD and Nvidia.Alchemist and Battlemage both appear to have 32 atomic ALUs attached to each Xe Core’s SLM unit, much like AMD’s RDNA and Nvidia’s Pascal. Meteor Lake’s Xe-LPG architecture may have half as many atomic ALUs per Xe Core.Battlemage has a two level cache hierarchy like its predecessor and Nvidia’s current GPUs. B580’s 18 MB L2 is slightly larger than A770’s 16 MB L2. A770 divided its L2 into 32 banks, each capable of handling a 64 byte access every cycle. At 2.4 GHz, that’s good for nearly 5 TB/s of bandwidth.Intel didn’t disclose B580’s L2 topology, but a reasonable assumption is that Intel increased bank size from 512 to 768 KB, keeping 4 L2 banks tied to each memory controller. If so, B580’s L2 would have 24 banks and 4.3 TB/s of theoretical bandwidth at 2.85 GHz. Microbenchmarking using Nemes’s Vulkan test gets a decent proportion of that bandwidth. Efficiency is much lower on the older A750, which gets approximately as much bandwidth as B580 despite probably having more theoretical L2 bandwidth on tap.Besides insulating the execution units from slow VRAM, the L2 can act as a point of coherency across the GPU. B580 is pretty fast when bouncing data between threads using global memory, and is faster than its predecessor.With atomic add operations on global memory, Battlemage does fine for a GPU of its size and massively outperforms its predecessor.I’m using INT32 operations, so 86.74 GOPS on the A750 would correspond to 351 GB/s of L2 bandwidth. On the B580, 220.97 GOPS would require 883.9 GB/s. VTune however reports far higher L2 bandwidth on A750. Somehow, A750 sees 1.37 TB/s of L2 bandwidth during the test, or nearly 4x more than it should need.Meteor Lake’s iGPU is a close relative of Alchemist, but its ratio of global atomic add throughput to Xe Core count is similar to Battlemage’s. VTune reports Meteor Lake’s iGPU using more L2 bandwidth than required, but only by a factor of 2x. Curiously, it also shows the expected bandwidth coming off the XVEs. I wonder if something in Intel’s cross-GPU interconnect didn’t scale well with bigger GPUs.With Battlemage, atomics are broken out into a separate category and aren’t reported as regular L2 bandwidth. VTune indicates atomics are passed through the load/store unit to L2 without any inflation. Furthermore, the L2 was only 79.6% busy, suggesting there’s a bit of headroom at that layer.sometimes use global atomic operationsB580 has a 192-bit GDDR6 VRAM subsystem, likely configured as six 2×16-bit memory controllers. Latency from OpenCL is higher than it was in the previous generation.I suspect this only applies to OpenCL, because latency from Vulkan (with Nemes’s test) shows just over 300 ns of latency. Latency at large test sizes will likely run into TLB misses, and I suspect Intel is using different page sizes for different APIs.Nvidia’s RTX 4060Intel’s balance of cache capacity and memory bandwidth seems to work well, at least in the few examples I checked. Even when VRAM bandwidth demands are high, the 18 MB L2 is able to catch enough traffic to avoid pushing GDDR6 bandwidth limits. If Intel hypothetically used a smaller GDDR6 memory subsystem like Nvidia’s RTX 4060, B580 would need a larger cache to avoid reaching VRAM bandwidth limits.Probably as a cost cutting measure, B580 has a narrower PCIe link than its predecessor. Still, a x8 Gen 4 link provides as much theoretical bandwidth as a x16 Gen 3 one. Testing with OpenCL doesn’t get close to theoretical bandwidth, but B580 is at a disadvantage compared to A750.often has minimal impact on gaming performanceDCS for example will uses over 12 GB of VRAM with mods. Observing different aircraft in different areas often causes stutters on the B580. VTune shows high PCIe traffic as the GPU must frequently read from host memory. Battlemage retains Alchemist’s high level goals and foundation, but makes a laundry list of improvements. Compute is easier to utilize, cache latency improves, and weird scaling issues with global memory atomics have been resolved. Intel has made some surprising optimizations too, like reducing scalar memory access latency. The result is impressive, with Arc B580 easily outperforming the outgoing A770 despite lagging in nearly every on-paper specification.Some of Intel’s GPU architecture changes nudge it a bit closer to AMD and Nvidia’s designs. Intel’s compiler often prefers SIMD32, a mode that AMD often chooses for compute code or vertex shaders, and one that Nvidia exclusively uses. SIMD1 optimizations create parallels to AMD’s scalar unit or Nvidia’s uniform datapath. Battlemage’s memory subsystem emphasizes caching more than its predecessor, while relying less on high VRAM bandwidth. AMD’s RDNA 2 and Nvidia’s Ada Lovelace made similar moves with their memory subsystems.Of course Battlemage is still a very different animal from its discrete GPU competitors. Even with larger XVEs, Battlemage still uses smaller execution unit partitions than AMD or Nvidia. With SIMD16 support, Intel continues to support shorter vector widths than the competition. Generating SIMD1 instructions gives Intel some degree of scalar optimization, but stops short of having a full-out scalar/uniform datapath like AMD or post-Turing Nvidia. And 18 MB of cache is still less than the 24 or 32 MB in Nvidia and AMD’s midrange cards.Driver overheadBut I hope Intel goes after higher-end GPU segments once they’ve found firmer footing. A third player in the high end dGPU market would be very welcome as many folks are still on Pascal or GCN due to folks feeling as if there is not a reasonable upgrade yet. Intel’s Arc B580 addresses some of that pent-up demand, at least when it’s not out-of-stock. I look forward to seeing Intel’s future GPU efforts.PatreonPayPalDiscord]]></content:encoded></item><item><title>[OC] Systems Programming: Everything is trying to kill you and how to stop it</title><link>https://www.reddit.com/r/rust/comments/1in1t0t/oc_systems_programming_everything_is_trying_to/</link><author>/u/Buzz_Cut</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Tue, 11 Feb 2025 15:55:40 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Hi everyone! I posted here about a week ago looking for some suggestions on types of bugs rust avoids. I hosted a workshop for some undergrads at my school about what even is systems programming followed by crash course live coding session in Rust.It's my first time holding a talk so I would love any of your feedback. There were some technical difficulties but 95% of the talk came out unscathed lol.Keep in mind that the talk was geared towards people who probably have never even heard of systems programming or memory. I hope my explanations can help some beginners out there!P.S. my club will be running a workshop on mathematical modeling of robots this coming week. If you are interested please join our discord! https://hacklab.space/discord]]></content:encoded></item><item><title>KubeCon Europe</title><link>https://www.reddit.com/r/kubernetes/comments/1in1hyg/kubecon_europe/</link><author>/u/Major-Bug-6518</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Tue, 11 Feb 2025 15:43:08 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[Any of you guys planning to attend in April? For those who were able to join in the previous events, what was the best parts of it?Any advice for a first timer like me? ]]></content:encoded></item><item><title>Simple strategy to understand error handling in Go</title><link>https://www.reddit.com/r/golang/comments/1in0tiw/simple_strategy_to_understand_error_handling_in_go/</link><author>/u/zakariachahboun</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Tue, 11 Feb 2025 15:13:53 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Error handling in Go is a topic that often sparks debate. Some prefer wrapping errors for more context, while others stick to returning and checking errors as simply as possible.I’ve been refining my own approach over time, balancing readability, maintainability, and practicality. I recently wrote an article discussing some key strategies and best practices I use. Would love to hear how others handle errors in their Go projects !This is a summary of the strategy for beginners:Low-level functions (utils, DB calls, API calls, etc.)Return errors as-is (don't log)Callers higher up should decide how to handle the errorBusiness logic (services, middle-layer functions)Adds context while preserving original error detailsHTTP Handlers, CLI commands, or main()Log errors, return user-friendly messagesAvoid exposing internal details to usersHow do you structure error handling in your Go projects? Any techniques or best practices that have worked well for you?]]></content:encoded></item><item><title>I tasted Honda&apos;s spicy rodent-repelling tape and I will do it again (2021)</title><link>https://haterade.substack.com/p/i-tasted-hondas-spicy-rodent-repelling</link><author>voxadam</author><category>dev</category><category>hn</category><pubDate>Tue, 11 Feb 2025 15:08:08 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Backblaze Drive Stats for 2024</title><link>https://www.backblaze.com/blog/backblaze-drive-stats-for-2024/</link><author>TangerineDream</author><category>dev</category><category>hn</category><pubDate>Tue, 11 Feb 2025 14:55:45 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[As of December 31, 2024, we had 305,180 drives under management. Of that number, there were 4,060 boot drives and 301,120 data drives. This report will focus on those data drives as we review the Q4 2024 annualized failure rates (AFR), the 2024 failure rates, and the lifetime failure rates for the drive models in service as of the end of 2024. Along the way, we’ll share our observations and insights on the data presented, and, as always, we look forward to you doing the same in the comments section at the end of the post.Q4 2024 hard drive failure ratesAs of the end of 2024, Backblaze was monitoring 301,120 hard drives used to store data. For our evaluation, we removed from consideration 487 drives, as they did not meet the criteria to be included. We’ll discuss the criteria we used in the next section of this report. Removing these drives leaves us with 300,633 hard drives to analyze. The table below shows the annualized failure rates for Q4 2024 for this collection of drives.. Seagate 24TB drives (model: ST24000NM002H) arrived in early December. The 1,200 drives filled one Backblaze Vault with no failed drives through the end of Q4. The 24TB Seagate drives join the 20TB Toshiba and 22TB WDC drive models in the 20-plus capacity club as we continue to dramatically increase storage capacity while optimizing existing storage server space.Zero failures for the quarter. Five drive models had zero failures for the quarter starting with the 24TB Seagate drive model noted above. The others are the 4TB HGST (model: HMS5C4040ALE640), the 8TB Seagate (model: ST8000NM000A), the 14TB Seagate (model: ST14000NM000J), and the 16TB Seagate (model: ST16000NM002J). All of the zeroes come with the caveat of having a relatively small number of drives and drive days, but zero failures in a quarter is always a good thing.The 4TB drives are nearly extinct. The 4TB drive count decreased by another 1,774 drives in Q4. (I discussed exactly how we migrate them in more detail if you want to dig in.) The remaining ~4,000 drives should be gone by the end of Q1 2025. They will be replaced by the incoming 20TB, 22TB, and 24TB drives. It should be noted that out of the 4TB drives in operation in Q4, only one failed, so those 20-plus TB drives have a lot to live up to from a failure perspective.The quarterly failure rate is down. The AFR for Q4 dropped from 1.89% in Q3 to 1.35% in Q4. While all drive sizes delivered some improvement from Q3 to Q4, one of the primary drivers is the addition of over 14,000 new 20-plus TB drives. As a group, these drives delivered an AFR of 0.77% for the quarter.We noted earlier we removed 487 drives from consideration when we produced the table above covering Q4 2024. There are two primary reasons we did not consider these drive models.. These are drives of a given model that we monitor and collect Drive Stats data on, but are not considered production drives at this time. For example, drives undergoing certification testing to determine if they are performant enough for our environment are not included in our Drive Stats calculations.Insufficient data points. When we calculate the annualized failure rate for a drive model for a given period of time (quarterly, annual, or lifetime), we want to ensure we have enough data to reliably do so. Therefore we have defined criteria for a drive model to be included in the tables and charts for the specified period of time. Models that do not meet these criteria are not included in the tables and charts for the period in question.Regardless of whether or not a given drive model is included in the charts and tables, all of the data for all of the drives we use is included in our Drive Stats dataset which you can download by visiting our Drive Stats page.As with the Q4 quarterly results, we will apply these criteria to the annual and lifetime charts that follow in this report.2024 annual hard drive failure ratesAs of the end of 2024, Backblaze was monitoring 301,120 hard drives used to store data. We removed nine drive models consisting of 2,012 drives from consideration as they did not meet the annual criteria we have defined. This leaves us with 298,954 drives divided across 27 different drive models. The table below shows the AFRs for 2024 for this collection of drives.. There were no qualifying drive models with zero failures in 2024. That said, the 16TB Seagate (model: ST16000NM002J) got close by recording just one drive failure back in Q3, giving the drive an AFR of 0.22% for 2024. . During 2024, our data center techs installed 53,337 drives. If we assume there are 2,080 work hours a year (52 weeks times 40 hours), that math is , and that means our intrepid DC techs installed 26 drives per hour. Busy, busy, busy! While there were 1,200 new 24TB Seagate drives added in 2024, they were installed in early December and did not accumulate enough drive days to make the cut for the annual, or lifetime, tables. Including the 24TB Seagate drive, there were three models that missed out on being included in the 2024 annual tables, these drive models are listed below.As a reminder, a drive model needs to have over 250 drives by the end of Q4 and accumulate at least 50,000 drive days during 2024 to be included in the annual tables.Comparing Drive Stats for 2022, 2023, and 2024The table below compares the annual failure rates by drive model for each of the last three years. The table includes just those drive models which met the annual criteria as of the end of 2024. The data for each year is inclusive of that year only for the operational drive models present at the end of each year. The table is sorted by drive size and then AFR. The 2024 AFR for all drives listed was 1.57%, this is down from 1.70% in 2023.  We expect the overall failure rates to continue to fall in 2025, but we will be watching the following for indicators.
The failure rates of the 8TB and 12TB drive models. All of the models will exceed their five years of service. In general, the failure rate will noticeably increase as the drives exceed five years of service. And, while there are outliers like the current HGST 4TB drives, you can’t assume that will happen.The failure rates of the 14TB and 16TB drive models. These models are approaching middle age—three to five years in operation. This is where, according to the bathtub curve, their failure rates could gradually increase—but not as severely as when they exceed five years. The failure rates for the 20TB, 22TB, and 24TB drives models. These drives will enter the flat portion of the bathtub curve, that is where their failure rate should be the lowest.Annualized failure rates vs. drive sizeNow, we can dig into the numbers to see what else we can learn. We’ll start by looking at the quarterly annualized failure rate by drive size over the last three years.Let’s take a look at the different drive sizes and how they affect the overall annualized failure rate over time.. The 4TB (blue line) drives and 10TB (gold line) drives have had little impact over the last year on the overall failure rate as each finished the year with a relatively small number of drives. Still, the wild ride delivered by the 10TB drives keeps our DC techs on their toes. . The 8TB (gray line) drives and 12TB (purple line) drives range in age from five to eight years and as such their overall failure rates should be increasing over time. The 12TB drives are following that pattern moving up from about 1% AFR back in 2021 to just about 3% in 2024. The failure rates of the 8TB drives, while erratic from quarter-to-quarter, have a nearly flat trendline over the same period.. The 14TB (green line) and 16TB (azure* line) drives comprise 57% of the drives in service and on average they range in age from two to four years. They are in the prime of their working lives. As such, they should have low and stable failure rates, and as you can see, they do.*  Maybe azure isn’t quite right, but robin’s egg blue seemed a bit pretentious.. The 22TB (orange line) drives are in their early days as we continue to add more drives on a regular basis. Once the drive population settles down, we’ll have a better sense of the AFR direction. Still, the early results are solid with a lifetime AFR of 1.06%.Annualized failure rates vs. manufacturerOne of the more popular ways we can look at this data is by the drive manufacturer as we’ve done below.To complete the picture, the chart below uses the same data, but displays just the linear trendlines for each of the manufacturers over the same three-year period.. While the HGST trendline is not pretty, it doesn’t tell the entire story. Looking at the first chart, until Q4 2023, the HGST drives were at or below the average for all of the drives, that is all manufacturers. At that point, HGST has exceeded the average, and then some. The table below contains results for just the HGST drives for 2024. We’ve sorted them, high to low, by the 2024 AFR.As you can see, there are two 12TB drive models driving the high AFR for the HGST drives. The HUH721212ALN604 model began showing signs of an increased quarterly AFR in Q1 2023 and the HUH721212ALE604 model followed suit in Q3 2024. Without these drive models, the 2024 AFR for HGST drive would be 0.55%.. The quarterly AFR trendline decreased for the Seagate drives from 2022 through 2024. While the decrease was slight, from 2.25% to 2.0%, Seagate was the only manufacturer to do so. The decrease appears, at least in part, to be due to the removal of the Seagate 4TB drives during that period. . Over the 2022 to 2024 period, the quarterly AFR for the Toshiba drive models varied within a fairly narrow range between 0.80% and 1.52%, with most quarters hovering slightly around 1.2%. Most importantly, none of the individual drive models were outliers, as the highest quarterly AFR for any Toshiba drive model was 1.58%. We like consistency. . While WDC drive models delivered a similar level of consistency as the Toshiba models, they did so with a lower AFR each quarter. From 2022 through 2024, the range of quarterly AFR values for the WDC models was 0.0% to 0.85%. The 0.0% AFR was in Q1 2022 when none of the 12,207 WDC drives in operation failed during that quarter.Lifetime hard drive statsAs of the end of 2024, Backblaze was monitoring 301,120 hard drives used to store data. Applying our drive criteria noted above for the lifetime period, we removed 11 drive models consisting of 2,736 drives from consideration as they did not meet the lifetime criteria we defined. This leaves us with 298,230 drives divided across 25 different drive models. The table below shows the lifetime AFRs for this collection of drives.The current lifetime AFR for all of the drives is 1.31%. This is down from 1.46% in 2023. The drop is primarily due to the completion of the migration of the 4TB Seagate drives in 2024, which left us with only two of these drives still in operation as of the end of 2024. As a consequence, the 79 million drive days and over 5,600 drive failures racked up by the 4TB Seagate drives by the end of 2023 are not included in the data presented in the 2024 lifetime table above.  In the final table below, we’ve taken the lifetime table and sorted out the drive models that have a lifetime AFR of 1.50% or less by drive size.A couple of caveats as you review the table.There is enough data for each model to say the AFR values are solid. That said, everything could change tomorrow. In general, the hard drive failure rate follows the bathtub curve as the drives age—unless it doesn’t. Some drives refuse to fail as they age, like the 4TB HGST drives. Other drives are great, and then “hit the wall” and bend the failure curve upward, fast.A drive model with a 1% annualized failure rate means that you can expect one drive out of 100 to fail in a year. If you’re a personal drive user, that one drive could be yours. If you have exactly one drive, your personal annualized failure rate is 100%. In other words, always have a backup, and don’t forget to test it.I have been authoring the various Drive Stats reports for the past ten years and this will be my last one. I am retiring, or perhaps in Drive Stats vernacular, it would be “migrating.” Either way, after 10 years in the U.S. Air Force and 30+ years in Silicon Valley Tech, it is time. Drive Stats will continue with Stephanie Doyle and David Johnson as the replacement drive models beginning with the Q1 2025 report. I wish them well.I want to say thank you to each of you who have taken your time to peruse and engage with the Drive Stats reports and data over the last 10 years. And, thank you as well for the comments, questions, and discussions that raced and raged across the various communities that care about something as mundane and awesome as a hard drive. It has been quite the ride—thanks again.The Hard Drive Stats dataThe complete data set used to create the tables and charts in this report is available on our Hard Drive Test Data page. You can download and use this data for free for your own purpose. All we ask are three things: 1) you cite Backblaze as the source if you use the data, 2) you accept that you are solely responsible for how you use the data, and 3) you do not sell this data itself to anyone; it is free.Good luck, and let us know if you find anything interesting.]]></content:encoded></item><item><title>FOSDEM 2025 - Rust for Linux</title><link>https://fosdem.org/2025/schedule/event/fosdem-2025-6507-rust-for-linux/</link><author>/u/l-const</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Tue, 11 Feb 2025 14:53:24 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Rust for Linux is the project adding support for the Rust language to the Linux kernel.This talk will give a high-level overview of the project and will walk its history from the beginning -- it sounds like it was yesterday when we started, but it has been more than 4 years now.How did the endeavor start? What challenges have we faced? Who are the people behind it? Which projects do we collaborate with? How are we reconfiguring a large system with huge inertia while it keeps running? Can a similar approach be applied to other projects? What do kernel developers think about Rust?Speaking about Rust, why did we go with Rust, and not something else? How stable is the Rust we use in the kernel? What does it mean to use unstable features in this context? How did we hedge against those? What is the situation with distribution toolchain support? What about GCC and Rust?And most importantly, since this is open source: how can someone contribute?]]></content:encoded></item><item><title>[ Removed by Reddit ]</title><link>https://www.reddit.com/r/kubernetes/comments/1in06ix/removed_by_reddit/</link><author>/u/wakenbaker55</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Tue, 11 Feb 2025 14:46:01 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[   submitted by    /u/wakenbaker55 ]]></content:encoded></item><item><title>Canonical announces 12 year Kubernetes LTS. This is huge!</title><link>https://canonical.com/blog/12-year-lts-for-kubernetes</link><author>/u/Unlucky_Armadillo959</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Tue, 11 Feb 2025 13:56:08 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[Canonical’s Kubernetes LTS (Long Term Support) will support FedRAMP compliance and receive at least 12 years of committed security maintenance and enterprise support on bare metal, public clouds, OpenStack, Canonical MicroCloud and VMware. Today, Canonical announced a 12 year security maintenance and support commitment starting with  Kubernetes 1.32. The new release is easy to install, operate and upgrade, with best-of-breed open source networking, DNS, gateway, metrics server, local storage, load balancer and ingress services. Canonical Kubernetes enables customers to upgrade at their own pace, with new upstream releases every four months for organizations that prefer to move fast, and a 12 year commitment for organizations that need long-term supported environments.“Constant Kubernetes upgrades are a drain on enterprise teams. Customers who deploy Canonical Kubernetes 1.32 LTS can focus on the future, because their clusters will receive security updates for 12 full years,” said Mark Shuttleworth. “Combined with Ubuntu Pro , which covers the widest range of open source applications, the entire open source stack can now be operated with greater confidence and simpler compliance to hardening standards like FedRAMP. From the public cloud to the data center to the edge, on servers and workstations and connected devices, we are excited to deliver a simple ‘deploy-and-move-forward’ approach to enterprise grade containers that keeps the business focus on applications and innovation rather than infrastructure.”Canonical Kubernetes is a multi-cloud distribution that provides a unified production-grade Kubernetes experience for developer workstations, cloud, data centers and edge deployments. Future releases of MicroK8s and Charmed Kubernetes will build on Canonical Kubernetes and also gain the 12 year LTS. Customers will also be able to orchestrate their Kubernetes clusters using standalone Canonical Kubernetes LTS binaries and container images, with full enterprise support from Canonical. Long term support and latest upstream KubernetesThe upstream Kubernetes release cycle is fast-paced, with a new version every four months and security maintenance provided for 14 months. Frequent Kubernetes version upgrades disrupt business continuity and require time-consuming and expensive maintenance. At the same time, the pace of innovation in Kubernetes makes newer versions attractive for developers and new deployments.Canonical will provide interim releases of its Kubernetes packages every four months, aligned with the upstream Kubernetes release cadence and versions. These interim releases will be security maintained and supported for 14 months, with upgrade paths from release to release.Like Ubuntu, Canonical will release LTS packages of Kubernetes every two years, starting with Canonical Kubernetes 1.32 LTS. With an Ubuntu Pro subscription, these LTS releases will get CVE security fixes for at least 12 years. Thereafter, Canonical will continue to support and patch these releases of Kubernetes based on customer needs for extremely long-lived deployments in telecommunications and other critical infrastructure.In this way, Canonical’s packaging and LTS commitment to Kubernetes enables both rapid acquisition of the latest upstream for fast-moving deployments and developers, and long term maintenance for production deployments that need to remain stable for many years. Best-of-breed open sourceCanonical Kubernetes ships with best-of-breed open source components for essential functions and uses only standard Kubernetes abstractions and APIs to achieve full Kubernetes conformance. This enables the platform to evolve to newer implementations with minimal user disruption. Canonical Kubernetes 1.32 LTS will provide Cilium, MetalLB, CoreDNS, OpenEBS and Metrics Server by default. Customers can replace these components in the rare cases they need to do so, and the Canonical-provided elements will still be supported.Canonical will also provide standardized container images of popular Kubernetes ecosystem services such as Istio, Cert Manager or OpenTelemetry Collector – also with a 12 year security and support commitment. Customers can request additional open source components to be added to this Kubernetes ecosystem portfolio through the Everything LTS offering.Easy to install and operate across environmentsDevOps teams often use different Kubernetes distributions or versions across the application development and deployment lifecycle. As workloads move from development to test to production, any difference between those environments is a source of friction, cost, and mistakes.Canonical Kubernetes offers a simple low-touch operational approach across the entire software lifecycle. The same package provides the same APIs on developer workstation environments, small clusters, and even larger, more complex scenarios.For single-node or smaller multi-node clusters, Canonical Kubernetes is installed with just two commands per node. These clusters will automatically update with security-only fixes for the maintenance lifetime of that version, making it an easy solution for both edge and workstation.Canonical Kubernetes can also be orchestrated in more complex environments for custom storage, networking (including Multus for telco requirements) and GPUs for AI/ML training and model serving. It also comes with cluster lifecycle automation through Juju, with the possibility to integrate other Canonical infrastructure stack components.“Canonical Kubernetes Platform offers the freedom to operate Kubernetes your own way,” said Marcin Stożek, Product Manager for Kubernetes at Canonical. “It enables you to choose between automatic cluster upgrades or staying on a long-term supported release for extended stability. By addressing infrastructure complexity – one of Kubernetes’ primary challenges – through reliable security maintenance, lifecycle automation, and additional open-source components, we simplify cloud-native platform operations for all users.”Security maintenance and support for Canonical Kubernetes can be purchased with an Ubuntu Pro subscription. Learn more at https://ubuntu.com/pro.Canonical, the publisher of Ubuntu, provides open source software, security, support and services. Our portfolio covers critical systems, from the smallest devices to the largest clouds, from the kernel to containers, from databases to AI. With customers that include top tech brands, emerging startups, governments and home users, Canonical delivers trusted open source for everyone.]]></content:encoded></item><item><title>PL/Rust: a loadable procedural language for writing PostgreSQL functions in Rust that compile to native code</title><link>https://github.com/tcdi/plrust</link><author>/u/kibwen</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Tue, 11 Feb 2025 13:52:03 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Engineering Ubuntu For The Next 20 Years</title><link>https://discourse.ubuntu.com/t/engineering-ubuntu-for-the-next-20-years/55000</link><author>/u/EvilDMP</author><category>dev</category><category>reddit</category><pubDate>Tue, 11 Feb 2025 13:38:23 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>KDE Plasma 6.3 released</title><link>https://kde.org/announcements/plasma/6/6.3.0/</link><author>/u/JRepin</author><category>dev</category><category>reddit</category><pubDate>Tue, 11 Feb 2025 13:25:36 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[One year on, with the teething problems a major new release inevitably brings firmly behind us, Plasma’s developers have worked on fine-tuning, squashing bugs and adding features to Plasma 6 — turning it into the best desktop environment for everyone!Read on to discover all the exciting new changes landing in this release…Duplicate your setup with one clickPinpoint individual pixelsFine tune your art hardwareWe want to make Plasma the best platform for creativity, and Plasma 6.3 takes the next step in that direction by providing features that help artists optimize and customize their graphics tablets to their liking.The System Settings’  page has been overhauled and split into multiple tabs to improve how things are organized, and new configuration options have been added to each section:You can map an area of a drawing tablet’s surface to the entire screen areaWe have refined the tablet calibration feature so that it produces more accurate calibrationsThe stylus testing feature shows information about tilt and pressureYou can customize the pressure curve and range of a stylus to chop off the high and/or low partsYou can also re-map or swap the functions of the stylus’s buttonsAfter finishing configuring your tablet, you’ll be able to see what changed thanks to ’ “Highlight changed settings” feature, which works for most of the  page.The most important news regarding graphics is a huge overhaul of how fractional scaling works. In Plasma 6.3, KWin makes a stronger effort to snap things to the screen’s pixel grid, greatly reducing blurriness and visual gaps everywhere and producing sharper and crisper images.This works at very high zoom levels as well, as KWin’s Zoom effect switches to a sharp pixel-perfect representation and overlays a grid on top of the screen. You can actually see how individual pixels look relative to other ones. Very useful for artists and designers.In the color department, screen colors are more accurate when using the  feature both with and without ICC profiles, and KWin offers the option to choose screen color accuracy — although this can sometimes affect system performance.A smaller but still nice detail is that widgets placed on the desktop are very slightly translucent, just like the popups of widgets placed on the panel: monitors CPU usage more accurately, and consumes vastly fewer CPU resources while doing it! If you’re using Plasma 6.3 on FreeBSD, you’re in luck: the System Monitor app and widgets can now collect GPU statistics on your system too. also provides more information, exposing data about all of your GPUs as well as your batteries’ cycle counts.Monitoring printers is equally easy, as each printer’s print queue is shown directly in the widget. The widget also shows a little spinner on any printers that are currently printing, so you can see at a glance which ones are in use.Plasma already includes a variety of background services that let you know when something has gone wrong and what to do about it. New in Plasma 6.3 is a service that detects when the kernel terminated an app because the system ran out of memory. The service shows a notification explaining what happened, and suggests ways of avoiding this issue in the future.Moving on to specific tools, Plasma 6.3’s  (the built-in search tool that also does conversions, calculations, definitions, graph plotting, and much more),  (Plasma’s software management/app store application), and the  widget all come with new features and improvements:KRunner and KRunner-powered searches now let you jump between categories using the / keys and +/+ key combinations.A security enhancement landing in Discover highlights sandboxed apps whose permissions will change after being updated. This allows you to check on such changes in case you suspect any shady behavior.In a similar vein, you can now see whether apps are packaged directly by their developer, or verified by a trusted third party.If you’re a fan of the forecasts provided by Deutscher Wetterdienst, you’re in luck: Plasma 6.3’s weather widget allows using this source for weather data.Plasma 6.3 makes things easy without ditching flexibility. If you prefer using a mouse with your laptop, you can now configure its built-in touchpad to switch off automatically, so it doesn’t interfere with your typing. Also, if you set up your machine as a network hotspot, Plasma generates a random password for the network so you don’t have to think one up.Finding help is easier in Plasma 6.3. A “Help” category has been added to the launcher (the menu that tends to live on the left hand side of your panel), and we have removed the  category entirely. Its contents have been merged into the  category, reducing the number of categories that don’t offer meaningful grouping.Speaking of menus, the default  launcher menu now switches categories only when you click on them, matching the behavior of all other sidebar lists. However, if you preferred the old switch-on-hover behavior, it’s still available too.We have made things clearer by adding a  item to the desktop context menu for symbolic links, the digital Clock widget displays all events on days with more than five of them (giving you a complete view of upcoming commitments), and when you want to reboot into the bootloader menu the next time your machine reboots, the logout screen now indicates this.To avoid overwhelming you with too much information, when notifications arrive while Plasma’s “Do Not Disturb” mode is engaged, exiting that mode shows the number of missed notifications, rather than sending them all in one giant torrent.Additionally, a subtle but important change: when you drag a file out of a window that’s partially below other windows, it no longer jumps to the top, potentially obscuring what you wanted to drag it into!Finally, what would Plasma be without customization? To begin with, panels can be cloned! You can also use scripting to change your panels’ opacity levels and what screen they appear on.In Plasma 6.2, we introduced symbolic icons in ’s category sidebar. Some people didn’t like that, so in 6.3 you can undo the change yourself: we modified the implementation to pull icons from the standard data source, allowing you to set them to whatever you want using the  app.Speaking of the  app, editing desktop files for apps from the “Edit Application…” menu item in  (and other launcher menus) opens the app in the editor, rather than showing you file properties. This lets you easily edit the entire applications list!If you have ever lost a widget in the process of customizing your system, you’ll love this new feature: in Plasma 6.3, the  gives you the opportunity to remove every instance of a widget, including those that got lost or are only present on unplugged screens.…and there’s much more. To see the full list of changes, check out the complete changelog for Plasma 6.3.You can give us feedback and get updates on our social media channels:
Your feedback is greatly appreciated.KDE is a Free Software community that exists and grows only because of the help of many volunteers that donate their time and effort. KDE is always looking for new volunteers and contributions, whether it is help with coding, bug fixing or reporting, writing documentation, translations, promotion, money, etc. All contributions are gratefully appreciated and eagerly accepted. Please read through the Supporting KDE page for further information or become a KDE e.V. supporting member through our Join the Game initiative.KDE is an international technology team that creates free and open source software for desktop and portable computing. Among KDE’s products are a modern desktop system for Linux and UNIX platforms, comprehensive office productivity and groupware suites and hundreds of software titles in many categories including Internet and web applications, multimedia, entertainment, educational, graphics and software development. KDE software is translated into more than 60 languages and is built with ease of use and modern accessibility principles in mind. KDE’s full-featured applications run natively on Linux, BSD, Windows, Haiku, and macOS.]]></content:encoded></item><item><title>Rendering React on Golang with V8Go</title><link>https://itnext.io/rendering-react-on-golang-with-v8go-fd84cdad2844?source=friends_link&amp;amp;sk=040bff7b0a2ca9d02179c7897476ace4</link><author>/u/congolomera</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Tue, 11 Feb 2025 13:03:36 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Server-side rendering (SSR) is essential for improving web performance, SEO, and initial page load experience. React provides built-in SSR capabilities through , but this typically requires a Node.js environment.What if we want to run and render React components without Node.js? This could be useful in cases where:We’re building a Go-based web server and want to keep everything self-contained.We don’t want to spin up an external Node.js process just to render React.We’re running in environments with limited dependencies, such as embedded systems or lightweight containers.For more details, you can see the full source code on GitHubhttps://github.com/highercomve/go-react-ssrInstead of running a separate Node.js process, we can embed , the JavaScript engine that powers Chrome, directly into our Go application using .This approach provides several benefits:: No need to start and communicate with an external Node.js process.: Everything runs inside the Go application, reducing complexity.: We control how the JavaScript…]]></content:encoded></item><item><title>Introducing the CrossValidationReport</title><link>https://www.youtube.com/watch?v=R6dRAE83Y2c</link><author>probabl</author><category>dev</category><category>ml</category><enclosure url="https://www.youtube.com/v/R6dRAE83Y2c?version=3" length="" type=""/><pubDate>Tue, 11 Feb 2025 12:45:02 +0000</pubDate><source url="https://www.youtube.com/channel/UCIat2Cdg661wF5DQDWTQAmg">Dev - Probabl</source><content:encoded><![CDATA[Skore version 0.6 introduces the `CrossValidationReport` that provides you with an `EstimatorReport` for each fold of your cross-validation, enabling you to inspect your estimator on each fold.

------
Links:

skore v0.6 documentation:
https://skore.probabl.ai/0.6/index.html

skore GitHub repository:
https://github.com/probabl-ai/skore

Website: https://probabl.ai/
LinkedIn: https://www.linkedin.com/company/probabl
Twitter: https://x.com/probabl_ai
Bluesky: https://bsky.app/profile/probabl.bsky.social
Discord: https://discord.probabl.ai

We also host a podcast called Sample Space, which you can find on your favourite podcast player. All the links can be found here:
https://rss.com/podcasts/sample-space/

#probabl]]></content:encoded></item><item><title>Migrating a Ray-Tracing Calculator from C to Rust – Planning a Master’s Thesis &amp; Looking for Advice (C, Rust, BSP-Tree)</title><link>https://www.reddit.com/r/rust/comments/1imws36/migrating_a_raytracing_calculator_from_c_to_rust/</link><author>/u/Smil3More</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Tue, 11 Feb 2025 11:48:43 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I am planning a  in which I need to modernize and parallelize an existing scientific C program for electromagnetic wave propagation calculation/simulation. The goal is to migrate the program to  to improve long-term maintainability, safety, and performance.It's a Linux command-line program that takes a TXT input file with room and material specifications, calculates the electromagnetic spread (heatmap), and outputs a TXT file, which is later converted into a graphical solution in another program.My focus is solely on the calculation part.Simulates electromagnetic wave propagation using ray tracing. (only mathematical, no graphical conversion)BSP tree (Binary Space Partitioning) as the core data structure for geometry management.C-based, currently single-threaded, running on . (takes a loooong time to calculate)Future goals: CPU parallelization & potential GPU extension (OpenCL, Vulkan Compute).BSP Tree in Rust – Feasible or alternative approaches?Is Rust well-suited for BSP trees, or are there better parallel alternatives?Are there  that could be useful for ray tracing & BSP trees?Rust beginner with decent programming experience – Is migration realistic?I have solid programming experience in C++, Python, Dart but very little Rust knowledge.Is Rust a good choice for complex scientific simulations, or is the learning curve too steep?Full migration vs. partial migration & C/Rust interoperabilityWould it make more sense to migrate only the core algorithm to Rust and keep the rest in C?Has anyone worked with C/Rust interop via FFI or for a mixed-language approach?How practical is a , running Rust and C in parallel?I would appreciate any best practices, experiences, and resources on using Rust for scientific applications! 🚀]]></content:encoded></item><item><title>smol-gpu: A tiny RISC-V GPU built to teach modern GPU architecture</title><link>https://github.com/Grubre/smol-gpu</link><author>/u/Direct-Title-3416</author><category>dev</category><category>reddit</category><pubDate>Tue, 11 Feb 2025 11:00:36 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Weekly: Questions and advice</title><link>https://www.reddit.com/r/kubernetes/comments/1imw1gz/weekly_questions_and_advice/</link><author>/u/gctaylor</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Tue, 11 Feb 2025 11:00:33 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[Have any questions about Kubernetes, related tooling, or how to adopt or use Kubernetes? Ask away!]]></content:encoded></item><item><title>We Replaced Our React Frontend with Go and WebAssembly</title><link>https://dagger.io/blog/replaced-react-with-go</link><author>/u/GarethX</author><category>dev</category><category>reddit</category><pubDate>Tue, 11 Feb 2025 10:31:32 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[A few weeks ago, we launched Dagger Cloud v3, a completely new user interface for Dagger Cloud. One of the main differences between v3 and its v2 predecessor is that the new UI is written in WebAssembly (WASM) using Go. At first glance, this might seem an odd choice - Go typically isn't the first language you think of when deciding to program a Web UI - but we had good reasons. In this blog post, I'll explain why we chose WebAssembly, some of our implementation challenges (and how we worked around them), and the results.Two Codebases = More Work, Fewer FeaturesDagger works by building up a DAG of operations and evaluating them, often in parallel. By nature, this is a difficult thing to display. To help users make sense of it, we offer two real-time visualization interfaces: the Dagger terminal UI (TUI), included in the Dagger CLI, and Dagger Cloud, an online Web dashboard. The Dagger TUI is implemented in Go, and Dagger Cloud (pre-v3) was written in React.Obviously, we want both user interfaces to be as close to each other as possible. But the actual act of interpreting Dagger's event stream in real-time and producing a UI is pretty involved. Some of the more complex event streams we've seen have hundreds of thousands of OpenTelemetry spans, and managing the data structures around them gets very complicated, very quickly. The Web UI often couldn't keep up with the huge volume of data it had to process and it would become laggy and slow; to fix this performance bottleneck, we were forced into a different implementation model for the React application.So, we ended up with two interfaces trying to accomplish the same thing, one of them in one language and ecosystem (TypeScript/React), the other in a totally different language and ecosystem (Go), and we couldn't easily share business logic between them. As a small team, we need to ship fast. Having to re-implement every feature twice was just a massive tax on our velocity.We started thinking about a new approach to Dagger Cloud, with two main goals:Unify the codebases, to eliminate duplication and make it more efficient to ship new featuresDeliver on the promise of a crisp, snappy Web UI, matching the speed and performance of the terminal UIChoosing Go + WebAssemblyOur starting goal was to be able to reuse one codebase for both Dagger Cloud and the TUI. We decided fairly early to make it a Go codebase. Technically, we could have gone the other way and used TypeScript for the TUI. But we're primarily a team of Go engineers, so selecting Go made it easier for others in the team to contribute, to add a feature or drop in for a few hours to help debug an issue. In addition to standardizing on a single language, it gave us flexibility and broke down silos in our team.Once we decided to run Go code directly in the browser, WebAssembly was the logical next step. But there were still a couple of challenges:The Go + WebAssembly combination is still not as mature as React and other JavaScript frameworks. There are no ready-made component libraries to pull from, the developer tooling isn't as rich, and so on. We knew that we would need to build most of our UI components from scratch.There is a hard 2 GB memory limit for WebAssembly applications in most browsers. We expected this to be a problem when viewing large traces, and we knew we would have to do a lot of optimization to minimize memory usage and keep the UI stable. This wasn't entirely bad though; the silver lining here was that any memory usage improvements made to the WebAssembly UI would also benefit TUI users, since it was now a shared codebase.Once we'd made the decision, the next question was, "how do we build this?" We decided to build the new WebAssembly-based UI in the Go-app framework. Go-app is a high-level framework specifically for Progressive Web Apps (PWAs) in WebAssembly. It offers key Go benefits, like fast compilation and native static typing, and it also follows a component-based UI model, like React, which made the transition easier.Since the Go + WebAssembly combination isn't mainstream, there was some healthy skepticism within the Dagger team about its feasibility. For example, there was no real ecosystem for Go-app UI components and we knew we’d have to write our own, but we weren’t sure how easy or difficult this would be. We also had concerns over integrations with other services (Tailwind, Auth0, Intercom, PostHog), and about rendering many hundreds of live-updating components at the same time. To answer these questions and de-risk the project, I spent almost a month prototyping, with the goal of re-implementing as much of the existing UI as possible in Go-app. As it turned out, there weren't many blockers: WebAssembly is already a well-documented open standard and most other questions were answered in Go-app’s own documentation. The biggest challenge, as expected, was the memory usage limit, which required careful design and optimization.From Prototype to ProductionOnce we had a working proof of concept, the team's comfort level increased significantly and we kicked off project "awesome wasm" to deliver a production implementation. Here are a few notes from the journey:Memory usage was easily the most existential threat to the project’s success. I spent a lot of time figuring out how to render 200k+ lines of log output without crashing. This led to optimizations deep in our virtual terminal rendering library, which dramatically reduced TUI memory usage at the same time (as mentioned already, sharing codebases means that important optimizations in one interface become "free" in the other!)Go WASM is slow at parsing large amounts of JSON, which led to dramatic architecture changes and the creation of a “smart backend” for incremental data loading over WebSockets, using Go's rarely-used encoding/gob format.Initially, the WASM file was around 32 MB. By applying Brotli compression, we were able to bring it down to around 4.6 MB. We tried to perform Brotli compression on-the-fly in our CDN but the file was too large, so eventually we just included the compression step into our build process.Apart from the memory challenges, most of our other initial worries turned out unfounded. The UI components weren’t very hard to write, integrations with other services were straightforward, and I found good techniques for handling component updates in real-time.There were a number of useful NPM packages I found, so I wondered if I could use them with Go. WebAssembly has a straightforward interface to both Go and JavaScript, so I built a Dagger module that uses Browserify to load an NPM package. This module allows us to generate a JavaScript file that can be included in a Go application. This means that we can work primarily in Go and then, if needed, we have a way to load helpers that are implemented in native JavaScript.Disclaimer: I'm not a React professional so with that in mind...it seemed to me that React had a very rigid way of implementing components, while Go-app was much more flexible. In Go-app, you can have any component update whenever you like, which gives you many more degrees of freedom for optimization. For example, I needed to optimize a component rendering 150,000+ lines of output. Just having the ability to try different approaches and then pick the one that worked best, made the entire exercise much easier!Even though Go-app doesn't have React-like developer tools built into the browser, I was able to use Go's own tools (pprof) plus the default profiler built into the browser for profiling and debugging. This was very useful to inspect functions calls, track CPU and memory usage, and evaluate the effectiveness of different approaches for optimizing memory usage.I discovered a side benefit of using Go-app: since Dagger Cloud is built as a PWA, it can be installed as a desktop or a mobile application. This makes it possible to launch Dagger Cloud like a native application and get a full-screen experience without needing to open a browser first, or just have a dedicated icon in your desktop taskbar/dock.We soft-launched Dagger Cloud v3 to our Dagger Commanders a few weeks ago to collect feedback and made it available to everyone shortly thereafter.Our switch from React to WASM has resulted in a more consistent user experience across all Dagger interfaces, and better overall performance and lower memory usage, especially when rendering large and complex traces.From an engineering perspective too, the benefits to our team are significant. Optimizations very often involve just as much, if not more, work than actually implementing features. So it's great to not have to spend time optimizing the Web UI, and then more time optimizing the TUI, and instead actually focus on delivering new features.Dagger Cloud v3 has the Dagger community buzzing and one of the more common questions we've been fielding recently is: who should consider doing this and who shouldn't?We want to be clear that we're not generally recommending making front-ends in Go. We had some very good reasons to do it: a team of strong Go engineers; a complex UI that TypeScript/React didn't scale well for; a requirement for standardization and reuse between two codebases; and a company-wide mandate to increase our velocity. That's a fairly specific set of circumstances. If you're in similar circumstances, this is certainly an option worth evaluating; if not, there are other tools and standards that you should consider first.Dagger Cloud v3 is still in beta and we're excited for you to try it out. If you'd like to know more about our implementation or simply have feedback to share on the new UI, join our Discord and let us know what you think!]]></content:encoded></item><item><title>Context-switching is the main productivity killer for developers</title><link>https://newsletter.techworld-with-milan.com/p/context-switching-is-the-main-productivity</link><author>/u/oatridbed</author><category>dev</category><category>reddit</category><pubDate>Tue, 11 Feb 2025 09:39:24 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Have you ever wondered what the biggest productivity killer for developers is? There are many, but one stands out—and it’s often underestimated.it costs that person 23 minutes of productive work,I’ve worked with development teams for over a decade, and we consistently underestimate the disruptive nature of interruptions. This article explores why context-switching is so costly and how to manage it effectively.Authorization can significantly impact your application’s security and scalability. From managing dynamic permissions to implementing fine-grained access controls, the challenges grow as your requirements and users scale. This ebook is based on insights from 500+ interviews with engineers and IAM leads. It explores over 20 technologies and approaches, providing practical guidance to design a future-proof authorization system. Learn how to create a solution that evolves with your business needs while avoiding technical debt.Each shift forces your brain to refocus, sorting through your memory to figure out where you wereThe term context-switching is borrowed from operating systems (OS). OS can handle multiple processes in one processing unit (usually CPU) by putting one process on hold and processing another. Those systems can do context-switching, while our brains cannot.We switch tasks more often than we realize because our tools, such as apps and notifications, are designed to grab our attention. Our brains love novelty and rush toward any new message or update. We also feel pressure from work to respond immediately, which splits our focus even more (and our work cultures usually reward this). And there’s so much information—emails, chats, open tabs—that it’s easy to get distracted. lives in your working memoryhold only up to 7 items in short-term memorydevelopers need an average of 23 minutes to rebuild their focus after an interruption fullyWhen we are interrupted, we're not just losing minutes – we're degrading the quality of their work in several ways:developers who face frequent interruptions show signs of mental fatigue much earlier in the day, leading to more errors in their afternoon workA research study by Amoroso d'Aragona et al. (2023) [3] examined the impact of interruptions and breaks on software development and found striking correlations between context switching and code quality degradation. Their analysis revealed that:Frequent breaks and interruptionsProlonged gaps in activityInterrupted coding sessionslower code maintainabilityThese numbers aren't just statistics – they represent real problems that teams must fix later, creating a cycle of technical debt and cleanup work. A five-minute interruption during a critical problem-solving moment can create a long delay in task completion.We must discuss the flow state to understand why interruptions are so bad. This isn't just programmer jargon – it's a well-researched psychological state first described by Mihaly Csikszentmihalyi in 1990 [4].Flow state is a mental state in which work feels effortless and time seems to disappear In that zone, you’re challenged enough to stay engaged, but not so much that you shut downyou must keep adjusting the difficulty of tasks as your abilities growthe flow state is very fragileon-screen interruptions significantly impactLet me share a story from a team I recently worked with. We tracked our interruptions for a month and found patterns that might sound familiar.Morning interruptions were particularly costlyThe story completion rate increased by 35%Bug reports decreased by 28%Team satisfaction scores improved by 45%The image below shows the calendar with the protected focus time for deep work. After studying this problem across multiple teams, I've found several approaches that consistently work:daily goals📝 Capture tasks on the TODO listTodoistMicrosoft To Dotask prioritization method🅿️ The parking lot technique🚧 Interruptible workflow:Zeigarnik EffectPomodoro Technique🤔 Be intentionally responsiveAlgorithms to Live ByBe no more responsive than that🗓 Plan the day strategically🚨 Interruption protocols:📨 Asynchronous communication No need to respond to this message right away if you're in the middle of something🤝 Make meetings meaningful Learn how to be more productive: How to Be 10x More Productive I was always amazed by top performers. I wondered what they do and how they are much better than others. Then, I started researching and talking directly to some of them. I finally managed to get some top performers as my mentors and, in the end, became one of them. I learned that they are not better than others, but they use some techniques that help t…2 years ago · 65 likes · 6 comments · Dr Milan MilanovićTo know if your interruption management is working, track these metrics:🔔 Unplanned interruptions⏳ Duration of uninterrupted coding sessions.😊 Team satisfaction scores.🏃‍♂️ Sprint velocity trends.increased developer productivitythey're a significant productivity killer that affects code quality, team morale, and project timelinesSo, start by implementing one or two of these suggestions. Measure the results. Adjust based on what works for your team. The goal isn't to eliminate all interruptions—it's to ensure that the interruptions that occur are worth their actual cost.Let me know if you like it.more than 350,000+ tech professionalsBook a working session with me]]></content:encoded></item><item><title>Hands-on workshop: OpenTelemetry and Linkerd (this Thursday)</title><link>https://www.reddit.com/r/kubernetes/comments/1imuq9x/handson_workshop_opentelemetry_and_linkerd_this/</link><author>/u/mmanciop</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Tue, 11 Feb 2025 09:22:31 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[if you're interested in OpenTelemetry and/or Linkerd, join the hands-on workshop I'll be co-hosting with Flynn (Linkerd) this Thursday.We will look into OpenTelemetry and what it does, how distributed tracing and service meshes interact and complete one another, and on the support for OpenTelemetry in Linkerd, which no longer requires translating from OpenCensus (pretty neat!).]]></content:encoded></item><item><title>Stateful vs. Stateless Architecture</title><link>https://blog.algomaster.io/p/stateful-vs-stateless-architecture</link><author>Ashish Pratap Singh</author><category>dev</category><enclosure url="https://substack-post-media.s3.amazonaws.com/public/images/7e4801c3-e3aa-4ab6-8fe6-759af4a1f91a_1684x1196.png" length="" type=""/><pubDate>Tue, 11 Feb 2025 08:46:26 +0000</pubDate><source url="https://blog.algomaster.io/">Dev - Algomaster</source><content:encoded><![CDATA[When a client interacts with a server, there are two ways to handle it: The client includes all necessary data in each request, so the server doesn’t store any prior information. The server retains some data from previous requests, making future interactions dependent on past state.In software systems,  refers to any data that persists across requests, such as user sessions, shopping carts, or authentication details.The choice between stateless and stateful architecture can affect scalability, performance, complexity, and cost.In this article, we’ll break down both the approaches, their advantages and trade-offs, and when to use each—with real-world examples.In a , the system remembers client or process data () across multiple requests.Once a client connects, the server holds on to certain details—like user preferences, shopping cart contents, or authentication sessions—so the client doesn’t need to resend everything with each request.Stateful systems typically store the state data in a database or in-memory storage. During online shopping, when you add items to your cart, the website remembers your selections. If you navigate away to browse more items and then return to your cart, your items are still there, waiting for you to check out.Common Patterns in Stateful ArchitectureIf you use  session storage (i.e., each app server keeps its own sessions locally), you can configure your load balancer for “sticky sessions.” This means: Once a client is assigned to , all subsequent requests from that client are routed to .: If Server A fails, the user’s session data is lost or the user is forced to re-log in. Sticky sessions are also less flexible when scaling because you can’t seamlessly redistribute user traffic to other servers.2. Centralized Session StoreA more robust approach is to store session data in a  or  store (e.g., Redis). : All servers can access and update session data for any user. Any server can handle any request, because the session data is not tied to a specific server’s memory.: You introduce network overhead and rely on an external storage. If the centralized storage fails, you lose session data unless you have a fallback strategy.Personalized Experiences: Stateful systems can deliver highly tailored interactions, as they remember user preferences and past actions. Users can seamlessly resume activities where they left off, even if they disconnect and reconnect. Certain operations can be faster because the server already possesses necessary data. Maintaining state for a large number of users can become resource-intensive and complex, as each server needs to keep track of specific sessions. Managing and synchronizing state across multiple servers (if needed) introduces additional challenges. If a server holding a user's state fails, their session data might be lost.E-commerce Shopping Carts – Stores cart contents and user preferences across multiple interactions, even if the user navigates away and returns.Video Streaming Services (Netflix, YouTube) – Remembers user watch progress, recommendations, and session data for a seamless experience.Messaging Apps (WhatsApp, Slack) – Maintains active user sessions and message history for real-time communication.In a  architecture, the server does  preserve client-specific data between individual requests.Each request is treated as , with no memory of previous interactions.Every request must include all necessary information for processing.Once the server responds, it discards any temporary data used for that request.: Most  follow a stateless design. For instance, when you request weather data from a public API, you must provide all required details (e.g., location) in each request. The server processes it, sends a response, and forgets the interaction.Common Patterns in Stateless Architecture1. Token-Based Authentication (JWT)A very popular way to implement statelessness is through tokens, particularly  (JSON Web Tokens):Client Authenticates Once: The user logs in using credentials (username/password) for the first time, and the server issues a signed .: The client includes JWT token in each request (e.g., Authorization: Bearer <token> header).: The server validates the token’s signature and any embedded claims (e.g., user ID, expiry time).: The server does  need to store session data; it just verifies the token on each request.Many APIs, including OAuth-based authentication systems, use JWTs to enable stateless, scalable authentication.Stateless architectures benefit from , ensuring that repeated requests produce the same result. This prevents inconsistencies due to network retries or client errors. A  request with the same payload  updates the user’s data but doesn’t create duplicates.Idempotent APIsensures consistency and reliability, especially in distributed systems where requests might be retried automatically. Stateless systems are inherently easier to scale horizontally. New servers can be added effortlessly, as they don't need to maintain any specific user sessions. Since servers don't track state, the architecture is generally simpler and easier to manage. The failure of a single server won't disrupt user sessions, as data isn't tied to specific servers.With no session data stored on the server, you free up memory that would otherwise be reserved for session management.Easier to Cache Responses: Since requests are self-contained, caching layers (like CDNs) can more easily store and serve responses. Stateless systems can't provide the same level of personalization or context awareness as stateful systems without additional effort (like using cookies or tokens).The client must keep track of the authentication token or relevant data. If it loses the token, it must re-authenticate. Every request needs to carry all the required information, potentially leading to larger payloads.Microservices Architecture: Each service handles requests independently, relying on external databases or caches instead of maintaining session data.Public APIs (REST, GraphQL): Clients send tokens with each request, eliminating the need for server-side sessions.Tokens are securely stored on the device and sent with every request to authenticate users.Stateless endpoints make caching easier since responses depend only on request parameters, not stored session data. A CDNcan cache and serve repeated requests, improving performance and reducing backend load.There's no one-size-fits-all answer when choosing between stateful and stateless architectures.The best choice depends on your application’s needs, scalability goals, and user experience expectations.When to Choose Stateful ArchitectureStateful systems are ideal when user context and continuity are critical. Consider a stateful approach if your application:Requires personalization (e.g., user preferences, session history)Needs real-time interactions (e.g., chat applications, multiplayer gaming)Manages multi-step workflows (e.g., online banking transactions, checkout processes)Must retain authentication sessions for security and convenience A shopping cart in an e-commerce app should persist, so users don’t have to re-add items after refreshing the page.When to Choose Stateless ArchitectureStateless systems work best when scalability, simplicity, and resilience are top priorities. Use a stateless approach if your application:Handles a high volume of requests and needs to scale efficientlyDoesn’t require storing client-specific data between requestsNeeds fast, distributed processing without server dependenciesMust ensure reliability and failover readiness A weather API doesn’t need to remember previous requests. Each query includes the location, and the response is processed independently.Hybrid Approaches: The Best of Both WorldsMany modern applications  stateful and stateless components for flexibility.This hybrid approach allows:Stateless APIs for core functionality, ensuring high scalabilityStateful sessions for personalization, improving user experienceExternal session stores (e.g., Redis) to manage state while keeping app servers stateless A video streaming platform (e.g., Netflix) uses a stateless backend for streaming but retains stateful user sessions to track watch history and recommendations.If you found it valuable, hit a like ❤️ and consider subscribing for more such content every week.If you have any questions or suggestions, leave a comment.This post is public so feel free to share it. If you’re enjoying this newsletter and want to get even more value, consider becoming a .I hope you have a lovely day!]]></content:encoded></item><item><title>nowdays linux is game ready too, kvm+looking-glass</title><link>https://www.reddit.com/r/linux/comments/1imsdso/nowdays_linux_is_game_ready_too_kvmlookingglass/</link><author>/u/Impossible_Fix_6127</author><category>dev</category><category>reddit</category><pubDate>Tue, 11 Feb 2025 06:29:46 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>VS Code update treats Copilot as &quot;out-of-the-box&quot; feature • DEVCLASS</title><link>https://devclass.com/2025/02/07/vs-code-update-treats-copilot-as-out-of-the-box-feature/</link><author>/u/stronghup</author><category>dev</category><category>reddit</category><pubDate>Tue, 11 Feb 2025 05:48:03 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Nvidia&apos;s RTX 5090 power connectors are melting</title><link>https://www.theverge.com/news/609207/nvidia-rtx-5090-power-connector-melting-burning-issues</link><author>ambigious7777</author><category>dev</category><category>hn</category><pubDate>Tue, 11 Feb 2025 04:13:47 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Ah shit, here we go again. Two owners of Nvidia’s new RTX 5090 Founders Edition GPUs have reported melted power connectors and damage to their PSUs. The images look identical to reports of RTX 4090 power cables burning or melting from two years ago. Nvidia blamed the issue on people not properly plugging the 12VHPWR power connection in fully and the PCI standards body blamed Nvidia.A Reddit poster upgraded from an RTX 4090 to an RTX 5090 and noticed “a burning smell playing ,” before turning off their PC and finding the damage. The images show burnt plastic at both the PSU end of the power connector and the part that connects directly to the GPU. The cable is one from MODDIY, a popular manufacturer of custom cables, and the poster claims it was “securely fastened and clicked on both sides (GPU and PSU).”While it’s tempting to blame the MODDIY cable, Spanish YouTuber Toro Tocho has experienced the same burnt cable (both at the GPU and PSU ends) with an RTX 5090 Founders Edition while using a cable supplied by PSU manufacturer FSP. Plastic has also melted into the PCIe 5.0 power connector on the power supply. MODDIY also responded in a Reddit thread, ruling out the “possibility of a defective cable or manufacturing error” and offering to cover the cost of repair if Nvidia and Asus don’t honor their warranties.YouTuber der8auer has also examined the Reddit poster’s equipment in person, and ruled out any form of user error in the process. He’s also found that this could be related to a current distribution problem with RTX 5090 Founders Edition models instead. Either way, nobody should be blaming 12VHPWR on end users.Nvidia originally introduced the 12VHPWR power connector on its RTX 40-series GPUs, and power supplies also debuted to support the new standard. The RTX 4090 Founders Edition was able to draw 450 watts over the 12VHPWR connector, while the new RTX 5090 draws up to 575 watts over a cable that’s rated up to 600 watts. After early issues with RTX 4090 connectors melting, PCI-SIG, the standards organization responsible for the 12VHPWR connector, has now updated it to a new 12V-2x6 connector on the GPU side and in some cases the PSU side, too.The 12V-2x6 connector has shorter sensing pins and longer conductor terminals, to improve reliability. “This might not sound like a huge difference, but it matters in ensuring that the power cable has been properly connected to whatever device is going to be pulling power from your system’s power supply,” explains Corsair.Nvidia uses the 12V-2x6 connector on its RTX 50-series GPUs, but you can still use existing 12VHPWR cables. “To be clear, this is not a new cable, it is an updated change to the pins in the socket, which is referred to as 12V-2x6,” says Corsair. PSU manufacturers like Corsair and MSI have adopted colored pins on their 12VHPWR cables so that if you can still see the yellow or grey pins it means the connector isn’t seated properly.While Intel and AMD are both members of the PCI-SIG group that helped develop the 12VHPWR power connector, only Nvidia has adopted the standard so far for consumer GPUs. Even AMD’s upcoming Radeon RX 9070-series are using existing 8-pin PCIe connections instead. AMD even suggested the 12VHPWR connector was a fire hazard in late 2022, when the company’s gaming marketing director Sasa Marinkovic tweeted “Stay safe this holiday season” alongside a picture of 8-pin connectors.12VHPWR has been branded a “dumpster fire,” thanks to design oversights that make it relatively easy for end users to not properly connect the cable securely. Cablemod was also forced to recall its 12VHPWR GPU power adapters last year after reports of melted adapters.We reached out to Nvidia to comment on these latest reports of RTX 5090 power connector issues, but the company refused to comment.: Article updated with comment from MODDIY and a new discovery from YouTuber der8auer.]]></content:encoded></item><item><title>jiff 0.2.0 released - A relatively new datetime library with automatic tzdb support, DST safe arithmetic/rounding and more</title><link>https://github.com/BurntSushi/jiff/releases/tag/0.2.0</link><author>/u/burntsushi</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Tue, 11 Feb 2025 02:58:48 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Optimization problems in high-performance programs with Go</title><link>https://www.reddit.com/r/golang/comments/1imo7jt/optimization_problems_in_highperformance_programs/</link><author>/u/JotaEspig</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Tue, 11 Feb 2025 02:36:33 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Hello guys, I'm currently working on a chess engine using golang, where one of the most important things about it is the performance. After running a profiler I got this:Showing nodes accounting for 21.63s, 48.64% of 44.47s totalDropped 1385 nodes (cum <= 0.22s)Showing top 15 nodes out of 18811.36s 25.55% 25.55% 11.43s 25.70% runtime.cgocall C:\Program Files\Go\src\runtime\cgocall.go:1672.35s 5.28% 30.83% 2.35s 5.28% runtime.stdcall2 C:\Program Files\Go\src\runtime\os_windows.go:10002.06s 4.63% 35.46% 2.06s 4.63% runtime.stdcall1 C:\Program Files\Go\src\runtime\os_windows.go:9911.06s 2.38% 37.85% 1.06s 2.38% runtime.stdcall0 C:\Program Files\Go\src\runtime\os_windows.go:9820.71s 1.60% 39.44% 0.71s 1.60% runtime.scanobject C:\Program Files\Go\src\runtime\mgcmark.go:14460.68s 1.53% 40.97% 0.68s 1.53% runtime.stdcall3 C:\Program Files\Go\src\runtime\os_windows.go:10090.59s 1.33% 42.30% 0.59s 1.33% runtime.procyield C:\Program Files\Go\src\runtime\asm_amd64.s:8070.50s 1.12% 43.42% 0.50s 1.12% runtime.stdcall4 C:\Program Files\Go\src\runtime\os_windows.go:10180.44s 0.99% 44.41% 0.44s 0.99% runtime.stdcall7 C:\Program Files\Go\src\runtime\os_windows.go:10450.38s 0.85% 45.27% 0.38s 0.85% runtime.memclrNoHeapPointers C:\Program Files\Go\src\runtime\memclr_amd64.s:930.38s 0.85% 46.12% 0.38s 0.85% runtime.scanblock C:\Program Files\Go\src\runtime\mgcmark.go:13620.31s 0.7% 46.82% 0.31s 0.7% runtime.scanblock C:\Program Files\Go\src\runtime\mgcmark.go:13590.29s 0.65% 47.47% 0.29s 0.65% runtime.(*mspan).base C:\Program Files\Go\src\runtime\mheap.go:4920.25s 0.56% 48.64% 0.40s 0.9% gce/pkg/chess.(*Board).IsKingInCheck D:\jotin\Documents\Informatica\projects\go-chess-engine\pkg\chess\board.go:150Apparently, the cpu usage is mostly at runtime. Why is that? How can I possibly avoid this?I already try to preallocate everythin I can, but not so much improvement.At the moment, the program can process and average of 25k nodes per seconds (node is a final position). One of the best engines in the world (Stockfish) runs at 2000 knps (2 million nodes per second). I would love to reach 100 knps. Any idea?]]></content:encoded></item><item><title>Go 1.24 is released!</title><link>https://go.dev/blog/go1.24</link><author>Junyang Shao, on behalf of the Go team</author><category>dev</category><category>go</category><pubDate>Tue, 11 Feb 2025 00:00:00 +0000</pubDate><source url="http://blog.golang.org/feed.atom">Dev - Golang Blog</source><content:encoded><![CDATA[
      Junyang Shao, on behalf of the Go team
      11 February 2025
      Today the Go team is excited to release Go 1.24,
which you can get by visiting the download page.Go 1.24 comes with many improvements over Go 1.23. Here are some of the notable
changes; for the full list, refer to the release notes.Several performance improvements in the runtime have decreased CPU overhead
by 2–3% on average across a suite of representative benchmarks. These
improvements include a new builtin  implementation based on
Swiss Tables, more efficient
memory allocation of small objects, and a new runtime-internal mutex
implementation.The  command now provides a mechanism for tracking tool dependencies for a
module. Use  to add a  directive to the current module. Use
 to run the tools declared with the  directive.
Read more on the go command in the release notes.The new  analyzer in  subcommand reports common mistakes in
declarations of tests, fuzzers, benchmarks, and examples in test packages.
Read more on vet in the release notes.Standard library additionsImproved WebAssembly supportGo 1.24 adds a new  directive for Go programs to export
functions to the WebAssembly host, and supports building a Go program as a WASI
reactor/library.
Read more on WebAssembly in the release notes.Please read the Go 1.24 release notes for the complete and
detailed information. Don’t forget to watch for follow-up blog posts that
will go in more depth on some of the topics mentioned here!Thank you to everyone who contributed to this release by writing code and
documentation, reporting bugs, sharing feedback, and testing the release
candidates. Your efforts helped to ensure that Go 1.24 is as stable as possible.
As always, if you notice any problems, please file an issue.]]></content:encoded></item><item><title>Brave Now Lets You Inject Custom JavaScript To Tweak Websites</title><link>https://tech.slashdot.org/story/25/02/10/2348258/brave-now-lets-you-inject-custom-javascript-to-tweak-websites?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>dev</category><category>slashdot</category><pubDate>Mon, 10 Feb 2025 23:50:00 +0000</pubDate><source url="https://developers.slashdot.org/">Dev - Slashdot - Dev</source><content:encoded><![CDATA[Brave Browser version 1.75 introduces "custom scriptlets," a new feature that allows advanced users to inject their own JavaScript into websites for enhanced customization, privacy, and usability. The feature is similar to the TamperMonkey and GreaseMonkey browser extensions, notes BleepingComputer. From the report: "Starting with desktop version 1.75, advanced Brave users will be able to write and inject their own scriptlets into a page, allowing for better control over their browsing experience," explained Brave in the announcement. Brave says that the feature was initially created to debug the browser's adblock feature but felt it was too valuable not to share with users. Brave's custom scriptlets feature can be used to modify webpages for a wide variety of privacy, security, and usability purposes.
 
For privacy-related changes, users write scripts that block JavaScript-based trackers, randomize fingerprinting APIs, and substitute Google Analytics scripts with a dummy version. In terms of customization and accessibility, the scriptlets could be used for hiding sidebars, pop-ups, floating ads, or annoying widgets, force dark mode even on sites that don't support it, expand content areas, force infinite scrolling, adjust text colors and font size, and auto-expand hidden content.
 
For performance and usability, the scriptlets can block video autoplay, lazy-load images, auto-fill forms with predefined data, enable custom keyboard shortcuts, bypass right-click restrictions, and automatically click confirmation dialogs. The possible actions achievable by injected JavaScript snippets are virtually endless. However, caution is advised, as running untrusted custom scriptlets may cause issues or even introduce some risk.]]></content:encoded></item><item><title>CNL: Optimizing Kyverno policy enforcement performance for large clusters</title><link>https://www.youtube.com/watch?v=DWmCAUCs3bc</link><author>CNCF [Cloud Native Computing Foundation]</author><category>dev</category><category>k8s</category><enclosure url="https://www.youtube.com/v/DWmCAUCs3bc?version=3" length="" type=""/><pubDate>Mon, 10 Feb 2025 18:13:08 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">Dev - CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io]]></content:encoded></item><item><title>Undergraduate shows that searches within hash tables can be much faster</title><link>https://www.quantamagazine.org/undergraduate-upends-a-40-year-old-data-science-conjecture-20250210/</link><author>Jhsto</author><category>dev</category><category>hn</category><pubDate>Mon, 10 Feb 2025 17:05:09 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Together, Krapivin (now a graduate student at the University of Cambridge), Farach-Colton (now at New York University) and Kuszmaul demonstrated in a January 2025 paper that this new hash table can indeed find elements faster than was considered possible. ln so doing, they had disproved a conjecture long held to be true.“It’s an important paper,” said Alex Conway of Cornell Tech in New York City. “Hash tables are among the oldest data structures we have. And they’re still one of the most efficient ways to store data.” Yet open questions remain about how they work, he said. “This paper answers a couple of them in surprising ways.”Hash tables have become ubiquitous in computing, partly because of their simplicity and ease of use. They’re designed to allow users to do exactly three things: “query” (search for) an element, delete an element, or insert one into an empty slot. The first hash tables date back to the early 1950s, and computer scientists have studied and used them ever since. Among other things, researchers wanted to figure out the speed limits for some of these operations. How fast, for example, could a new search or insertion possibly be?The answer generally depends on the amount of time it takes to find an empty spot in a hash table. This, in turn, typically depends on how full the hash table is. Fullness can be described in terms of an overall percentage — this table is 50% full, that one’s 90% — but researchers often deal with much fuller tables. So instead, they may use a whole number, denoted by , to specify how close the hash table is to 100% full. If  is 100, then the table is 99% full. If  is 1,000, the table is 99.9% full. This measure of fullness offers a convenient way to evaluate how long it should take to perform actions like queries or insertions.Researchers have long known that for certain common hash tables, the expected time required to make the worst possible insertion — putting an item into, say, the last remaining open spot — is proportional to . “If your hash table is 99% full,” Kuszmaul said, “it makes sense that you would have to look at around 100 different positions to find a free slot.”In a 1985 paper, the computer scientist Andrew Yao, who would go on to win the A.M. Turing Award, asserted that among hash tables with a specific set of properties, the best way to find an individual element or an empty spot is to just go through potential spots randomly — an approach known as uniform probing. He also stated that, in the worst-case scenario, where you’re searching for the last remaining open spot, you can never do better than . for 40 years, most computer scientists assumed that Yao’s conjecture was true.Krapivin was not held back by the conventional wisdom for the simple reason that he was unaware of it. “I did this without knowing about Yao’s conjecture,”  he said. His explorations with tiny pointers led to a new kind of hash table — one that did not rely on uniform probing. And for this new hash table, the time required for worst-case queries and insertions is proportional to (log ) — far faster than . This result directly contradicted Yao’s conjecture. Farach-Colton and Kuszmaul helped Krapivin show that (log ) is the optimal, unbeatable bound for the popular class of hash tables Yao had written about.“This result is beautiful in that it addresses and solves such a classic problem,” said Guy Blelloch of Carnegie Mellon.“It’s not just that they disproved [Yao’s conjecture], they also found the best possible answer to his question,” said Sepehr Assadi of the University of Waterloo.  “We could have gone another 40 years before we knew the right answer.”]]></content:encoded></item><item><title>Show HN: Global 3D topography explorer</title><link>https://topography.jessekv.com/</link><author>jessekv</author><category>dev</category><category>hn</category><pubDate>Mon, 10 Feb 2025 15:54:45 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[
                Click catchements or regions on the map to render them in 3D.
                Learn more]]></content:encoded></item><item><title>Surnames from nicknames nobody has any more</title><link>https://blog.plover.com/lang/etym/nickname-names.html</link><author>JNRowe</author><category>dev</category><category>hn</category><pubDate>Mon, 10 Feb 2025 13:54:38 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[English has a pattern of common patronymic names.  For example, "John
Peters" and "John Peterson" are someone whose father was named
"Peter".  ("Peters" should be understood as "Peter's".)  Similarly we
have John Williams and John Williamson, John Roberts and John
Robertson, John Richards and John Richardson, John James and John
Jameson, John Johns and John Johnson, and so on.Often Dad's name was a nickname.  For example, a common nickname for
"John" is "Jack" and we have (less commonly) John Jacks and (more
commonly) John Jackson. John Bills and John Bilson, John Wills and
John Wilson, and John Willis and John Willison are Bill, Will, and
Wille, all short for William."Richard" is "Dick", and we have John Dicks (or Dix) and John Dickson
(or Dixon).  "Nicholas" is "Nick" and we have John Nicks (or Nix) and John
Nickson (or Nixon).Sometimes the name has the diminutive suffix “-kin” inserted.  Wilkins
is little Will's son, as is Wilkinson; Peterkins is little Peter's
son.These patterns are so common that if you find surnames that follow
them you can almost always infer a forename, although it may be one
that is no longer common, or that is spelled differently.  For
example, many people are named Pierce, Pearse, Pierson, or Pearson,
which is from the name Pierre, Piers or Pierce, still used in English
although much less common than in the past.  (It is from the same root
as Peter.)  Perkins is little Pierre.  
Robin used to be a nickname for
Robert (it's “Robkin” with the difficult “-bk-” simplified to just
“-b-”) and we have John Robins and John Robinson. Sometimes, the pattern is there but the name is unclear because it is
a nickname that is now so uncommon that it is neatly forgotten.  The
fathers of John Watts, Watson, and Watkins were called Wat, which used
to be short for Walter.  John Hobbs, John Hobson, and Hobkins are
named for Hob, which was short for Robert in the same way that Rob and
Bob are still.  (I had a neighbor who was called Hob, and told me his
family claimed that it was short for Robert, but that he wasn't sure.
I assured him that they were correct.)  “Daw”, an archaic nickname for
“David”, gives us Dawes, Dawkins, and Dawson.Back in September when I started this article I thought on John Gibbs
and John Gibson.  Who's named "Gib", and why?  Is it archaic nickname?
Yes!  It was short for Gilbert.  Then I forgot about the draft article
until today when I woke up wondering about John Simpson (and, I
realize now, John Simms and John Simkins). And it transpired "Sim" or
"Simme" was once a common nickname for Simon.I would welcome further examples.Vicki Rosenzweig reminds me that another lost nickname for Richard is
"Hick" (like "Rick" and "Dick") and from this we get Hicks, Hix,
Hickson, Hixon, Hickman, and (she informs me) "Hickmott"."-mott" perhaps indicates a male in-law, according to the Dictionary
of American Family Names via
Ancestrry.com.
The only other clear example of this that I noticed was "Willmott".
Note that Gaelic names like Dermott are not of this type; the
resemblance is coincidental.We get Hodge / Hodges / Hodgson / Hodgkins from "Hodge", an arcahic
nickname for Roger.Lucy Keer suggests that Bateson / Bates is from "Bat", an archair
nickname for Bartholomew.Christopher Taylor reminds me that in Atkins and Atkinson the "At-" is
short for "Adam", and   similarly in "Addis" and "Addison".  I know a
guy named Atteson but I'm not sure it is the same thing.Response to this article has been too much for me to keep up with."Dob" is another old nickname for "Robert", giving us "Dobson";"Dodge" another nickname for Roger, hence Dodgson, as in Louis Carrol's real name, Charles Dodgson;"Tibb" is an old nickname for Theobald, giving surnames like "Tibbs" and "Tibbets";"Hud" for "Hugh", giving us the Hudsons.I also asked Claude for suggestions:I'm going to paste a blog article that discusses common English
  surnames that are patronymic versions of forenames that no longer
  exist.  For example, the common surname "Watson" is from "Wat", and
  "Wat" was once a common nickname for "Walter", but no longer used.
  I would like you to list ten similar examples that are not mentioned
  in the article.Here are ten examples of common English surnames derived from obsolete
  forenames or nicknames, different from those mentioned in the article:Higgins/Higgs - from "Hick," a medieval pet form of RichardTimms/Timson - from "Tim," originally a medieval nickname for TimothyHodgson/Hodges - from "Hodge," once a common nickname for RogerAtkins/Atkinson - from "At" or "Atte," medieval diminutive of AdamParkins/Parkinson - from "Perkin," diminutive of Peter/PierreJudson/Judd - from "Judd," medieval nickname for JordanMalkins/Malkinson - from "Malk," diminutive of MalcolmLarkins/Larkin - from "Larry" or "Laurence," medieval form of LawrenceCollins/Colson - from "Col," medieval nickname for NicholasHuggins/Huggett - from "Hug," medieval pet form of HughRemember that some of these might be made up.  For example,
Wiktionary claims that #7 is ,
from , a nickname for Mary.  Who's right?  I don't know.  The
only way to settle this is with a cage match!  I hope.I complained that #2 was not to spec because “Tim” is still common,
and demanded a replacement:Rankins/Rankin - from "Ran," a medieval diminutive of RandolphI also asked Claude if I had made any errors, and it said:The article claims that "Robin" comes from "Robkin" with "bk"
  simplified to "b". This is incorrect."Robin" is actually a French
  diminutive of Robert, coming from Old French Robin/Rovin. …]]></content:encoded></item><item><title>Show HN: HTML visualization of a PDF file&apos;s internal structure</title><link>https://github.com/desgeeko/pdfsyntax/blob/main/docs/browse.md</link><author>desgeeko</author><category>dev</category><category>hn</category><pubDate>Mon, 10 Feb 2025 13:52:53 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[Hi,
I've just finished a rebuild of this function and added a lot of new features: info, page index, minimap, inverted index,... 
I think it may be useful for inspection, debugging or just as a learning resource showcasing the PDF file format.
This is a pet project and I would be happy to receive some feedback!
Regards]]></content:encoded></item><item><title>The hallucinatory thoughts of the dying mind</title><link>https://thereader.mitpress.mit.edu/the-hallucinatory-thoughts-of-the-dying-mind/</link><author>anarbadalov</author><category>dev</category><category>hn</category><pubDate>Mon, 10 Feb 2025 13:11:28 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Delirium is one of the most perplexing deathbed phenomena, exposing the gap between our cultural ideals of dying words and the reality of a disoriented mind.BeeLine Reader uses subtle color gradients to help you read more efficiently.When William Brahms’s anthology of last words, “Last Words of Notable People,” came out in 2010, he encountered readers who entertained themselves by speculating what their last words might be. Initially, Brahms said, they would propose something witty and profound. Later, they admitted that they’d likely express appreciation and love, or something spiritual — if they could even manage to speak, when lucidity and intelligibility become hurdles. Brahms witnessed this firsthand at the bedside of his dying mother. Even though she’d spent years helping him edit his book, they’d never discussed what they thought their last words would be.It didn’t matter, because the reality was quite different.“In her case, due to debilitating illness, it was not a clear punctuated statement,” Brahms told me in an email, with evident surprise. “It was more of a slow and fading dialogue.”This dissonance between the idealized notion of dying speech, based on people’s expectations and cultural ideals, and the reality of final moments so often complicates our understanding of death. And when delirium enters the equation, the gap between expectation and reality widens even further.What should we make of people who can speak and yet make no sense?On one hand, there’s a very good chance that a dying person will be delirious at the end of life. In fact, in palliative care and hospice spaces, 58 to 88 percent of cancer patients are delirious in the last week to hours before death. In William Osler’s landmark study of dying, 28 patients out of the total 486 were described as delirious, with his observers using words like “irrational,” “mind wandering,” “demented,” and in one case “raging.”“I don’t want to call it a nearly universal feature of the end-of-life experience,” David Wright, a Canadian medical ethnographer, told me, “but as you die, unless you die very suddenly in an instant, your various bodily systems start to work differently until they stop working at all. And that includes the way that you think, and that includes the way that you communicate.”In palliative care and hospice spaces, well over half of cancer patients are delirious in the last week to hours before death.Ancient medical writers distinguished between two types of delirium: , the restless variety, and , an inert, dull state. Modern researchers keep this distinction, describing delirium as either quiet, listless, apathetic, and “pseudo wakeful,” or as a restlessness in which patients are “muttering” and “talkative.” Delirium was one of the first medical conditions ever described by Greek and Roman writers almost 2,000 years ago. The Greek philosopher Celsus introduced the word  in the second century CE; the word came from the Latin , which means to go out of the furrow ( is Latin for “furrow”). The delirious plow leaves the furrow of sense. A modern person might think that leaving the planned track is a desirable thing, until they familiarize themselves with a single-bladed plow and realize the inconvenience of an errant ox-pulled plow skittering across the field’s surface.Despite this pedigree, the exact biological mechanisms behind delirium aren’t well understood, but it appears to stem from neuronal dysfunction, probably due to neurotransmitter fluctuations. Neurons in the brain aren’t dying (which is why sometimes people can recover from delirium) but disconnecting from each other. Basically, delirium is the result of a neurochemical commotion. Now clinicians look for three types of delirium: a restless, hyperactive form, a listless form, and a mix of the two.Delirium is very common among the dying, particularly in the later stages, where that lethargic type shows up often. As a diagnosis, it covers a complex of symptoms and isn’t a single thing. Despite its prevalence, doctors don’t reliably recognize delirium. It’s often mistaken for dementia, depression, or psychosis. In a study of 100 consecutive cases of delirium in a palliative care unit, researchers found that 33 percent of patients were classified with the hypoactive, lethargic form. They had the same impairment in cognitive functioning as patients with other variants, showing similar deficits in orientation, memory, and comprehension on cognitive test scores. This variegated presentation makes it hard to tell when someone is delirious. Though clinicians want to take it seriously, they don’t have a uniform method for recognizing delirium or dealing with it therapeutically.On the other hand, delirium seems to outstrip people’s ability — and their willingness — to grasp a phenomenon as simultaneously biological, emotional, and social. It has what David Wright called a relational dimension, in the sense that any individual’s delirium impacts other people’s perceptions of the relationship. Some find it traumatizing and distressing, others less so. In either case, what seems to help is when delirium is described as a normal part of dying, in the same way that baby babbling is described as a normal feature of language acquisition. Some medical staff try to normalize the delirium as part of the natural process of dying, and they encourage family members to enter the hallucinatory world — or at least not to fight it. Many hospices recommend the latter as well: “Do not contradict, explain away, belittle or argue about what the person claims to have seen or heard,” reads a short text that a hospice provides about the dying process. “Just because you cannot see or hear it does not mean it is not real to your loved one. Affirm his or her experience. They are normal and common.”By itself, the brain explanation isn’t soothing. What does help is taking advantage of the interaction window that remains. Or you open another window, as if that’s the one that the other person wants to operate with. You might respond, “So you say you’re going on a trip. Who do you think will be waiting for you?” Or, “Tell me some nice things you remember about your mother.” Family members might be told that the patient has already undergone a sort of social death; though their body is present, the previous person they is gone, so a new relationship is required. So while your father’s brain is the author of some insult, and his body its animator, your father as you knew him isn’t the principal of that offensive utterance.Some families take it better than others — some want to deny what’s happening, while others roll with it (“Oh, you’re seeing bugs? What’s it look like?”). For other family members, some meaning needs to be made from the delirium. “It can be very meaningful for family members to say, ‘Oh, Dad’s now with Mom,’ who is dead. That can be a great source of comfort,” a doctor who often treats delirium told me.I faced this with my own grandmother. I’d known my father’s mother, Norma, from childhood. A bright-faced, cheery woman, she’d fold me in her arms, say that she loved me, then laugh chirpingly and say that she prayed for me, that unmistakable Irish grandmother code for “I want you to go to Mass.” Now I was no longer a child, and now she was dying. The gray skies on our visit to the hospital will be forever seared in my memory. When my wife and I entered my grandmother’s room, I told her who I was. Her eyelids fluttered open, and her eyes focused on me, her gaunt face lit up, she reached out her arms, I hugged and kissed her. “Michael-Jean, you’re an angel!” she exclaimed. “You’re an angel!”At first I was concerned — does she know it’s me, not an angel? Apparently so, she said my name, then repeated joyfully, “You’re an angel, you’re an angel.” Delirium? Maybe. But when your beloved grandmother calls you an angel, you reach for the interpretation that feels most loving. That has to be her talking, she’s still there. She said what she said; the words, they’re right there, her last ones, as far as I’m concerned. She became unresponsive shortly after and died two days later.My father remembers this differently. He maintains that she roused to a saying of the Lord’s Prayer. But I won’t argue — from both of our stories, we end up with some morsels of closure, and that’s what feeds us.From a linguistic perspective, delirious language is unhinged and incoherent. People who are delirious after surgery and then recover can sometimes relate their feelings of being briefly outside the furrow of sense, which reduced them to a state of abject terror.Osler might have been surprised. “I remember that everything changed to me,” reported one patient. “Suddenly I was a prisoner in a Nazi camp, and I thought that the nurses were the Nazi camp guards . . . and I wondered whatever happened since the nurses had become so unkind to me although they were so nice before.” They reported seeing things that were beautiful, awe-inspiring, and pleasurable. They saw fragments of past events mixed up with the present. And they heard people talking to them but didn’t know what they wanted, nor could they communicate with them.As with all these phenomena, delirium involves experiences at the edge of the interaction window, or far beyond it. In many instances, that window has been shut, perhaps not forever. In the case above, family members who knew the person in a “normal” state were also shocked at losing emotional and cognitive access to their loved one. They missed knowing about the other person’s interior life and struggled to figure out if the person with delirium was comfortable or distressed. To deal with this, the advice seems to be: Don’t engage in the main interaction window, because your loved one’s not there; instead, open another interaction window on their terms. That will allow you to be present without expectations or demands.Of course, another option is to deny the existence of delirium altogether, as Maggie Callanan and Patricia Kelley’s 1992 book “Final Gifts” does. Their book proposed that dying people begin using a symbolic language particular to the end of life that must be carefully listened to, written down, and interpreted. “Health-care professionals and families may assume that what they’re hearing and seeing is confusion . . . unfortunately, dying people are often labeled ‘confused’ without adequate assessment,” Callanan and Kelley wrote. To those authors, the only reality is the special symbolic language of the dying, and the brain-based understandings of delirium diminish those messages or their meanings. (They don’t have a monopoly on this, by the way. Nearly 40 percent of Indian family members said that delirium was the cause of supernatural beliefs, emotional stress, or a failure of religious observance — and did not have biomedical origins.)Given the documented prevalence of delirium, it was a bold position to take. However, in a 1998 interview, Callanan walked the rejection back. “I’m not here to say that everything a confused patient says has great significance. It’s often very hard, even paying close attention, to find exactly what the person is saying. Seventy percent of people dying of illness, at some point, have some confusion; that’s a lot of confusion. However, our point is to always listen specifically to the words.”Several years later, on a speaking tour to Chicago, Callanan reversed again, telling the  that delirious behaviors aren’t the product of mental confusion or drug mis-dosing. “The confusion is ours,” she said. “The patient knows what’s going on.”The newspaper reporter also quoted, in contrast, the views of Joel Frader, a palliative care physician, who identified the source of hallucinations and other unusual behaviors as the failing brain. “There are some fairly characteristic changes in brain chemistry that people have documented as death occurs, whether it’s lack of oxygen or high levels of carbon dioxide or changes in nutrition getting to the brain,” he told the newspaper.If no one gets a clear model about the language they should produce at the end of their lives, except that it be pithy, poignant, and meaningful, then it risks ending up delirious and nonsensical.His take was not only frank and sharp, but it reiterated what Victorian doctors also warned: The existence of God or the afterlife won’t be proven with people’s dying words or their hallucinations, because they are “matters of faith.” The organic response of the brain to the body’s failure, on the other hand, is a matter of physiology. Implicit is the idea it works that way for everybody.“I do understand people trying to get some meaning out of everything they witness, but delirium is real, and it happens a lot,” Romayne Gallagher, a palliative care physician in British Columbia told me. She didn’t recommend “Final Gifts” to people because of its unrealistic view of delirium. “Certainly the misperception of reality plays a role in delirium — people seeing ants on the ceiling (spotted ceiling tiles) and seeing a bowl of rice (a glowing white lamp globe) — but if delirium goes beyond that, it seems to empty the limbic system’s treasure trove of horrors, which doesn’t seem to relate to the person’s experience. By that I mean people seeing a train coming at them or seeing a loved one on fire just outside the window.”My speculation is that the interpretive control promoted in “Final Gifts” reflects a cultural adaptation to dilemmas posed by delirium to a  approach to language at the end of life. That is, if no one gets a clear model about the language they should produce at the end, except for the vague sense that it should be pithy, poignant, and uniquely reflective of who they were, then it risks ending up delirious and nonsensical (if it exists at all). And if loved ones are directed to attend to all of the utterances, how should the non-sensical ones be interpreted? As if they contain some meaning.Meanwhile, other religious practices keep dying people occupied, authoring for them what to utter, chant, or pray. Sharing linguistic agency in this way looks like a wise cultural adaptation to the prevalence of delirium. In those traditions, only the prescribed words matter — everything else becomes noise, harmless and inconsequential.]]></content:encoded></item><item><title>Building a personal, private AI computer on a budget</title><link>https://ewintr.nl/posts/2025/building-a-personal-private-ai-computer-on-a-budget/</link><author>marban</author><category>dev</category><category>hn</category><pubDate>Mon, 10 Feb 2025 11:59:41 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[As everyone is well aware, the world is still going nuts trying to develop more, newer and better AI tools. Mostly by throwing absurd amounts of money at the problem. Many of those billions go towards building cheap or free services that operate at a significant loss. The tech giants that run them all are hoping to attract as many users as possible, so that they can capture the market, and become the dominant or only party that can offer them. It is the classic Silicon Valley playbook. Once dominance is reached, expect the enshittification to begin.A likely way to earn back all that money for developing these LLMs will be by tweaking their outputs to the liking of whoever pays the most. An example of what that such tweaking looks like is the refusal of DeepSeek's R1 to discuss what happened at Tiananmen Square in 1989. That one is obviously politically motivated, but ad-funded services won't exactly be fun either. In the future, I fully expect to be able to have a frank and honest discussion about the Tiananmen events with an American AI agent, but the only one I can afford will have assumed the persona of Father Christmas who, while holding a can of Coca-Cola, will intersperse the recounting of the tragic events with a joyful "Ho ho ho... Didn't you know? The holidays are coming!"But there is hope. One of the tricks of an upcoming player to shake up the market, is to undercut the incumbents by releasing their model for free, under a permissive license. This is what DeepSeek just did with their DeepSeek-R1. Google did it earlier with the Gemma models, as did Meta with Llama. We can download these models ourselves and run them on our own hardware. Better yet, people can take these models and scrub the biases from them. And we can download those scrubbed models and run those on our own hardware. And then we can finally have some truly useful LLMs.That hardware can be a hurdle, though. There are two options to choose from if you want to run an LLM locally. You can get a big, powerful video card from Nvidia, or you can buy an Apple. Either is expensive. The main spec that indicates how well an LLM will perform is the amount of memory available. VRAM in the case of GPUs, normal RAM in the case of Apples. Bigger is better here. More RAM means bigger models, which will dramatically improve the quality of the output. Personally, I'd say one needs at least over 24GB to be able to run anything useful. That will fit a 32 billion parameter model with a little headroom to spare. Building, or buying, a workstation that is equipped to handle that can easily cost thousands of euros.So what to do, if you don't have that amount of money to spare? You buy second-hand! This is a viable option, but as always, there is no such thing as a free lunch. Memory may be the primary concern, but don't underestimate the importance of memory bandwidth and other specs. Older equipment will have lower performance on those aspects. But let's not worry too much about that now. I am interested in building something that at least can run the LLMs in a usable way. Sure, the latest Nvidia card might do it faster, but the point is to be able to do it at all. Powerful online models can be nice, but one should at the very least have the option to switch to a local one, if the situation calls for it.Below is my attempt to build such a capable AI computer without spending too much. I ended up with a workstation with 48GB of VRAM that cost me around 1700 euros. I could have done it for less. For instance, it was not strictly necessary to buy a brand new dummy GPU (see below), or I could have found someone that would 3D print the cooling fan shroud for me, instead of shipping a ready-made one from a faraway country. I'll admit, I got a bit impatient at the end when I found out I had to buy yet another part to make this work. For me, this was an acceptable tradeoff.This is the full cost breakdown:Nvidia Tesla Cooling Fan KitMODDIY Main Power Adaptor CableAnd this is what it looked liked when it first booted up with all the parts installed:I'll give some context on the parts below, and after that, I'll run a few quick tests to get some numbers on the performance.The Z440 was an easy pick because I already owned it. This was the starting point. About two years ago, I wanted a computer that could serve as a host for my virtual machines. The Z440 has a Xeon processor with 12 cores, and this one sports 128GB of RAM. Many threads and a lot of memory, that should work for hosting VMs. I bought it secondhand and then swapped the 512GB hard drive for a 6TB one to store those virtual machines. 6TB is not required for running LLMs, and therefore I did not include it in the breakdown. But if you plan to collect many models, 512GB might not be enough.I have come to like this workstation. It feels all very solid, and I haven't had any problems with it. At least, until I started this project. It turns out that HP does not like competition, and I encountered some difficulties when swapping components.This is the magic ingredient. GPUs are expensive. But, as with the HP Z440, often one can find older equipment, that used to be top of the line and is still very capable, second-hand, for relatively little money. These Teslas were meant to run in server farms, for things like 3D rendering and other graphic processing. They come equipped with 24GB of VRAM. Nice. They fit in a PCI-Express 3.0 x16 slot. The Z440 has two of those, so we buy two. Now we have 48GB of VRAM. Double nice.The catch is the part about that they were meant for servers. They will work fine in the PCIe slots of a normal workstation, but in servers the cooling is managed differently. Beefy GPUs consume a lot of power and can run very hot. That is the reason consumer GPUs always come equipped with big fans. The cards need to take care of their own cooling. The Teslas, however, have no fans whatsoever. They get just as hot, but expect the server to supply a steady flow of air to cool them. The enclosure of the card is somewhat shaped like a pipe, and you have two options: blow in air from one side or blow it in from the other side. How is that for flexibility? You absolutely must blow some air into it, though, or you will damage it as soon as you put it to work.The solution is simple: just mount a fan on one end of the pipe. And indeed, it seems a whole cottage industry has grown of people that sell 3D-printed shrouds that hold a standard 60mm fan in just the right place. The problem is, the cards themselves are already quite bulky, and it is not easy to find a configuration that fits two cards and two fan mounts in the computer case. The seller who sold me my two Teslas was kind enough to include two fans with shrouds, but there was no way I could fit all of those into the case. So what do we do? We buy more parts.This is where things got annoying. The HP Z440 had a 700 Watt PSU, which might have been enough. But I wasn't sure, and I needed to buy a new PSU anyway because it did not have the right connectors to power the Teslas. Using this handy website, I deduced that 850 Watt would be sufficient, and I bought the NZXT C850. It is a modular PSU, meaning that you only need to plug in the cables that you actually need. It came with a neat bag to store the spare cables. One day, I might give it a good cleaning and use it as a toiletry bag.Unfortunately, HP does not like things that are not HP, so they made it difficult to swap the PSU. It does not fit physically, and they also changed the main board and CPU connectors. All PSUs I have ever seen in my life are rectangular boxes. The HP PSU also is a rectangular box, but with a cutout, making sure that none of the normal PSUs will fit. For no technical reason at all. This is just to mess with you.The mounting was eventually solved by using two random holes in the grill that I somehow managed to align with the screw holes on the NZXT. It sort of hangs stable now, and I feel lucky that this worked. I have seen Youtube videos where people resorted to double-sided tape.The connector required... another purchase.There is another issue with using server GPUs in this consumer workstation. The Teslas are intended to crunch numbers, not to play video games with. Consequently, they don't have any ports to connect a monitor to. The BIOS of the HP Z440 does not like this. It refuses to boot if there is no way to output a video signal. This computer will run headless, but we have no other choice. We have to get a third video card, that we don't to intent to use ever, just to keep the BIOS happy.This can be the most scrappy card that you can find, of course, but there is a requirement: we must make it fit on the main board. The Teslas are bulky and fill the two PCIe 3.0 x16 slots. The only slots left that can physically hold a card are one PCIe x4 slot and one PCIe x8 slot. See this website for some background on what those names mean. One cannot buy any x8 card, though, because often even when a GPU is advertised as x8, the actual connector on it might be just as wide as an x16. Electronically it is an x8, physically it is an x16. That won't work on this main board, we really need the small connector.Nvidia Tesla Cooling Fan KitAs said, the challenge is to find a fan shroud that fits in the case. After some searching, I found this kit on Ebay an bought two of them. They came delivered complete with a 40mm fan, and it all fits perfectly.Be warned that they make an awful lot of noise. You don't want to keep a computer with these fans under your desk.To keep an eye on the temperature, I whipped up this quick script and put it in a cron job. It periodically reads out the temperature on the GPUs and sends that to my Homeassistant server:In Homeassistant I added a graph to the dashboard that displays the values over time:As one can see, the fans were noisy, but not particularly effective. 90 degrees is far too hot. I searched the internet for a reasonable upper limit but could not find anything specific. The documentation on the Nvidia site mentions a temperature of 47 degrees Celsius. But, what they mean by that is the temperature of the ambient air surrounding the GPU, not the measured value on the chip. You know, the number that actually is reported. Thanks, Nvidia. That was helpful.After some further searching and reading the opinions of my fellow internet citizens, my guess is that things will be fine, provided that we keep it in the lower 70s. But don't quote me on that.My first attempt to remedy the situation was by setting a maximum to the power consumption of the GPUs. According to this Reddit thread, one can lower the power consumption of the cards by 45% at the cost of only 15% of the performance. I tried it and... did not notice any difference at all. I wasn't sure about the drop in performance, having only a couple of minutes of experience with this configuration at that point, but the temperature characteristics were definitely unchanged.And then a light bulb flashed on in my head. You see, just before the GPU fans, there is a fan in the HP Z440 case. In the photo above, it is in the right corner, inside the black box. This is a fan that sucks air into the case, and I figured this would work in tandem with the GPU fans that blow air into the Teslas. But this case fan was not spinning at all, because the rest of the computer did not need any cooling. Looking into the BIOS, I found a setting for the  of the case fans. It ranged from 0 to 6 stars and was currently set to 0. Putting it at a higher setting did wonders for the temperature. It also made more noise.I'll reluctantly admit that the third video card was helpful when adjusting the BIOS setting.MODDIY Main Power Adaptor Cable and Akasa Multifan AdaptorFortunately, sometimes things just work. These two items were plug and play. The MODDIY adaptor cable connected the PSU to the main board and CPU power sockets.I used the Akasa to power the GPU fans from a 4-pin Molex. It has the nice feature that it can power two fans with 12V and two with 5V. The latter obviously reduces the speed and thus the cooling power of the fan. But it also reduces noise. Fiddling a bit with this and the case fan setting, I found an acceptable tradeoff between noise and temperature. For now at least. Maybe I will need to revisit this in the summer.Inference speed. I collected these numbers by running  with the  flag and asking it five times to write a story and averaging the result:Performancewise,  is configured with:All models have the default quantization that  will pull for you if you don't specify anything.Another important finding: Terry is by far the most popular name for a tortoise, followed by Turbo and Toby. Harry is a favorite for hares. All LLMs are loving alliteration.Over the days I kept an eye on the power consumption of the workstation:Note that these numbers were taken with the 140W power cap active.As one can see, there is another tradeoff to be made. Keeping the model on the card improves latency, but consumes more power. My current setup is to have two models loaded, one for coding, the other for generic text processing, and keep them on the GPU for up to an hour after last use.After all that, am I happy that I started this project? Yes, I think I am.I spent a bit more money than planned, but I got what I wanted: a way of locally running medium-sized models, completely under my own control.It was a good choice to start with the workstation I already owned, and see how far I could come with that. If I had started with a new machine from scratch, it definitely would have cost me more. It would have taken me much longer too, as there would have been many more options to choose from. I would also have been very tempted to follow the hype and buy the latest and greatest of everything. New and shiny toys are fun. But if I buy something new, I want it to last for years. Confidently predicting where AI will go in 5 years time is impossible right now, so having a cheaper machine, that will last at least some while, feels satisfactory to me.I wish you good luck on your own AI journey. I'll report back if I find something new or interesting.]]></content:encoded></item></channel></rss>