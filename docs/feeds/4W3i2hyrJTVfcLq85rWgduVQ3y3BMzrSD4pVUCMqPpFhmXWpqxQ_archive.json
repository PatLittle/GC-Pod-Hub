{"id":"4W3i2hyrJTVfcLq85rWgduVQ3y3BMzrSD4pVUCMqPpFhmXWpqxQ","title":"Hacker News: Front Page","displayTitle":"HN Front","url":"https://hnrss.org/frontpage?points=75","feedLink":"https://news.ycombinator.com/","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":15,"items":[{"title":"NOAA's public weather data powers the local forecasts on your phone and TV","url":"https://theconversation.com/noaas-vast-public-weather-data-powers-the-local-forecasts-on-your-phone-and-tv-a-private-company-alone-couldnt-match-it-249451","date":1739309503,"author":"rntn","guid":242,"unread":true,"content":"<p><em>When a hurricane or tornado starts to form, your local weather forecasters can quickly pull up maps tracking its movement and showing where it’s headed. But have you ever wondered where they get all that information?</em></p><p><em>The forecasts can seem effortless, but behind the scenes, a vast network of satellites, airplanes, radar, computer models and weather analysts are providing access to the latest data – and warnings when necessary. This data comes from analysts at the <a href=\"https://www.noaa.gov/weather\">National Oceanic and Atmospheric Administration</a>, known as NOAA, and its National Weather Service.</em></p><p><em>Atmospheric scientists <a href=\"https://scholar.google.com/citations?user=eFsAAoAAAAAJ&amp;hl=en\">Christine Wiedinmyer</a> and <a href=\"https://cires.colorado.edu/people/kari-bowen\">Kari Bowen</a>, who is a former National Weather Service forecaster, explained NOAA’s central role in most U.S. weather forecasts.</em></p><h2>When people see a weather report on TV, what went on at NOAA to make that forecast possible?</h2><p>All of that information goes <a href=\"https://www.weather.gov/about/forecast-process\">into the agency’s computers</a>, which process the data to begin defining what’s going on in different parts of the atmosphere. </p><p>NOAA forecasters use computer models that simulate physics and the behavior of the atmosphere, along with their own experience and local knowledge, to start to paint a picture of the weather – what’s coming in a few minutes or hours or days. They also use that data to <a href=\"https://www.weather.gov/hun/climateforecast\">project seasonal conditions</a> out over weeks or months.</p><p>All of this analysis happens before the information reaches private weather apps and TV stations.</p><p>No matter who you are, you can freely access that data and the analyses. In fact, a large number of private companies use NOAA data to create fancy maps and other weather products that they sell.</p><p>It would be extremely difficult to do all of that without NOAA.</p><p>The agency operates a <a href=\"https://www.nesdis.noaa.gov/our-satellites/currently-flying\">fleet of 18 satellites</a> that are packed with instruments dedicated to observing weather phenomena essential to predicting the weather, from how hot the land surface is to the water content of the atmosphere. Some are <a href=\"https://www.nesdis.noaa.gov/our-satellites/currently-flying/geostationary-satellites\">geostationary satellites</a> which sit high above different parts of the U.S. measuring weather conditions 24/7. Others orbit the planet. Many of these are operated as part of partnerships with NASA or the Air Force.</p><p>Some private companies are starting to invest in satellites, but it would take an enormous <a href=\"https://spacesecurity.wse.jhu.edu/2024/06/23/next-generation-goes-satellites-to-be-developed-by-lockheed-martin\">amount of money</a> to replicate the <a href=\"https://www.nesdis.noaa.gov/our-satellites/currently-flying/geostationary-satellites/goes-r-series-spacecraft-instruments\">range of instrumentation</a> and coverage that NOAA has in place. Satellites only last so long and take time to build, so NOAA is continually <a href=\"https://www.nesdis.noaa.gov/our-satellites/future-programs/geostationary-extended-observations-geoxo\">planning for the future</a>, and using its technical expertise to develop new instruments and computer algorithms to interpret the data.</p><p>Maritime buoys are another measuring system that would be difficult to replicate. <a href=\"https://www.ndbc.noaa.gov/\">Over 1,300 buoys</a> across oceans around the world measure water temperature, wind and wave height – all of which are essential for coastal warnings, as well as long-term forecasts.</p><p>Weather observation has been around a long time. President Ulysses S. Grant created the <a href=\"https://www.weather.gov/timeline/\">first national weather service</a> in the War Department in 1870. It became a civilian service in 1880 under the Department of Agriculture and is now in the Commerce Department. The information its scientists and technologists produce is essential for safety and also benefits people and industries in a lot of ways. </p><h2>Could a private company create forecasts on its own without NOAA data?</h2><p>It would be difficult for one company to provide comprehensive weather data in a reliable way that is also accessible to the entire public.</p><p>Some companies might be able to launch their own satellite, but one satellite only gives you part of the picture. NOAA’s weather observation network has been around for a long time and collects data from points <a href=\"https://www.wrh.noaa.gov/map/?obs=true&amp;wfo=lox\">all over the U.S.</a> and <a href=\"https://www.ndbc.noaa.gov/\">the oceans</a>. Without that robust data, computer models and the broad network of forecasters and developers, forecasting also becomes less reliable.</p><p>Analyzing that data is also complex. You’re not going to be able to take satellite data, run a model on a standard laptop and suddenly have a forecast. </p><p>And there’s a question of whether a private company would want to <a href=\"https://youtu.be/-FQ3TOpkHhU?feature=shared\">take on the legal risk</a> of being responsible for the nation’s forecasts and severe weather warnings. </p><figure><figcaption></figcaption></figure><p>NOAA is <a href=\"https://crsreports.congress.gov/product/pdf/R/R48157\">taxpayer-funded</a>, so it is a public good – its services provide safety and security for everyone, not just those who can pay for it.</p><p>If weather data was only available at a price, one town might be able to afford the weather information necessary to protect its residents, while a smaller town or a rural area across the state might not. If you’re in a tornado-prone area or coastal zone, that information can be the difference between life or death.</p><h2>Is climate data and research into the changing climate important for forecasts?</h2><p>The Earth’s systems – its land, water and the atmosphere – are changing, and we have to be able to assess how those changes will impact weather tomorrow, in two weeks and far into the future.</p><p>Rising global temperatures <a href=\"https://theconversation.com/the-water-cycle-is-intensifying-as-the-climate-warms-ipcc-report-warns-that-means-more-intense-storms-and-flooding-165590\">affect weather patterns</a>. Dryness can <a href=\"https://theconversation.com/southern-california-is-extremely-dry-and-thats-fueling-fires-maps-show-just-how-dry-246983\">fuel wildfires</a>. Forecasts have to take the changing climate into account to be accurate, no matter who is creating the forecast.</p><p>Drought is an example. The dryness of the Earth controls how much water gets exchanged with the atmosphere to form clouds and rainfall. To have an accurate weather prediction, we need to know <a href=\"https://droughtmonitor.unl.edu/\">how dry things are</a> at the surface and how that has changed over time. That requires long-term climate information.</p><h2>NOAA doesn’t do all of this by itself – who else is involved?</h2><p>NOAA partners with private sector, academia, nonprofits and many others around the world to ensure that everyone has the best information to produce the most robust weather forecasts. Private weather companies and media also play important roles in getting those forecasts and alerts out more widely to the public.</p><p>A lot of businesses rely on accuracy from NOAA’s weather data and forecasts: aviation, energy companies, insurance, even modern tractors’ <a href=\"https://geoawesome.com/eo-hub/how-satellite-data-is-transforming-agriculture/\">precision farming</a> equipment. The agency’s long-range forecasts are essential for managing state reservoirs to ensure enough water is saved and to avoid flooding.</p><p>The government agency can be <a href=\"https://crsreports.congress.gov/product/pdf/IF/IF12667\">held accountable</a> in a way private businesses are not because it answers to Congress. So, the data is trustworthy, accessible and developed with the goal to protect public safety and property for everyone. Could the same be said if only for-profit companies were producing that data?</p>","contentLength":6324,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43018643"},{"title":"Thomson Reuters wins first major AI copyright case in the US","url":"https://www.wired.com/story/thomson-reuters-ai-copyright-lawsuit/","date":1739307381,"author":"johnneville","guid":241,"unread":true,"content":"<p>Thomson Reuters has <a data-offer-url=\"https://storage.courtlistener.com/recap/gov.uscourts.ded.72109/gov.uscourts.ded.72109.770.0.pdf\" data-event-click=\"{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://storage.courtlistener.com/recap/gov.uscourts.ded.72109/gov.uscourts.ded.72109.770.0.pdf&quot;}\" href=\"https://storage.courtlistener.com/recap/gov.uscourts.ded.72109/gov.uscourts.ded.72109.770.0.pdf\" rel=\"nofollow noopener\" target=\"_blank\">won</a> the first major AI copyright case in the United States.</p><p>In 2020, the media and technology conglomerate filed an unprecedented <a href=\"https://www.wired.com/story/ai-copyright-case-tracker/\">AI copyright lawsuit</a> against the legal AI startup Ross Intelligence. In the complaint, Thomson Reuters claimed the AI firm reproduced materials from its legal research firm Westlaw. Today, a judge ruled in Thomson Reuters’ favor, finding that the company’s copyright was indeed infringed by Ross Intelligence’s actions.</p><p>“None of Ross’s possible defenses holds water. I reject them all,” wrote US District Court of Delaware judge Stephanos Bibas, in a summary judgement.</p><p>Thomson Reuters and Ross Intelligence did not immediately respond to requests for comment.</p><p>The generative AI boom has led to a spate of additional <a href=\"https://www.wired.com/story/matthew-butterick-ai-copyright-lawsuits-openai-meta/\">legal fights</a> about how AI companies can use copyrighted material, as many major AI tools were developed by training on copyrighted works including books, films, visual artwork, and websites. Right now, there are several dozen lawsuits currently winding through the US court system, as well as international challenges in China, Canada, the UK, and other countries.</p><p>Notably, Judge Bibas ruled in Thomson Reuters’ favor on the question of fair use. The <a href=\"https://www.wired.com/story/andy-warhol-fair-use-prince-generative-ai/\">fair use doctrine</a> is a <a href=\"https://www.wired.com/story/battle-over-books3/\">key component</a> of how AI companies are seeking to defend themselves against claims that they used copyrighted materials illegally. The idea underpinning fair use is that sometimes it’s legally permissible to use copyrighted works without permission—for example, to create parody works, or in noncommercial research or news production. When determining whether fair use applies, courts use a four-factor test, looking at the reason behind the work, the nature of the work (whether it’s poetry, nonfiction, private letters, et cetera), the amount of copyrighted work used, and how the use impacts the market value of the original. Thomson Reuters prevailed on two of the four factors, but Bibas described the fourth as the most important, and ruled that Ross “meant to compete with Westlaw by developing a market substitute.”</p><p>Thomson Reuters spokesperson Jeffrey McCoy applauded the ruling in a statement emailed to WIRED. “We are pleased that the court granted summary judgment in our favor and concluded that Westlaw’s editorial content created and maintained by our attorney editors, is protected by copyright and cannot be used without our consent,” he wrote. “The copying of our content was not ‘fair use.’”</p><p>Even before this ruling, Ross Intelligence had already felt the impact of the court battle: The startup <a data-offer-url=\"https://www.lawnext.com/2020/12/legal-research-company-ross-to-shut-down-under-pressure-of-thomson-reuters-lawsuit.html\" data-event-click=\"{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.lawnext.com/2020/12/legal-research-company-ross-to-shut-down-under-pressure-of-thomson-reuters-lawsuit.html&quot;}\" href=\"https://www.lawnext.com/2020/12/legal-research-company-ross-to-shut-down-under-pressure-of-thomson-reuters-lawsuit.html\" rel=\"nofollow noopener\" target=\"_blank\">shut down</a> in 2021, citing the cost of litigation. In contrast, many of the AI companies still duking it out in court, like OpenAI and Google, are financially equipped to weather prolonged legal fights.</p><p>Still, this ruling is a blow to AI companies, according to Cornell University professor of digital and internet law James Grimmelmann: “If this decision is followed elsewhere, it's really bad for the generative AI companies.” Grimmelmann believes that Bibas’ judgement suggests that much of the case law that generative AI companies are citing to argue fair use is “irrelevant.”</p><p>Chris Mammen, a partner at Womble Bond Dickinson who focuses on intellectual property law, concurs that this will complicate AI companies’ fair use arguments, although it could vary from plaintiff to plaintiff. “It puts a finger on the scale towards holding that fair use doesn’t apply,” he says.</p><p><em>Update 2/11/25 5:09 ET: This story has been updated to include additional comment from Thomson Reuters.</em></p>","contentLength":3582,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43018251"},{"title":"The first yearly drop in average CPU performance in its 20 years of benchmarks","url":"https://www.tomshardware.com/pc-components/cpus/passmark-sees-the-first-yearly-drop-in-average-cpu-performance-in-its-20-years-of-benchmark-results","date":1739304013,"author":"LorenDB","guid":240,"unread":true,"content":"<p><a data-analytics-id=\"inline-link\" href=\"https://www.tomshardware.com/tag/benchmark\" data-auto-tag-linker=\"true\" data-before-rewrite-localise=\"https://www.tomshardware.com/tag/benchmark\">Benchmarking</a> software developer PassMark publishes the average results of all Windows PC tests across the globe every two weeks in a line graph. In line with what many enthusiasts might expect, the <a data-analytics-id=\"inline-link\" href=\"https://www.cpubenchmark.net/year-on-year.html\" data-url=\"https://www.cpubenchmark.net/year-on-year.html\" target=\"_blank\" referrerpolicy=\"no-referrer-when-downgrade\" data-hl-processed=\"none\">PassMark graph</a> has always shown a consistent increase in processor performance year-on-year. However, for the first time since the company started keeping track in 2004, the average CPU mark score for desktop and laptop processors has dropped, with laptops dropping 3.4% year-over-year.</p><p>We see the biggest drop in laptop CPU performance results. PassMark recorded an average result of 14,632 across 101,316 samples last year. But, in 2025, the average score sat at an average of 14,130 points between 25,541 samples, decreasing the average score by 3.4%.</p><p>The average desktop PC result in 2024 netted 26,436 points for 186,053 samples. But for 2025, the average score currently sits at 26,311 points for over 47,810 samples — a 0.5% drop from last year. While that drop is small, we should only see a continued progression of faster performance.</p><p>We’ve also seen results for the top-performing CPUs, and it seems that we’ve basically reached a performance plateau, will little to no uplift in PassMark scores for the past three years for desktop chips and laptop CPUs. This happened after we received a massive uplift of 58.6% in the Top Desktop CPU benchmark scores in 2023, with the introduction of the AMD Ryzen Threadripper Pro 7995WX. The arrival of the AMD Ryzen 9 7945HX3D laptop CPU in the same year also delivered a 69.9% jump in average performance for laptops. However, a new desktop chip that will dethrone the Threadripper Pro 7995WX is yet to arrive, while Intel’s new Core Ultra 275HX has provided a measly 6.8% increase in performance points on mobile.</p><p>What’s quite baffling, though, is that AMD, Intel, and even Qualcomm have just released new desktop and laptop CPUs that should have increased performance levels, according to their marketing. While it’s true that the <a data-analytics-id=\"inline-link\" href=\"https://www.tomshardware.com/pc-components/cpus/amd-updates-zen-5-ryzen-9000-benchmark-comparisons-to-intel-chips-details-admin-mode-boosts-chipset-driver-fix\" data-before-rewrite-localise=\"https://www.tomshardware.com/pc-components/cpus/amd-updates-zen-5-ryzen-9000-benchmark-comparisons-to-intel-chips-details-admin-mode-boosts-chipset-driver-fix\">AMD Ryzen 9000 series</a> (except for the new X3D chips) and <a data-analytics-id=\"inline-link\" href=\"https://www.tomshardware.com/pc-components/cpus/intel-launches-arrow-lake-core-ultra-200s-big-gains-in-productivity-and-power-efficiency-but-not-in-gaming\" data-before-rewrite-localise=\"https://www.tomshardware.com/pc-components/cpus/intel-launches-arrow-lake-core-ultra-200s-big-gains-in-productivity-and-power-efficiency-but-not-in-gaming\">Intel Arrow Lake Core Ultra 200S</a> processors were a disappointment for many enthusiasts, these should have at least delivered a modest increase in performance, particularly in productivity applications.</p><p>Passmark itself <a data-analytics-id=\"inline-link\" href=\"https://x.com/PassMarkInc/status/1889070672883941789?s=31\" data-url=\"https://x.com/PassMarkInc/status/1889070672883941789?s=31\" target=\"_blank\" referrerpolicy=\"no-referrer-when-downgrade\" data-hl-processed=\"none\">mused on X (formerly Twitter)</a> that it could be that people are switching to more affordable machines that deliver lower power and performance. Or maybe <a data-analytics-id=\"inline-link\" href=\"https://www.tomshardware.com/tag/windows-11\" data-auto-tag-linker=\"true\" data-before-rewrite-localise=\"https://www.tomshardware.com/tag/windows-11\">Windows 11</a> is depressing performance scores versus Windows 10, especially as people transition to it with the upcoming demise of the latter. We've certainly seen plenty of examples of reduced performance in gaming with some of the newer versions of Windows 11, particularly as Intel and AMD struggled to upstream needed updates into the OS.</p><p>At the moment, we don’t know the cause of this drop, but it could also be that we’re just in the first quarter of 2025. As more people get newer gear and run tests for the remainder of the year, this number could climb and reflect the performance of newer chips that arrived recently, or will arrive in the coming months.</p><p>PassMark also muses that bloatware could contribute to the sudden decline in performance, but that seems like a longshot. In the end, the decline could simply be due to an odd interaction between the benchmark itself and the latest versions of Windows 11.</p>","contentLength":3337,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43017612"},{"title":"DeepScaleR: Surpassing O1-Preview with a 1.5B Model by Scaling RL","url":"https://pretty-radio-b75.notion.site/DeepScaleR-Surpassing-O1-Preview-with-a-1-5B-Model-by-Scaling-RL-19681902c1468005bed8ca303013a4e2","date":1739303940,"author":"sijuntan","guid":239,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43017599"},{"title":"The subtle art of designing physical controls for cars","url":"https://www.theturnsignalblog.com/the-subtle-art-of-designing-physical-control-for-cars/","date":1739301302,"author":"vsdlrd","guid":238,"unread":true,"content":"<p>I focused on the fundamental goal: making passengers comfortable with minimum interactions. I discovered thermal comfort depends on four environmental factors: air temperature, heat radiation, airflow, and humidity. More importantly, staying within a specific range of these factors creates a comfortable environment for most people. This makes it possible to rely heavily on automation.</p><p>I created an automated system controlled by a temperature dial. This, in turn, determines the fan speed and seat heating. If the cabin temperature deviates significantly from the set temperature, the system will increase the fan speed and seat heating or cooling accordingly.</p><p>This led me to create the first design concept that used a mixed physical/touch interface. While the automated system sets the fan speed and seat heating, the driver can always override this.</p><p>For the second iteration, I decided to remove the seat heating from the automated system as this is a highly personal feature that is better controlled individually. I also introduced a dial design that would sit on top of the touch display so the touch-sensitive center is used to temporarily override the automated system:</p><p>While the concept showed promise, it remained theoretical. So, it was time to pick it back up and build a prototype.</p><p>My initial plan was to mount a dial onto a touch display. Having tested various implementations, I discovered this technology is not good enough. Registering accurate touch events is tricky, and it renders the space around the dial useless as it has to remain empty to prevent accidental touch interactions when rotating the dial. So, instead of bringing a dial to the display, I decided to bring the display to the dial.</p><p>That's when I came across <a href=\"https://github.com/scottbez1/smartknob\">Scott Bezek's open-source Smart Knob project</a>. He used a brushless DC motor to emulate an analog dial. By playing with the force and resistance of the motor, it's possible to create fake detents that are totally controlled by software. This means you can simulate different types of haptic feedback, like different detent strengths and hard stops. Additionally, it has a vibration motor that simulates a button press when pushing down on the dial. Combined with a small display, it creates a fully customizable physical control that can simulate almost any type of interaction with a physical dial.</p><p>After gaining significant attention in the maker community, <a href=\"https://store.seedlabs.it/products/smartknob-devkit-v0-1\">Seedlabs</a> turned it into a pre-fabricated development kit. Included are a couple of examples that show off the capability of the device.</p><p>It was the perfect starting point for my exploration, so I ordered one and got to work.</p><p>A rotary dial like this may look simple from a design perspective. But there is so much to explore! The fact that you can control everything about the software and haptic feedback is a fantastic opportunity to figure out what types of interactions are possible.</p><p>When you see any physical control, you automatically get certain expectations from its physical properties like size, shape, and weight. These 'affordances,' as they are called, tell you how the object can be used. For example, a round dial communicates that it can be rotated. A big dial communicates that it controls important functions requiring more precision, whereas a small one controls fewer, less essential functions. Similarly, signifiers, such as labels, can explain something about the function of the control, the number of steps, or the state.</p><p>Similarly, the haptic feedback from rotating the dial is a communication layer. When a dial controls different settings, such as the media source, it often has hard detents to indicate the significance of the change. The detents are smaller if the dial controls different values within a single feature, such as volume.</p><p>I did a couple of experiments to better understand these properties of rotary dials. For example, I tried implementing a simple volume control with three different haptic patterns:</p><h4>1. Smooth, linear resistance</h4><p>This felt lifeless and artificial, even though it was technically the most precise.</p><h4>2. Subtle detents at regular intervals</h4><p>This is the most common way to handle a control like volume; therefore, it felt the most natural.</p><h4>3. Dynamic resistance that increased in force with volume level</h4><p>Increasing the resistance proportionally to the volume is an interesting concept that is hard to create with an analog dial. It, therefore, feels unusual, especially for a typical control like volume, but it has potential in other application areas.</p><p>Based on my experiments, I created a set of guidelines for designing haptic interfaces with rotary inputs:</p><h2>Design Guidelines for Haptic Interfaces</h2><h3>Keep haptic patterns consistent across similar operations</h3><p>Much like a door handle should communicate whether to pull or push a door, the dial sets expectations in our minds of how it feels and how heavy the rotation is. The haptic feedback should match that, and the interaction between similar operations should not be mixed. If volume and fan speed have the same type of value range, do not change the interaction pattern. Don't use a continuous rotation for one and a stepped rotation for the other.</p><h3>Allow for both precise and rapid adjustments</h3><p>Certain features, like volume, allow two different behaviors. During regular usage, passengers adjust the volume in small steps to match their preferences. However, on occasion, the passengers need to mute volume quickly. Both options should be possible. Another type of function where this is common is one with an off state. In the example below, the first position of the dial is the off state. Rotating the dial fully to the left instinctively means turning the function off.</p><h3>Synchronize physical and visual feedback</h3><p>Matching the digital interface to the physical rotation of the dial is crucial. This comes down to two things. The first is matching the total rotation of the dial to the interface. If the dial can rotate 270°, the interface must also be 270°. The second is to match the detent's position to the interface's position. This is trickier than it seems. For example, if the total range of the dial is 180° and there are 6 detents, each detent is 30°. You would expect the UI to update between each detent. However, this leads to the UI feeling out of sync. Therefore, it's better to delay updating the UI. In my experiments, the UI updates when the dial is 20% past the detent, giving a more natural feeling.</p><h3>Scale detent strength inversely with value range</h3><p>The detents should be subtle if the data ranges from [0,99]. If the range is small, like [0,3], detents have to be stronger to clearly communicate the position of the dial.</p><h3>Place stronger detents at significant values</h3><p>To allow for finer communication, it can help to differentiate between major and minor values through detent strength. It adds a new layer of communication to the haptic feedback that is easy to understand. For example, temperature values with a decimal value.</p><h3>Vary resistance and step size to indicate extreme values</h3><p>Another way to add a layer of communication to the haptic feedback is by increasing the force for extreme values. This indicates that the consequences of this action are more intense than those of normal values. For example, in an automated climate mode, setting a temperature that varies significantly from the current cabin temperature will result in the system taking more serious action to achieve this temperature.</p><p>This is especially helpful in situations where the extreme values are dynamic. In my automated climate system, the cabin temperature isn't always the same; therefore, the same set temperature can have different consequences depending on how close the cabin temperature is to the set temperature.</p><p>Another way of achieving this is by increasing the number of degrees for an important step. For example, an 'off' mode can be larger than the steps for the on mode.</p><h3>Add subtle \"preview\" resistance before state changes</h3><p>The curve of the force should not be linear but logarithmic. This way, resistance increases the closer you move to the detent. This makes it clear when precisely the step is triggered, and you can feel it when you get close to the next step.</p><p>After establishing the design principles, I implemented the concepts I created previously. I created a fake automated system with all three functions: temperature, fan speed, and seat heating. In my previous article, I concluded that adding seat heating to the automated system wasn't suitable as it is a highly personal function. This remains true, but I wanted to include it in the dial to explore whether controlling three different functions through one dial is possible.</p><p>For temperature control, I added progressive haptic resistance to communicate the magnitude of changes. The further you adjust from the current temperature, the more resistance you feel, as setting a significantly higher temperature will increase the fan speed and seat heating.</p><p>Fan speed and seat heating received the same haptic profiles - fan speed uses five clear steps, whereas seat heating uses 4, with the first one being the \"off\" position with stronger feedback. By pressing down on the dial, you can cycle through the functions.</p><p>I highlight the active function through a small paginator at the bottom of the display. However, it should also communicate the relationship between the functions in the automated system. When fan speed and seat heating change after setting a temperature, the driver should be aware of that without having to cycle through the functions. I explored this further in Figma:</p><p>Being able to modify the angle, force, and detents of the dial through software is amazing and gave me a better understanding of the effect of different kinds of haptic feedback on usability. It's fun and insightful to update the different values until they are perfect.</p><p>Showing three different data types in one dial is possible but definitely the maximum. When adding a fourth function, keeping track of your position in the interface without looking down becomes too difficult. One of the main challenges is the small display that needs to show a lot of information. Since I created an automated system in which all three data types are linked, it is challenging to communicate this link through a tiny display, especially if it's intended to be used while driving. It becomes a lot easier if there are only two functions — temperature and fan speed. This makes more sense conceptually, and there is now enough space in the interface to clearly communicate the status of the automated system.</p><p>This setup is also my recommendation after all my experiments. A rotary dial with two functions is easy to understand and operate. Relying on an automated system keeps the number of interactions to a minimum, allowing the driver to override it easily if needed. Ideally, seat heating is also a physical control that enables passengers to set their preferred setting with one button press instead of cycling through the strength with multiple presses.</p><h2>Example of implementations today</h2><p>I want to highlight two carmakers with similar interesting solutions. The first is Jaguar, who had a clever solution for a dial with three functions by adding a depth dimension.</p><p>By default, the dial controls temperature. Pushing down on the dial activates the seat heating, and pulling up on the dial activates the fan speed. It's easy to learn and operate while keeping your eyes on the road. Unfortunately, like most carmakers, Jaguar has discontinued physical climate controls in favor of touch screens.</p><p>The second carmaker is Skoda. They have an interesting concept with three 'Smart Dials' in current higher-end models. Each passenger has a dial to control the temperature, and by pressing down, they control the seat heating. The driver can configure the middle dial to control up to 6 different functions, such as volume, drive mode, fan speed, and air direction. It's a simple and superb design that deserves more praise, especially considering today's trend of touch interfaces.</p><p>In my <a href=\"https://www.theturnsignalblog.com/the-rise-of-touch-screens-in-cars-explained/\">most popular article</a>, I explain the rise of touch screens in cars. Touch screens are a necessary evil, especially for more complicated interactions such as navigation. However, frequent, simpler interactions like climate controls should not be in a touch interface.</p><p>The often-cited reason for this is cost. However, surprisingly, it's often the budget brands like Skoda and Renault that offer physical controls today, showing that it's not simply a matter of cost, but of priority. Carmakers with touch-only interfaces prioritize cost and marketing over ergonomics and safety.</p><p>There is an inherent satisfaction and quality in operating physical controls. For years, brands like Mercedes prided themselves on spending thousands of hours perfecting the feel of their switches and buttons. The feeling of operating a physical control gives the car an inherent quality and character. This feeling is gone with touch screens and, therefore, in most modern vehicles.</p><p>I want to see more carmakers bring back physical controls and consider them a prominent part of the in-car experience. As I've demonstrated with this project, there is a lot to explore when designing physical controls, and I hope my project will inspire others to do the same. You can find the Seedlabs developer kit <a href=\"https://store.seedlabs.it/products/smartknob-devkit-v0-1\">here</a> and I have made my code public on my <a href=\"https://github.com/casperkessels/smartknob-firmware\">GitHub</a>.</p><div><p>Get notified of new posts</p><p for=\"email\">Don't worry, I won't spam you with anything else</p></div>","contentLength":13411,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43017010"},{"title":"Japan can be a science heavyweight once more if it rethinks funding","url":"https://www.nature.com/articles/d41586-025-00394-8","date":1739298613,"author":"rntn","guid":237,"unread":true,"content":"<p>From CRISPR gene editing to protein-structure predictions driven by artificial intelligence, great innovations stem from interdisciplinary research. Solutions to climate change, biodiversity loss, health inequities and other global crises will also rely on insights that bridge many fields.</p><p>Yet interdisciplinary research is still sidelined in many countries. Even though these papers attract more citations and have more impact on research, interdisciplinary research proposals tend to be less likely to receive funding than are those with a narrower scope.</p><p>Some countries have adjusted their research funding strategies accordingly. For example, between 2016 and 2018, UK research councils awarded 30% more grants to interdisciplinary investigators than they had a decade earlier, with 44% of funded projects in 2018 spanning at least two research subjects. A similar move has been made in the United States: between 2015 and 2020, university departments that submitted some interdisciplinary grant proposals received almost five times more funding — in terms of total and individual awards — from the National Institutes of Health (NIH) and National Science Foundation (NSF) than did those that submitted in only one discipline.</p><p>Alas, this isn’t the case in Japan. The nation’s funding agencies still mostly support research in tight disciplinary boundaries, such as engineering or chemistry. Specialists who evaluate these grant proposals <a href=\"https://www.nature.com/articles/d41586-023-03290-1\" data-track=\"click\" data-label=\"https://www.nature.com/articles/d41586-023-03290-1\" data-track-category=\"body text link\">tend to favour work in fields that they are familiar with</a> over interdisciplinary studies they do not understand well.</p><p>This narrow approach is leading to substantial underfunding of interdisciplinary research in Japan. It also means that the nation is missing out on breakthroughs. Japan’s natural resources are limited, and its economy has long relied on science and technology. The decline in its research and innovation is unmistakable, as indicated by the country’s share of the world’s top 10% of most-cited research articles, which has dropped from 6% to 2% over the past two decades or so.</p><p>As scientists and engineers based in Japanese research institutions, we urge the government and funding agencies to do more to support interdisciplinary research — or risk eroding the country’s global standing in science and its economy. Here we highlight five directions to foster such a shift.</p><h2><b>Fund people, not projects</b></h2><p>We argue that Japanese funding agencies should pivot from funding projects to supporting talented researchers. Such a model has proven to be effective in leading institutions worldwide. For example, the Howard Hughes Medical Institute’s (HHMI) Investigator Program provides outstanding scientists with flexible, long-term funding to pursue high-impact biomedical research — some US$11 million over a seven-year term, which can be renewed. This approach has led to groundbreaking discoveries, including the mechanistic understanding of circadian rhythms, the computational prediction of protein folding and the discovery of RNA interference.</p><p>Similarly, in Germany, the Max Planck Society prioritizes curiosity-driven research that has long-term objectives over immediate applications. By fostering interdisciplinary collaboration and providing institute directors with generous internal funding, free from the continual need to secure external grants, this strategy has enabled breakthroughs including the development of CRISPR–Cas9 as a gene-editing tool and structural insights into ribosomes.</p><p>Japan does have some programmes to support talented researchers, but these are too timid. The Japan Science and Technology Agency (JST) and the Japan Society for the Promotion of Science (JSPS), for example, dedicate some funds to basic science and some to curiosity-driven research. But the JST adopts a top-down approach, which often favours research on trending topics and can miss more-original ideas or projects . And the annual budget for the JSPS’s main grant programme, the Grants-in-Aid for Scientific Research (KAKENHI), has remained stagnant for the past decade. Adjusted for inflation and the weakening yen, the average funding per project has halved since 2013.</p><p>However, one Japanese institution has adopted researcher-focused funding with great success: the Okinawa Institute of Science and Technology Graduate University (OIST). Unlike national universities, which are funded by the Ministry of Education, Culture, Sports, Science and Technology, OIST is a private university that is funded by Japan’s Cabinet Office. Launched in 2011, it has a horizontal organizational structure and a strong commitment to diversity and interdisciplinary collaboration (see ‘Steps to success’). OIST is now ranked as <a href=\"https://www.nature.com/articles/d42473-021-00382-2\" data-track=\"click\" data-label=\"https://www.nature.com/articles/d42473-021-00382-2\" data-track-category=\"body text link\">Japan’s leading research institute</a> by the Nature Index, and around 20% of its publications involve contributions across more than one discipline. More Japanese institutions should follow its lead.</p><div><div><p>At the Okinawa Institute of Science and Technology Graduate University (OIST), faculty members receive core funding for five years — with flexibility in what to do with it. Unlike many other universities, in which laboratories must cover the costs of core facilities and technical staff, access to these resources at OIST is provided free of charge. This fosters collaboration and enables high-risk, high-reward projects. A review of each faculty member every five years ensures accountability, with continued support granted only to those who demonstrate significant achievements.</p><p>OIST also operates without conventional departments, helping scientists from different fields to collaborate. The buildings host open spaces, lounges, shared kitchens and open desk areas that encourage casual interactions. PhD students must also rotate through multiple labs during their first year to widen their horizons. These rotations facilitate the exchange of ideas and technologies, embedding interdisciplinary collaboration into the fabric of the institution.</p><p>Although further progress is needed, including in terms of gender and disability, OIST fosters a greater diversity in its community than do other Japanese institutions. This is partly because it attracts scholars from around the world. In 2024, more than 60% of faculty members and 80% of students at OIST were not Japanese citizens, bringing a dynamic mix of perspectives and expertise.</p></div></div><h2><b>Embrace high-stakes projects</b></h2><p>Interdisciplinary research is often perceived as riskier than more conventional, single-discipline work. This might be because it can be hard to learn concepts and methods, and even communicate efficiently, across disciplines. And priorities between fields can conflict.</p><p>To overcome these challenges, Japanese funding agencies should consider adopting a ‘high risk, high impact’ funding model, similar to that of the US Defense Advanced Research Projects Agency (DARPA), which anticipates a success rate of just 50%. They should recognize that even ‘unsuccessful’ high-risk projects can generate valuable knowledge, contributing to broader scientific and technological advancements. The DARPA funding model has inspired others, such as the Advanced Research Projects Agency for Health (ARPA-H), launched in the NIH to drive biomedical breakthroughs.</p><p>Another example is the Chan Zuckerberg Biohub in California, which supports interdisciplinary teams comprising biologists, engineers and data scientists to work on ambitious goals, such as the eradication of infectious diseases. It <a href=\"https://www.nature.com/articles/nature.2017.21440\" data-track=\"click\" data-label=\"https://www.nature.com/articles/nature.2017.21440\" data-track-category=\"body text link\">provides substantial funding over several years</a> to enable teams to pivot research directions in response to emerging challenges. Success is assessed through regular milestone-based evaluations. Projects that show limited potential are promptly terminated, ensuring that resources can be reallocated to more-promising endeavours.</p><p>In Japan, we are not advocating for existing funds to be redistributed. Rather, Japanese policymakers should allocate money to high-risk, high-reward projects from a separate pot, managed through dedicated DARPA-like programmes.</p><p>Initiatives such as the JST’s Diversity and Inclusiveness programme aim to lessen the homogeneity of the research workforce — for example, through awards and networking opportunities for women in science and by providing support or extensions for life events as well as for childcare and caregiving commitments — which tend to fall mostly on women (see page 295). This is laudable, but these efforts should be more widespread: they should be extended across genders and also include funding agency officers, programme managers and reviewers.</p><figure><picture><img alt=\"The Japan Agency for Marine-Earth Science and Technology lifts a research submersible onto a ship.\" loading=\"lazy\" src=\"//media.nature.com/lw767/magazine-assets/d41586-025-00394-8/d41586-025-00394-8_50602814.jpg\"><figcaption></figcaption></picture></figure><p>Japan’s funding agencies should also ensure that their own decision makers are less homogeneous in terms of their discipline, background, gender, age, nationality and culture. This would help to favour interdisciplinary research, which by nature weaves different approaches together and can benefit from being planned, conducted and assessed by a diverse community.</p><p>The role of immigration in boosting diversity should also be recognized.</p>","contentLength":8989,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43016353"},{"title":"Reviving the joy and honor of working with your hands (2015)","url":"https://richmond.com/holmberg-reviving-the-joy-and-honor-of-working-with-your-hands-will-strengthen-our-nation/article_d8130166-855d-53b6-94e1-cb735edcd7cc.html","date":1739298183,"author":"skadamat","guid":236,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43016248"},{"title":"Launch HN: A0.dev (YC W25) – React Native App Generator","url":"https://news.ycombinator.com/item?id=43015267","date":1739293715,"author":"sethburster","guid":235,"unread":true,"content":"Hi HN — we’re Seth and Ayo and we’re building a0.dev (<a href=\"https://a0.dev\">https://a0.dev</a>). a0.dev is a platform built to cut down React Native development time from weeks to just a few hours.<p>We’ve been building mobile apps together for seven years and have had several successes. One thing that’s always bothered us though is how much harder it is to build a mobile app than a website. If you’ve built an app with React Native before you’re familiar with the pains of working with both Xcode and Android Studio, writing tons of boilerplate before you can even start making the app, setting up state management, and of course going through the dreaded app review. We decided to build a platform that would make the app development process faster.</p><p>We’ve seen the success of new code-gen platforms like v0 and wanted something for React Native that goes further. We built an AI app generator that takes a user's prompt and creates a custom React Native app with an instant live preview.</p><p>a0.dev is great for quickly prototyping components and screens in React Native and users are able to copy the code from our generator into their preferred development environment. Our landing page has a couple of example prompts, but we encourage you to get creative when trying it out. We’ve had success generating not only functional screens like an Instagram Feed but also 2d games like Minesweeper or Flappy Bird.</p><p>Our chat has a “UI Expert” and “Advanced Logic” model users can switch between depending on the task at hand. Users can upgrade from working on a single screen to creating a full app by clicking on the “Need a Full App” button in the top right of the page. This changes the scope from a standalone chat with a single file to a full project that can include multiple chats and files. We launched an IOS app that users can download in order to preview the app on a physical device. We find that many apps look and feel better on a physical device so we recommend trying it out.</p><p>Our goal is to continue to improve the app generator while adding more features to help developers get their apps to the app store and make money from those apps. The main features on our roadmap right now are a Supabase integration and a “one click submit” button to let developers publish their app to the App Store.</p><p>There are a few limitations to note. We’re working on releasing our Android app, but Android users should be able to preview their app using the Expo Go App. The app is running React Native Web in the browser so any dependencies that don’t support web won’t work with the web preview but should work on the phone. There are also some dependencies that our system can’t handle because they require native modules that aren’t packaged into our app currently.</p><p>We hope you guys will check it out and try making an app with a0.dev. We’re available on Discord around the clock to help developers with any problems they may face and want to guide people to actually releasing their app on the App Store. Let us know what features you’d like to see and any problems you’ve faced building apps, we’d love to hear about your experience.</p><p>We dropped the need to sign up for the first message so you can just jump in and try it out.</p><p>Looking forward to your thoughts!</p>","contentLength":3274,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43015267"},{"title":"LLMs can teach themselves to better predict the future","url":"https://arxiv.org/abs/2502.05253","date":1739292020,"author":"bturtel","guid":234,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43014918"},{"title":"Intel's Battlemage Architecture","url":"https://chipsandcheese.com/p/intels-battlemage-architecture","date":1739289659,"author":"ksec","guid":233,"unread":true,"content":"<p>Intel’s Alchemist architecture gave the company a foot in the door to the high performance graphics segment. The Arc A770 proved to be a competent first effort, able to run many games with credible performance. Now, Intel is passing the torch to a new graphics architecture, named Battlemage.</p><p>Like Alchemist, Battlemage targets the midrange segment. It doesn’t try to compete with AMD or Nvidia’s high end cards. While it’s not as flashy as Nvidia’s RTX 4090 or AMD’s RX 7900 XTX, midrange GPUs account for a much larger share of the discrete GPU market, thanks to their lower prices. Unfortunately, today’s midrange cards like the RTX 4060 and RX 7600 only come with 8 GB of VRAM, and are poor value. Intel takes advantage of this by launching the Arc B580 at $250, undercutting both competitors while offering 12 GB of VRAM.</p><p>For B580 to be successful, its new Battlemage architecture has to execute well across a variety of graphics workloads. Intel has made numerous improvements over Alchemist, aiming to achieve better performance with less compute power and less memory bandwidth. I’ll be looking at the Arc B580, with comparison data from the A770 and A750, as well as scattered data I have lying around.</p><p>Battlemage is organized much like its predecessor. Xe Cores continue to act as a basic building block. Four Xe Cores are grouped into a Render Slice, which also contains render backends, a rasterizer, and associated caches for those fixed function units. The entire GPU shares an 18 MB L2 cache.</p><p>The Arc B580 overall is a smaller GPU than its outgoing Alchemist predecessors. B580 has five Render Slices to A770’s eight. In total, B580 has 2560 FP32 lanes to A770’s 4096.</p><p>Battlemage launches with a smaller memory subsystem too. The B580 has a 192-bit GDDR6 bus running at 19 GT/s, giving it 456 GB/s of theoretical bandwidth. A770 has 560 GB/s of GDDR6 bandwidth, thanks to a 256-bit bus running at 17.5 GT/s.</p><p>Even the host interface has been cut down. B580 only has a PCIe 4.0 x8 link, while A770 gets a full size x16 one. Intel’s new architecture has a lot of heavy lifting to do if it wants to beat a much larger implementation of its predecessor.</p><p>Battlemage’s architectural changes start at its Xe Cores. The most substantial changes between the two generations actually debuted on Lunar Lake. Xe Cores are further split into XVEs, or Xe Vector engines. Intel merged pairs of Alchemist XVEs into ones that are twice as wide, completing a transition towards larger execution unit partitions. Xe Core throughput stays the same at 128 FP32 operations per cycle.</p><p>A shared instruction cache feeds all eight XVEs in a Xe Core. Alchemist had a 96 KB instruction cache, and Battlemage almost certainly has an instruction cache at least as large. Instructions on Intel GPUs are generally 16 bytes long, with a 8 byte compacted form in some cases. A 96 KB instruction cache therefore has a nominal capacity of 6-12K instructions.</p><p>XVEs form the smallest partition in Intel GPUs. Each XVE tracks up to eight threads, switching between them to hide latency and keep its execution units fed. A 64 KB register file stores thread state, giving each thread up to 8 KB of registers while maintaining maximum occupancy. Giving a register count for Intel GPUs doesn’t really work, because Intel GPU instructions can address the register file with far more flexibility than Nvidia or AMD architectures. Each instruction can specify a vector width, and access a register as small as a single scalar element.</p><p><a href=\"https://chipsandcheese.com/p/intels-ambitious-meteor-lake-igpu\" rel=\"\">Meteor Lake article</a></p><p>Battlemage’s divergence behavior by comparison is intuitive and straightforward. SIMD16 achieves full utilization if groups of 16 threads go the same way. The same applies for SIMD32 and groups of 32 coherent threads. Thus Battlemage is actually more agile than its predecessor when dealing with divergent branches, while enjoying the efficiency advantage of using larger vectors.</p><p>Like Alchemist, Battlemage executes most math operations down two ports (ALU0, ALU1). ALU0 handles basic FP32 and FP16 operations, while ALU1 handles integer math and less common instructions. Intel’s port layout has parallels to Nvidia’s Turing, which also splits dispatch bandwidth between 16-wide FP32 and INT32 units. A key difference is that Turing uses fixed 32-wide vectors, and keeps both units occupied by feeding them on alternate cycles. Intel can issue instructions of the same type back-to-back, and can select multiple instructions to issue per cycle to different ports.</p><p>In another similarity to Turing, Battlemage carries forward Alchemist’s “XMX” matrix multiplication units. Intel claims 3-way co-issue, implying XMX is on a separate port. However, VTune only shows multiple pipe active metrics for ALU0+ALU1 and ALU0+XMX. I’ve drawn XMX as a separate port above, but the XMX units could be on ALU1.</p><p>Gaming workloads tend to use more floating point operations. During compute heavy sections, ALU1 offloads other operations and keeps ALU0 free to deal with floating point math. XeSS exercises the XMX unit, with minimal co-issue alongside vector operations. A generative AI workload shows even less XMX+vector co-issue.</p><p>As expected for any specialized execution unit, XMX software support is far from guaranteed. Running AI image generation or language models using other frameworks heavily exercises B580’s regular vector units, while leaving the XMX units idle.</p><p>In microbenchmarks, Intel’s older A770 and A750 can often use their larger shader arrays to achieve higher compute throughput than B580. However, B580 behaves more consistently. Alchemist had trouble with FP32 FMA operations. Battlemage in contrast has no problem getting right up to its theoretical throughput. FP32+INT32 dual issue doesn’t happen perfectly on Battlemage, but it barely happened at all on A750.</p><p>Each XVE also has a branch port for control flow instructions, and a “send” port that lets the XVE talk with the outside world. Load on these ports is typically low, because GPU programs don’t branch as often as CPU ones, and shared functions accessed through the “send” port won’t have enough throughput to handle all XVEs hitting it at the same time.</p><p>Battlemage’s memory subsystem has a lot in common with Alchemist’s, and traces its origins to Intel’s integrated graphics architectures over the past decade. XVEs access the memory hierarchy by sending a message to the appropriate shared functional unit. At one point, the entire iGPU was basically the equivalent of a Xe Core, with XVE equivalents acting as basic building blocks. XVEs would access the iGPU’s texture units, caches, and work distribution hardware over a messaging fabric. Intel has since built larger subdivisions, but the terminology remains.</p><p>Each Xe Core has eight TMUs, or texture samplers in Intel terminology. The samplers have a 32 KB texture cache, and can return 128 bytes/cycle to the XVEs. Battlemage is no different from Alchemist in this respect. But the B580 has less texture bandwidth on tap than its predecessor. Its higher clock speed isn’t enough to compensate for having far fewer Xe Cores.</p><p>B580 runs at higher clock speeds, which brings down texture cache hit latency too. In clock cycle terms though, Battlemage has nearly identical texture cache hit latency to its predecessor. L2 latency has improved significantly, so missing the texture cache isn’t as bad on Battlemage.</p><p>Global memory accesses are first cached in a 256 KB block, which serves double duty as Shared Local Memory (SLM). It’s larger than Alchemist and Lunar Lake’s 192 KB L1/SLM block, so Intel has found the transistor budget to keep more data closer to the execution units. Like Lunar Lake, B580 favors SLM over L1 capacity even when a compute kernel doesn’t allocate local memory.</p><p>Intel may be able to split the L1/SLM block in another way, but a latency test shows exactly the same result regardless of whether I allocate local memory. Testing with Nemes’s Vulkan test suite also shows 96 KB of L1.</p><p>Global memory access on Battlemage offers lower latency than texture accesses, even though the XVEs have to handle array address generation. With texture accesses, the TMUs do all the address calculations. All the XVEs do is send them a message. L1 data cache latency is similar to Alchemist in clock cycle terms, though again higher clock speeds give B580 an actual latency advantage.</p><p>Battlemage gets a clock cycle latency reduction too with scalar memory accesses. Intel does not have separate scalar instructions like AMD. But Intel’s GPU ISA lets each instruction specify its SIMD width, and SIMD1 instructions are possible. Intel’s compiler has been carrying out scalar optimizations and opportunistically generating SIMD1 instructions well before Battlemage, but there was no performance difference as far as I could tell. Now there is.</p><p>On B580, L1 latency for a SIMD1 (scalar) access is about 15 cycles faster than a SIMD16 access. SIMD32 accesses take one extra cycle when microbenchmarking, though that’s because the compiler generates two sets of SIMD16 instructions to calculate addresses across 32 lanes. I also got Intel’s compiler to emit scalar INT32 adds, but those didn’t see improved latency over vector ones. Therefore, the scalar latency improvements almost certainly come from an optimized memory pipeline.</p><p>SIMD1 instructions also help within the XVEs. Intel doesn’t use a separate scalar register file, but can more flexibly address their vector register file than AMD or Nvidia. Instructions can access individual elements (sub-registers) and read out whatever vector width they want. Intel’s compiler could pack many “scalar registers” into the equivalent of a vector register, economizing register file capacity.</p><p><a href=\"https://www.intel.com/content/www/us/en/docs/oneapi/optimization-guide-gpu/2024-2/intel-xe-gpu-architecture.html\" rel=\"\">L1 can deliver 512 bytes per cycle</a></p><p>Even if the L1 can only provide 256 bytes per cycle, that still gives Intel’s Xe Core as much L1 bandwidth as an AMD RDNA WGP, and twice as much L1 bandwidth as an Nvidia Ampere SM. 512 bytes per cycle would let each XVE complete a SIMD16 load every cycle, which is kind of overkill anyway.</p><p>Battlemage uses the same 256 KB block for L1 cache and SLM. SLM provides an address space local to a group of threads, and acts as a fast software managed scratchpad. In OpenCL, that’s exposed via the local memory type. Everyone likes to call it something different, but for this article I’ll use OpenCL and Intel’s term.</p><p>Even though both local memory and L1 cache hits are backed by the same physical storage, SLM accesses enjoy better latency. Unlike cache hits, SLM accesses don’t need tag checks or address translation. Accessing Battlemage’s 256 KB block of memory in SLM mode brings latency down to just over 15 ns. It’s faster than doing the same on Alchemist, and is very competitive against recent GPUs from AMD and Nvidia.</p><p>Alchemist and Battlemage both appear to have 32 atomic ALUs attached to each Xe Core’s SLM unit, much like AMD’s RDNA and Nvidia’s Pascal. Meteor Lake’s Xe-LPG architecture may have half as many atomic ALUs per Xe Core.</p><p>Battlemage has a two level cache hierarchy like its predecessor and Nvidia’s current GPUs. B580’s 18 MB L2 is slightly larger than A770’s 16 MB L2. A770 divided its L2 into 32 banks, each capable of handling a 64 byte access every cycle. At 2.4 GHz, that’s good for nearly 5 TB/s of bandwidth.</p><p>Intel didn’t disclose B580’s L2 topology, but a reasonable assumption is that Intel increased bank size from 512 to 768 KB, keeping 4 L2 banks tied to each memory controller. If so, B580’s L2 would have 24 banks and 4.3 TB/s of theoretical bandwidth at 2.85 GHz. Microbenchmarking using Nemes’s Vulkan test gets a decent proportion of that bandwidth. Efficiency is much lower on the older A750, which gets approximately as much bandwidth as B580 despite probably having more theoretical L2 bandwidth on tap.</p><p>Besides insulating the execution units from slow VRAM, the L2 can act as a point of coherency across the GPU. B580 is pretty fast when bouncing data between threads using global memory, and is faster than its predecessor.</p><p>With atomic add operations on global memory, Battlemage does fine for a GPU of its size and massively outperforms its predecessor.</p><p>I’m using INT32 operations, so 86.74 GOPS on the A750 would correspond to 351 GB/s of L2 bandwidth. On the B580, 220.97 GOPS would require 883.9 GB/s. VTune however reports far higher L2 bandwidth on A750. Somehow, A750 sees 1.37 TB/s of L2 bandwidth during the test, or nearly 4x more than it should need.</p><p>Meteor Lake’s iGPU is a close relative of Alchemist, but its ratio of global atomic add throughput to Xe Core count is similar to Battlemage’s. VTune reports Meteor Lake’s iGPU using more L2 bandwidth than required, but only by a factor of 2x. Curiously, it also shows the expected bandwidth coming off the XVEs. I wonder if something in Intel’s cross-GPU interconnect didn’t scale well with bigger GPUs.</p><p>With Battlemage, atomics are broken out into a separate category and aren’t reported as regular L2 bandwidth. VTune indicates atomics are passed through the load/store unit to L2 without any inflation. Furthermore, the L2 was only 79.6% busy, suggesting there’s a bit of headroom at that layer.</p><p><a href=\"https://x.com/lamchester/status/1853882650186203540\" rel=\"\">sometimes use global atomic operations</a></p><p>B580 has a 192-bit GDDR6 VRAM subsystem, likely configured as six 2×16-bit memory controllers. Latency from OpenCL is higher than it was in the previous generation.</p><p>I suspect this only applies to OpenCL, because latency from Vulkan (with Nemes’s test) shows just over 300 ns of latency. Latency at large test sizes will likely run into TLB misses, and I suspect Intel is using different page sizes for different APIs.</p><p><a href=\"https://www.techpowerup.com/gpu-specs/geforce-rtx-4060.c4107\" rel=\"\">Nvidia’s RTX 4060</a></p><p>Intel’s balance of cache capacity and memory bandwidth seems to work well, at least in the few examples I checked. Even when VRAM bandwidth demands are high, the 18 MB L2 is able to catch enough traffic to avoid pushing GDDR6 bandwidth limits. If Intel hypothetically used a smaller GDDR6 memory subsystem like Nvidia’s RTX 4060, B580 would need a larger cache to avoid reaching VRAM bandwidth limits.</p><p>Probably as a cost cutting measure, B580 has a narrower PCIe link than its predecessor. Still, a x8 Gen 4 link provides as much theoretical bandwidth as a x16 Gen 3 one. Testing with OpenCL doesn’t get close to theoretical bandwidth, but B580 is at a disadvantage compared to A750.</p><p><a href=\"https://www.techpowerup.com/review/nvidia-geforce-rtx-4090-pci-express-scaling/28.html\" rel=\"\">often has minimal impact on gaming performance</a></p><p>DCS for example will uses over 12 GB of VRAM with mods. Observing different aircraft in different areas often causes stutters on the B580. VTune shows high PCIe traffic as the GPU must frequently read from host memory. </p><p>Battlemage retains Alchemist’s high level goals and foundation, but makes a laundry list of improvements. Compute is easier to utilize, cache latency improves, and weird scaling issues with global memory atomics have been resolved. Intel has made some surprising optimizations too, like reducing scalar memory access latency. The result is impressive, with Arc B580 easily outperforming the outgoing A770 despite lagging in nearly every on-paper specification.</p><p>Some of Intel’s GPU architecture changes nudge it a bit closer to AMD and Nvidia’s designs. Intel’s compiler often prefers SIMD32, a mode that AMD often chooses for compute code or vertex shaders, and one that Nvidia exclusively uses. SIMD1 optimizations create parallels to AMD’s scalar unit or Nvidia’s uniform datapath. Battlemage’s memory subsystem emphasizes caching more than its predecessor, while relying less on high VRAM bandwidth. AMD’s RDNA 2 and Nvidia’s Ada Lovelace made similar moves with their memory subsystems.</p><p>Of course Battlemage is still a very different animal from its discrete GPU competitors. Even with larger XVEs, Battlemage still uses smaller execution unit partitions than AMD or Nvidia. With SIMD16 support, Intel continues to support shorter vector widths than the competition. Generating SIMD1 instructions gives Intel some degree of scalar optimization, but stops short of having a full-out scalar/uniform datapath like AMD or post-Turing Nvidia. And 18 MB of cache is still less than the 24 or 32 MB in Nvidia and AMD’s midrange cards.</p><p><a href=\"https://chipsandcheese.com/p/digging-into-driver-overhead-on-intels\" rel=\"\">Driver overhead</a></p><p>But I hope Intel goes after higher-end GPU segments once they’ve found firmer footing. A third player in the high end dGPU market would be very welcome as many folks are still on Pascal or GCN due to folks feeling as if there is not a reasonable upgrade yet. Intel’s Arc B580 addresses some of that pent-up demand, at least when it’s not out-of-stock. I look forward to seeing Intel’s future GPU efforts.</p><p><a href=\"https://www.patreon.com/ChipsandCheese\" rel=\"\">Patreon</a><a href=\"https://www.paypal.com/donate/?hosted_button_id=4EMPH66SBGVSQ\" rel=\"\">PayPal</a><a href=\"https://discord.gg/TwVnRhxgY2\" rel=\"\">Discord</a></p>","contentLength":16674,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43014408"},{"title":"Backblaze Drive Stats for 2024","url":"https://www.backblaze.com/blog/backblaze-drive-stats-for-2024/","date":1739285745,"author":"TangerineDream","guid":231,"unread":true,"content":"<p>As of December 31, 2024, we had 305,180 drives under management. Of that number, there were 4,060 boot drives and 301,120 data drives. This report will focus on those data drives as we review the Q4 2024 annualized failure rates (AFR), the 2024 failure rates, and the lifetime failure rates for the drive models in service as of the end of 2024. Along the way, we’ll share our observations and insights on the data presented, and, as always, we look forward to you doing the same in the comments section at the end of the post.</p><h2>Q4 2024 hard drive failure rates</h2><p>As of the end of 2024, Backblaze was monitoring 301,120 hard drives used to store data. For our evaluation, we removed from consideration 487 drives, as they did not meet the criteria to be included. We’ll discuss the criteria we used in the next section of this report. Removing these drives leaves us with 300,633 hard drives to analyze. The table below shows the annualized failure rates for Q4 2024 for this collection of drives.</p><ul><li>. Seagate 24TB drives (model: ST24000NM002H) arrived in early December. The 1,200 drives filled one Backblaze Vault with no failed drives through the end of Q4. The 24TB Seagate drives join the 20TB Toshiba and 22TB WDC drive models in the 20-plus capacity club as we continue to dramatically increase storage capacity while optimizing existing storage server space.</li><li><strong>Zero failures for the quarter</strong>. Five drive models had zero failures for the quarter starting with the 24TB Seagate drive model noted above. The others are the 4TB HGST (model: HMS5C4040ALE640), the 8TB Seagate (model: ST8000NM000A), the 14TB Seagate (model: ST14000NM000J), and the 16TB Seagate (model: ST16000NM002J). All of the zeroes come with the caveat of having a relatively small number of drives and drive days, but zero failures in a quarter is always a good thing.</li><li><strong>The 4TB drives are nearly extinct</strong>. The 4TB drive count decreased by another 1,774 drives in Q4. (I discussed exactly <a href=\"https://www.backblaze.com/blog/how-backblaze-scales-our-storage-cloud/\" target=\"_blank\" rel=\"noreferrer noopener\">how we migrate them</a> in more detail if you want to dig in.) The remaining ~4,000 drives should be gone by the end of Q1 2025. They will be replaced by the incoming 20TB, 22TB, and 24TB drives. It should be noted that out of the 4TB drives in operation in Q4, only one failed, so those 20-plus TB drives have a lot to live up to from a failure perspective.</li><li><strong>The quarterly failure rate is down.</strong> The AFR for Q4 dropped from 1.89% in Q3 to 1.35% in Q4. While all drive sizes delivered some improvement from Q3 to Q4, one of the primary drivers is the addition of over 14,000 new 20-plus TB drives. As a group, these drives delivered an AFR of 0.77% for the quarter.</li></ul><p>We noted earlier we removed 487 drives from consideration when we produced the table above covering Q4 2024. There are two primary reasons we did not consider these drive models.</p><ul><li>. These are drives of a given model that we monitor and collect Drive Stats data on, but are not considered production drives at this time. For example, drives undergoing certification testing to determine if they are performant enough for our environment are not included in our Drive Stats calculations.</li><li><strong>Insufficient data points. </strong>When we calculate the annualized failure rate for a drive model for a given period of time (quarterly, annual, or lifetime), we want to ensure we have enough data to reliably do so. Therefore we have defined criteria for a drive model to be included in the tables and charts for the specified period of time. Models that do not meet these criteria are not included in the tables and charts for the period in question.</li></ul><table><thead><tr></tr></thead><tbody></tbody></table><p>Regardless of whether or not a given drive model is included in the charts and tables, all of the data for all of the drives we use is included in our Drive Stats dataset which you can download by visiting our <a href=\"https://www.backblaze.com/cloud-storage/resources/hard-drive-test-data\" target=\"_blank\" rel=\"noreferrer noopener\">Drive Stats page</a>.</p><p>As with the Q4 quarterly results, we will apply these criteria to the annual and lifetime charts that follow in this report.</p><h2>2024 annual hard drive failure rates</h2><p>As of the end of 2024, Backblaze was monitoring 301,120 hard drives used to store data. We removed nine drive models consisting of 2,012 drives from consideration as they did not meet the annual criteria we have defined. This leaves us with 298,954 drives divided across 27 different drive models. The table below shows the AFRs for 2024 for this collection of drives.</p><ul><li>. There were no qualifying drive models with zero failures in 2024. That said, the 16TB Seagate (model: ST16000NM002J) got close by recording just one drive failure back in Q3, giving the drive an AFR of 0.22% for 2024.&nbsp;</li><li>. During 2024, our data center techs installed 53,337 drives. If we assume there are 2,080 work hours a year (52 weeks times 40 hours), that math is , and that means our intrepid DC techs installed 26 drives per hour. Busy, busy, busy!&nbsp;</li><li>While there were 1,200 new 24TB Seagate drives added in 2024, they were installed in early December and did not accumulate enough drive days to make the cut for the annual, or lifetime, tables. Including the 24TB Seagate drive, there were three models that missed out on being included in the 2024 annual tables, these drive models are listed below.</li></ul><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr></tbody></table><p>As a reminder, a drive model needs to have over 250 drives by the end of Q4 and accumulate at least 50,000 drive days during 2024 to be included in the annual tables.</p><h2>Comparing Drive Stats for 2022, 2023, and 2024</h2><p>The table below compares the annual failure rates by drive model for each of the last three years. The table includes just those drive models which met the annual criteria as of the end of 2024. The data for each year is inclusive of that year only for the operational drive models present at the end of each year. The table is sorted by drive size and then AFR.</p><ul><li> The 2024 AFR for all drives listed was 1.57%, this is down from 1.70% in 2023.&nbsp; We expect the overall failure rates to continue to fall in 2025, but we will be watching the following for indicators.\n<ul><li><strong>The failure rates of the 8TB and 12TB drive models. </strong>All of the models will exceed their five years of service. In general, the failure rate will noticeably increase as the drives exceed five years of service. And, while there are outliers like the current HGST 4TB drives, you can’t assume that will happen.</li><li><strong>The failure rates of the 14TB and 16TB drive models.</strong> These models are approaching middle age—three to five years in operation. This is where, according to <a href=\"https://www.backblaze.com/blog/drive-failure-over-time-the-bathtub-curve-is-leaking/\" target=\"_blank\" rel=\"noreferrer noopener\">the bathtub curve</a>, their failure rates could gradually increase—but not as severely as when they exceed five years.&nbsp;</li><li><strong>The failure rates for the 20TB, 22TB, and 24TB drives models.</strong> These drives will enter the flat portion of the bathtub curve, that is where their failure rate should be the lowest.</li></ul></li></ul><h2>Annualized failure rates vs. drive size</h2><p>Now, we can dig into the numbers to see what else we can learn. We’ll start by looking at the quarterly annualized failure rate by drive size over the last three years.</p><p>Let’s take a look at the different drive sizes and how they affect the overall annualized failure rate over time.</p><p>. The 4TB (blue line) drives and 10TB (gold line) drives have had little impact over the last year on the overall failure rate as each finished the year with a relatively small number of drives. Still, the wild ride delivered by the 10TB drives keeps our DC techs on their toes.&nbsp;</p><p>. The 8TB (gray line) drives and 12TB (purple line) drives range in age from five to eight years and as such their overall failure rates should be increasing over time. The 12TB drives are following that pattern moving up from about 1% AFR back in 2021 to just about 3% in 2024. The failure rates of the 8TB drives, while erratic from quarter-to-quarter, have a nearly flat trendline over the same period.</p><p>. The 14TB (green line) and 16TB (azure* line) drives comprise 57% of the drives in service and on average they range in age from two to four years. They are in the prime of their working lives. As such, they should have low and stable failure rates, and as you can see, they do.</p><p>*&nbsp; Maybe azure isn’t quite right, but robin’s egg blue seemed a bit pretentious.</p><p>. The 22TB (orange line) drives are in their early days as we continue to add more drives on a regular basis. Once the drive population settles down, we’ll have a better sense of the AFR direction. Still, the early results are solid with a lifetime AFR of 1.06%.</p><h2>Annualized failure rates vs. manufacturer</h2><p>One of the more popular ways we can look at this data is by the drive manufacturer as we’ve done below.</p><p>To complete the picture, the chart below uses the same data, but displays just the linear trendlines for each of the manufacturers over the same three-year period.</p><p>. While the HGST trendline is not pretty, it doesn’t tell the entire story. Looking at the first chart, until Q4 2023, the HGST drives were at or below the average for all of the drives, that is all manufacturers. At that point, HGST has exceeded the average, and then some. The table below contains results for just the HGST drives for 2024. We’ve sorted them, high to low, by the 2024 AFR.</p><p>As you can see, there are two 12TB drive models driving the high AFR for the HGST drives. The HUH721212ALN604 model began showing signs of an increased quarterly AFR in Q1 2023 and the HUH721212ALE604 model followed suit in Q3 2024. Without these drive models, the 2024 AFR for HGST drive would be 0.55%.</p><p>. The quarterly AFR trendline decreased for the Seagate drives from 2022 through 2024. While the decrease was slight, from 2.25% to 2.0%, Seagate was the only manufacturer to do so. The decrease appears, at least in part, to be due to the removal of the Seagate 4TB drives during that period.&nbsp;</p><p>. Over the 2022 to 2024 period, the quarterly AFR for the Toshiba drive models varied within a fairly narrow range between 0.80% and 1.52%, with most quarters hovering slightly around 1.2%. Most importantly, none of the individual drive models were outliers, as the highest quarterly AFR for any Toshiba drive model was 1.58%. We like consistency.&nbsp;</p><p>. While WDC drive models delivered a similar level of consistency as the Toshiba models, they did so with a lower AFR each quarter. From 2022 through 2024, the range of quarterly AFR values for the WDC models was 0.0% to 0.85%. The 0.0% AFR was in Q1 2022 when none of the 12,207 WDC drives in operation failed during that quarter.</p><h2>Lifetime hard drive stats</h2><p>As of the end of 2024, Backblaze was monitoring 301,120 hard drives used to store data. Applying our drive criteria noted above for the lifetime period, we removed 11 drive models consisting of 2,736 drives from consideration as they did not meet the lifetime criteria we defined. This leaves us with 298,230 drives divided across 25 different drive models. The table below shows the lifetime AFRs for this collection of drives.</p><p>The current lifetime AFR for all of the drives is 1.31%. This is down from 1.46% in 2023. The drop is primarily due to the completion of the migration of the 4TB Seagate drives in 2024, which left us with only two of these drives still in operation as of the end of 2024. As a consequence, the 79 million drive days and over 5,600 drive failures racked up by the 4TB Seagate drives by the end of 2023 are not included in the data presented in the 2024 lifetime table above.&nbsp;&nbsp;</p><p>In the final table below, we’ve taken the lifetime table and sorted out the drive models that have a lifetime AFR of 1.50% or less by drive size.</p><p>A couple of caveats as you review the table.</p><ul><li>There is enough data for each model to say the AFR values are solid. That said, everything could change tomorrow. In general, the hard drive failure rate follows the bathtub curve as the drives age—unless it doesn’t. Some drives refuse to fail as they age, like the 4TB HGST drives. Other drives are great, and then “hit the wall” and bend the failure curve upward, fast.</li><li>A drive model with a 1% annualized failure rate means that you can expect one drive out of 100 to fail in a year. If you’re a personal drive user, that one drive could be yours. If you have exactly one drive, your personal annualized failure rate is 100%. In other words, always have a backup, and don’t forget to test it.</li></ul><p>I have been authoring the various Drive Stats reports for the past ten years and this will be my last one. I am retiring, or perhaps in Drive Stats vernacular, it would be “migrating.” Either way, after 10 years in the U.S. Air Force and 30+ years in Silicon Valley Tech, it is time. Drive Stats will continue with <a href=\"https://www.backblaze.com/blog/author/stephanie/\" target=\"_blank\" rel=\"noreferrer noopener\">Stephanie Doyle</a> and <a href=\"https://www.backblaze.com/blog/author/davidjohnson/\" target=\"_blank\" rel=\"noreferrer noopener\">David Johnson</a> as the replacement drive models beginning with the Q1 2025 report. I wish them well.</p><p>I want to say thank you to each of you who have taken your time to peruse and engage with the Drive Stats reports and data over the last 10 years. And, thank you as well for the comments, questions, and discussions that raced and raged across the various communities that care about something as mundane and awesome as a hard drive. It has been quite the ride—thanks again.</p><h2>The Hard Drive Stats data</h2><p>The complete data set used to create the tables and charts in this report is available on our&nbsp;<a href=\"https://www.backblaze.com/cloud-storage/resources/hard-drive-test-data/\" target=\"_blank\" rel=\"noreferrer noopener\">Hard Drive Test Data</a>&nbsp;page. You can download and use this data for free for your own purpose. All we ask are three things: 1) you cite Backblaze as the source if you use the data, 2) you accept that you are solely responsible for how you use the data, and 3) you do not sell this data itself to anyone; it is free.</p><p>Good luck, and let us know if you find anything interesting.</p>","contentLength":13388,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43013431"},{"title":"Nvidia's RTX 5090 power connectors are melting","url":"https://www.theverge.com/news/609207/nvidia-rtx-5090-power-connector-melting-burning-issues","date":1739247227,"author":"ambigious7777","guid":230,"unread":true,"content":"<div><p>Ah shit, here we go again. Two owners of Nvidia’s new RTX 5090 Founders Edition GPUs have reported melted power connectors and damage to their PSUs. The images look identical to reports of <a href=\"https://www.theverge.com/2022/10/25/23422349/nvidia-rtx-4090-power-cables-connectors-melting-burning\">RTX 4090 power cables burning</a> or melting from two years ago. <a href=\"https://www.theverge.com/2022/11/18/23466974/nvidia-rtx-4090-power-cable-12vhpwr-melt-burn-plugged-in\">Nvidia blamed</a> the issue on people not properly plugging the 12VHPWR power connection in fully and the PCI standards body <a href=\"https://www.theverge.com/2022/12/1/23488276/nvidia-12vhpwr-cable-16-pin-pci-sig-response\">blamed Nvidia</a>.</p></div><div><p><a href=\"https://www.reddit.com/r/nvidia/comments/1ilhfk0/rtx_5090fe_molten_12vhpwr/\">A Reddit poster</a> upgraded from an RTX 4090 to an RTX 5090 and noticed “a burning smell playing ,” before turning off their PC and finding the damage. The images show burnt plastic at both the PSU end of the power connector and the part that connects directly to the GPU. The cable is one from <a href=\"https://www.moddiy.com/products/ATX-3.0-PCIe-5.0-600W-12VHPWR-16-Pin-to-16-Pin-PCIE-Gen-5-Power-Cable.html\">MODDIY</a>, a popular manufacturer of custom cables, and the poster claims it was “securely fastened and clicked on both sides (GPU and PSU).”</p></div><div><p>While it’s tempting to blame the MODDIY cable, Spanish <a href=\"https://www.youtube.com/watch?v=Nw7HaVRUN9k\">YouTuber Toro Tocho</a> has experienced the same burnt cable (both at the GPU and PSU ends) with an RTX 5090 Founders Edition while using a cable supplied by PSU manufacturer FSP. Plastic has also melted into the PCIe 5.0 power connector on the power supply. MODDIY also <a href=\"https://www.reddit.com/r/GamersNexus/comments/1ilgii6/comment/mc0t80d/\">responded in a Reddit thread</a>, ruling out the “possibility of a defective cable or manufacturing error” and offering to cover the cost of repair if Nvidia and Asus don’t honor their warranties.</p></div><div><p><a href=\"https://www.youtube.com/watch?v=Ndmoi1s0ZaY\">YouTuber der8auer</a> has also examined the Reddit poster’s equipment in person, and ruled out any form of user error in the process. He’s also found that this could be related to a current distribution problem with RTX 5090 Founders Edition models instead. Either way, nobody should be blaming 12VHPWR on end users.</p></div><div><p>Nvidia originally introduced the 12VHPWR power connector on its RTX 40-series GPUs, and power supplies also debuted to support the new standard. The RTX 4090 Founders Edition was able to draw 450 watts over the 12VHPWR connector, while the new RTX 5090 draws up to 575 watts over a cable that’s rated up to 600 watts. After early issues with RTX 4090 connectors melting, PCI-SIG, the standards organization responsible for the 12VHPWR connector, has now updated it to a new 12V-2x6 connector on the GPU side and in some cases the PSU side, too.</p></div><div><p>The 12V-2x6 connector has shorter sensing pins and longer conductor terminals, to improve reliability. “This might not sound like a huge difference, but it matters in ensuring that the power cable has been properly connected to whatever device is going to be pulling power from your system’s power supply,” <a href=\"https://go.corsair.com/c/482924/490888/8513?u=https%3A%2F%2Fwww.corsair.com%2Fuk%2Fen%2Fexplorer%2Fdiy-builder%2Fpower-supply-units%2Fevolving-standards-12vhpwr-and-12v-2x6%2F%3F\" rel=\"sponsored\">explains Corsair</a>.</p></div><div><p>Nvidia uses the 12V-2x6 connector on its RTX 50-series GPUs, but you can still use existing 12VHPWR cables. “To be clear, this is not a new cable, it is an updated change to the pins in the socket, which is referred to as 12V-2x6,” says Corsair. PSU manufacturers like Corsair and MSI have adopted colored pins on their 12VHPWR cables so that if you can still see the yellow or grey pins it means the connector isn’t seated properly.</p></div><div><p>While Intel and AMD are both members of the PCI-SIG group that helped develop the 12VHPWR power connector, only Nvidia has adopted the standard so far for consumer GPUs. Even AMD’s upcoming <a href=\"https://www.theverge.com/2025/1/6/24336246/amd-radeon-rx-9070-series-rdna-4-fsr-4-ces-2025\">Radeon RX 9070-series</a> are using existing 8-pin PCIe connections instead. AMD even suggested the 12VHPWR connector was a fire hazard in late 2022, when the company’s gaming marketing director Sasa Marinkovic <a href=\"https://x.com/SasaMarinkovic/status/1593243804538372096?\">tweeted</a> “Stay safe this holiday season” alongside a picture of 8-pin connectors.</p></div><div><p>12VHPWR has been <a href=\"https://www.youtube.com/watch?v=Y36LMS5y34A\">branded a “dumpster fire,</a>” thanks to design oversights that make it relatively easy for end users to not properly connect the cable securely. Cablemod was also <a href=\"https://www.theverge.com/2024/2/8/24029465/after-74500-in-damage-cablemod-is-fully-recalling-its-nvidia-12vhpwr-gpu-power-adapters\">forced to recall</a> its 12VHPWR GPU power adapters last year after reports of melted adapters.</p></div><div><p>We reached out to Nvidia to comment on these latest reports of RTX 5090 power connector issues, but the company refused to comment.</p></div><div><p><em>: Article updated with comment from MODDIY and a new discovery from YouTuber der8auer.</em></p></div>","contentLength":3952,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43008879"},{"title":"The hallucinatory thoughts of the dying mind","url":"https://thereader.mitpress.mit.edu/the-hallucinatory-thoughts-of-the-dying-mind/","date":1739193088,"author":"anarbadalov","guid":229,"unread":true,"content":"<div>Delirium is one of the most perplexing deathbed phenomena, exposing the gap between our cultural ideals of dying words and the reality of a disoriented mind.</div><div><div><a href=\"https://www.beelinereader.com\" target=\"_blank\" rel=\"nofollow\">BeeLine Reader</a> uses subtle color gradients to help you read more efficiently.</div></div><p>When William Brahms’s anthology of last words, “Last Words of Notable People,” came out in 2010, he encountered readers who entertained themselves by speculating what their last words might be. Initially, Brahms said, they would propose something witty and profound. Later, they admitted that they’d likely express appreciation and love, or something spiritual — if they could even manage to speak, when lucidity and intelligibility become hurdles. Brahms witnessed this firsthand at the bedside of his dying mother. Even though she’d spent years helping him edit his book, they’d never discussed what they thought their last words would be.</p><p>It didn’t matter, because the reality was quite different.</p><p>“In her case, due to debilitating illness, it was not a clear punctuated statement,” Brahms told me in an email, with evident surprise. “It was more of a slow and fading dialogue.”</p><p>This dissonance between the idealized notion of dying speech, based on people’s expectations and cultural ideals, and the reality of final moments so often complicates our understanding of death. And when delirium enters the equation, the gap between expectation and reality widens even further.</p><p>What should we make of people who can speak and yet make no sense?</p><p>On one hand, there’s a very good chance that a dying person will be delirious at the end of life. In fact, in palliative care and hospice spaces, 58 to 88 percent of cancer patients are delirious in the last week to <a href=\"https://pmc.ncbi.nlm.nih.gov/articles/PMC6691600/\" target=\"_blank\" rel=\"nofollow\">hours before death.</a> In William Osler’s <a href=\"https://pubmed.ncbi.nlm.nih.gov/33722079/\" target=\"_blank\" rel=\"nofollow\">landmark study</a> of dying, 28 patients out of the total 486 were described as delirious, with his observers using words like “irrational,” “mind wandering,” “demented,” and in one case “raging.”</p><p>“I don’t want to call it a nearly universal feature of the end-of-life experience,” David Wright, a Canadian medical ethnographer, told me, “but as you die, unless you die very suddenly in an instant, your various bodily systems start to work differently until they stop working at all. And that includes the way that you think, and that includes the way that you communicate.”</p><figure><blockquote><p>In palliative care and hospice spaces, well over half of cancer patients are delirious in the last week to hours before death.</p></blockquote></figure><p>Ancient medical writers distinguished between two types of delirium: , the restless variety, and , an inert, dull state. Modern researchers keep this distinction, describing delirium as either quiet, listless, apathetic, and “pseudo wakeful,” or as a restlessness in which patients are “muttering” and “talkative.” Delirium was one of the first medical conditions ever described by Greek and Roman writers almost 2,000 years ago. The Greek philosopher Celsus introduced the word  in the second century CE; the word came from the Latin , which means to go out of the furrow ( is Latin for “furrow”). The delirious plow leaves the furrow of sense. A modern person might think that leaving the planned track is a desirable thing, until they familiarize themselves with a single-bladed plow and realize the inconvenience of an errant ox-pulled plow skittering across the field’s surface.</p><p>Despite this pedigree, the exact biological mechanisms behind delirium aren’t well understood, but it appears to stem from neuronal dysfunction, probably due to neurotransmitter fluctuations. Neurons in the brain aren’t dying (which is why sometimes people can recover from delirium) but disconnecting from each other. Basically, delirium is the result of a neurochemical commotion. Now clinicians look for three types of delirium: a restless, hyperactive form, a listless form, and a mix of the two.</p><p>Delirium is very common among the dying, particularly in the later stages, where that lethargic type shows up often. As a diagnosis, it covers a complex of symptoms and isn’t a single thing. Despite its prevalence, doctors don’t reliably recognize delirium. It’s often mistaken for dementia, depression, <a href=\"https://pubmed.ncbi.nlm.nih.gov/25976839/\" target=\"_blank\" rel=\"nofollow\">or psychosis</a>. In a study of 100 consecutive cases of delirium in a palliative care unit, researchers found that 33 percent of patients were classified with the hypoactive, lethargic form. They had the same impairment in cognitive functioning as patients with other variants, showing similar deficits in orientation, memory, and comprehension <a href=\"https://pubmed.ncbi.nlm.nih.gov/21677247/\" target=\"_blank\" rel=\"nofollow\">on cognitive test scores</a>. This variegated presentation makes it hard to tell when someone is delirious. Though clinicians want to take it seriously, they don’t have a uniform method for recognizing delirium or dealing with it therapeutically.</p><p>On the other hand, delirium seems to outstrip people’s ability — and their willingness — to grasp a phenomenon as simultaneously biological, emotional, and social. It has what David Wright called a relational dimension, in the sense that any individual’s delirium impacts other people’s perceptions of the relationship. Some find it traumatizing and distressing, others less so. In either case, what seems to help is when delirium is described as a normal part of dying, in the same way that baby babbling is described as a normal feature of language acquisition. Some medical staff try to normalize the delirium as part of the natural process of dying, and they encourage family members to enter the hallucinatory world — <a href=\"https://journals.sagepub.com/doi/10.1177/0269216307081129?icid=int.sj-full-text.similar-articles.4\" target=\"_blank\" rel=\"nofollow\">or at least not to fight it</a>. Many hospices recommend the latter as well: “Do not contradict, explain away, belittle or argue about what the person claims to have seen or heard,” reads a short text that a hospice provides about the dying process. “Just because you cannot see or hear it does not mean it is not real to your loved one. Affirm his or her experience. They are normal and common.”</p><p>By itself, the brain explanation isn’t soothing. What does help is taking advantage of the interaction window that remains. Or you open another window, as if that’s the one that the other person wants to operate with. You might respond, “So you say you’re going on a trip. Who do you think will be waiting for you?” Or, “Tell me some nice things you remember about your mother.” Family members might be told that the patient has already undergone a sort of social death; though their body is present, the previous person they is gone, so a new relationship is required. So while your father’s brain is the author of some insult, and his body its animator, your father as you knew him isn’t the principal of that offensive utterance.</p><p>Some families take it better than others — some want to deny what’s happening, while others roll with it (“Oh, you’re seeing bugs? What’s it look like?”). For other family members, some meaning needs to be made from the delirium. “It can be very meaningful for family members to say, ‘Oh, Dad’s now with Mom,’ who is dead. That can be a great source of comfort,” a doctor who often treats delirium told me.</p><p>I faced this with my own grandmother. I’d known my father’s mother, Norma, from childhood. A bright-faced, cheery woman, she’d fold me in her arms, say that she loved me, then laugh chirpingly and say that she prayed for me, that unmistakable Irish grandmother code for “I want you to go to Mass.” Now I was no longer a child, and now she was dying. The gray skies on our visit to the hospital will be forever seared in my memory. When my wife and I entered my grandmother’s room, I told her who I was. Her eyelids fluttered open, and her eyes focused on me, her gaunt face lit up, she reached out her arms, I hugged and kissed her. “Michael-Jean, you’re an angel!” she exclaimed. “You’re an angel!”</p><p>At first I was concerned — <em>does she know it’s me, not an angel?</em> Apparently so, she said my name, then repeated joyfully, “You’re an angel, you’re an angel.” Delirium? Maybe. But when your beloved grandmother calls you an angel, you reach for the interpretation that feels most loving. <em>That has to be her talking, she’s still there. </em>She said what she said; the words, they’re right there, her last ones, as far as I’m concerned. She became unresponsive shortly after and died two days later.</p><p>My father remembers this differently. He maintains that she roused to a saying of the Lord’s Prayer. But I won’t argue — from both of our stories, we end up with some morsels of closure, and that’s what feeds us.</p><p>From a linguistic perspective, delirious language is unhinged and incoherent. People who are delirious after surgery and then recover can sometimes relate their feelings of being briefly outside the furrow of sense, which reduced them to a state of abject terror.</p><p>Osler might have been surprised. “I remember that everything changed to me,” <a href=\"https://pubmed.ncbi.nlm.nih.gov/17462032/\" target=\"_blank\" rel=\"nofollow\">reported one patient</a>. “Suddenly I was a prisoner in a Nazi camp, and I thought that the nurses were the Nazi camp guards . . . and I wondered whatever happened since the nurses had become so unkind to me although they were so nice before.” They reported seeing things that were beautiful, awe-inspiring, and pleasurable. They saw fragments of past events mixed up with the present. And they heard people talking to them but didn’t know what they wanted, nor could they communicate with them.</p><p>As with all these phenomena, delirium involves experiences at the edge of the interaction window, or far beyond it. In many instances, that window has been shut, perhaps not forever. In the case above, family members who knew the person in a “normal” state were also shocked at losing emotional and cognitive access to their loved one. They missed knowing about the other person’s interior life and struggled to figure out if the person with delirium was comfortable or distressed. To deal with this, the advice seems to be: Don’t engage in the main interaction window, because your loved one’s not there; instead, open another interaction window on their terms. That will allow you to be present without expectations or demands.</p><p>Of course, another option is to deny the existence of delirium altogether, as Maggie Callanan and Patricia Kelley’s 1992 book “Final Gifts” does. Their book proposed that dying people begin using a symbolic language particular to the end of life that must be carefully listened to, written down, and interpreted. “Health-care professionals and families may assume that what they’re hearing and seeing is confusion . . . unfortunately, dying people are often labeled ‘confused’ without adequate assessment,” Callanan and Kelley wrote. To those authors, the only reality is the special symbolic language of the dying, and the brain-based understandings of delirium diminish those messages or their meanings. (They don’t have a monopoly on this, by the way. Nearly 40 percent of Indian family members said that delirium was the cause of supernatural beliefs, emotional stress, or a failure of religious observance — and <a href=\"https://www.ajol.info/index.php/ajpsy/article/view/77352\" target=\"_blank\" rel=\"nofollow\">did not have biomedical origins.</a>)</p><p>Given the documented prevalence of delirium, it was a bold position to take. However, in <a href=\"https://maggiecallanan.com/pdf/interview_bedard.pdf\" target=\"_blank\" rel=\"nofollow\">a 1998 interview</a>, Callanan walked the rejection back. “I’m not here to say that everything a confused patient says has great significance. It’s often very hard, even paying close attention, to find exactly what the person is saying. Seventy percent of people dying of illness, at some point, have some confusion; that’s a lot of confusion. However, our point is to always listen specifically to the words.”</p><p>Several years later, on a speaking tour to Chicago, Callanan <a href=\"https://www.chicagotribune.com/2013/06/19/striking-similarity-of-dying-words/\" target=\"_blank\" rel=\"nofollow\">reversed again</a>, telling the  that delirious behaviors aren’t the product of mental confusion or drug mis-dosing. “The confusion is ours,” she said. “The patient knows what’s going on.”</p><p>The newspaper reporter also quoted, in contrast, the views of Joel Frader, a palliative care physician, who identified the source of hallucinations and other unusual behaviors as the failing brain. “There are some fairly characteristic changes in brain chemistry that people have documented as death occurs, whether it’s lack of oxygen or high levels of carbon dioxide or changes in nutrition getting to the brain,” he told the newspaper.</p><figure><blockquote><p>If no one gets a clear model about the language they should produce at the end of their lives, except that it be pithy, poignant, and meaningful, then it risks ending up delirious and nonsensical.</p></blockquote></figure><p>His take was not only frank and sharp, but it reiterated what Victorian doctors also warned: The existence of God or the afterlife won’t be proven with people’s dying words or their hallucinations, because they are “matters of faith.” The organic response of the brain to the body’s failure, on the other hand, is a matter of physiology. Implicit is the idea it works that way for everybody.</p><p>“I do understand people trying to get some meaning out of everything they witness, but delirium is real, and it happens a lot,” Romayne Gallagher, a palliative care physician in British Columbia told me. She didn’t recommend “Final Gifts” to people because of its unrealistic view of delirium. “Certainly the misperception of reality plays a role in delirium — people seeing ants on the ceiling (spotted ceiling tiles) and seeing a bowl of rice (a glowing white lamp globe) — but if delirium goes beyond that, it seems to empty the limbic system’s treasure trove of horrors, which doesn’t seem to relate to the person’s experience. By that I mean people seeing a train coming at them or seeing a loved one on fire just outside the window.”</p><p>My speculation is that the interpretive control promoted in “Final Gifts” reflects a cultural adaptation to dilemmas posed by delirium to a  approach to language at the end of life. That is, if no one gets a clear model about the language they should produce at the end, except for the vague sense that it should be pithy, poignant, and uniquely reflective of who they were, then it risks ending up delirious and nonsensical (if it exists at all). And if loved ones are directed to attend to all of the utterances, how should the non-sensical ones be interpreted? As if they contain some meaning.</p><p>Meanwhile, other religious practices keep dying people occupied, authoring for them what to utter, chant, or pray. Sharing linguistic agency in this way looks like a wise cultural adaptation to the prevalence of delirium. In those traditions, only the prescribed words matter — everything else becomes noise, harmless and inconsequential.</p>","contentLength":14601,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42999788"},{"title":"Building a personal, private AI computer on a budget","url":"https://ewintr.nl/posts/2025/building-a-personal-private-ai-computer-on-a-budget/","date":1739188781,"author":"marban","guid":228,"unread":true,"content":"<p>As everyone is well aware, the world is still going nuts trying to develop more, newer and better AI tools. Mostly by throwing absurd amounts of money at the problem. Many of those billions go towards building cheap or free services that operate at a significant loss. The tech giants that run them all are hoping to attract as many users as possible, so that they can capture the market, and become the dominant or only party that can offer them. It is the classic Silicon Valley playbook. Once dominance is reached, expect the enshittification to begin.</p><p>A likely way to earn back all that money for developing these LLMs will be by tweaking their outputs to the liking of whoever pays the most. An example of what that such tweaking looks like is <a href=\"https://www.theguardian.com/technology/2025/jan/28/we-tried-out-deepseek-it-works-well-until-we-asked-it-about-tiananmen-square-and-taiwan\">the refusal of DeepSeek's R1 to discuss what happened at Tiananmen Square in 1989</a>. That one is obviously politically motivated, but ad-funded services won't exactly be fun either. In the future, I fully expect to be able to have a frank and honest discussion about the Tiananmen events with an American AI agent, but the only one I can afford will have assumed the persona of Father Christmas who, while holding a can of Coca-Cola, will intersperse the recounting of the tragic events with a joyful <em>\"Ho ho ho... Didn't you know? The holidays are coming!\"</em></p><p>But there is hope. One of the tricks of an upcoming player to shake up the market, is to undercut the incumbents by releasing their model for free, under a permissive license. This is what DeepSeek just did with their DeepSeek-R1. Google did it earlier with the Gemma models, as did Meta with Llama. We can download these models ourselves and run them on our own hardware. Better yet, people can take these models and scrub the biases from them. And we can download those <a href=\"https://huggingface.co/huihui-ai/Llama-3.3-70B-Instruct-abliterated\">scrubbed models</a> and run those on our own hardware. And then we can finally have some truly useful LLMs.</p><p>That hardware can be a hurdle, though. There are two options to choose from if you want to run an LLM locally. You can get a big, powerful video card from Nvidia, or you can buy an Apple. Either is expensive. The main spec that indicates how well an LLM will perform is the amount of memory available. VRAM in the case of GPUs, normal RAM in the case of Apples. Bigger is better here. More RAM means bigger models, which will dramatically improve the quality of the output. Personally, I'd say one needs at least over 24GB to be able to run anything useful. That will fit a 32 billion parameter model with a little headroom to spare. Building, or buying, a workstation that is equipped to handle that can easily cost thousands of euros.</p><p>So what to do, if you don't have that amount of money to spare? You buy second-hand! This is a viable option, but as always, there is no such thing as a free lunch. Memory may be the primary concern, but don't underestimate the importance of memory bandwidth and other specs. Older equipment will have lower performance on those aspects. But let's not worry too much about that now. I am interested in building something that at least can run the LLMs in a usable way. Sure, the latest Nvidia card might do it faster, but the point is to be able to do it at all. Powerful online models can be nice, but one should at the very least have the option to switch to a local one, if the situation calls for it.</p><p>Below is my attempt to build such a capable AI computer without spending too much. I ended up with a workstation with 48GB of VRAM that cost me around 1700 euros. I could have done it for less. For instance, it was not strictly necessary to buy a brand new dummy GPU (see below), or I could have found someone that would 3D print the cooling fan shroud for me, instead of shipping a ready-made one from a faraway country. I'll admit, I got a bit impatient at the end when I found out I had to buy yet another part to make this work. For me, this was an acceptable tradeoff.</p><p>This is the full cost breakdown:</p><table><tbody><tr></tr><tr></tr><tr></tr><tr></tr><tr><td>Nvidia Tesla Cooling Fan Kit</td></tr><tr><td>MODDIY Main Power Adaptor Cable</td></tr><tr></tr></tbody></table><p>And this is what it looked liked when it first booted up with all the parts installed:</p><p>I'll give some context on the parts below, and after that, I'll run a few quick tests to get some numbers on the performance.</p><p>The Z440 was an easy pick because I already owned it. This was the starting point. About two years ago, I wanted a computer that could serve as a host for my virtual machines. The Z440 has a Xeon processor with 12 cores, and this one sports 128GB of RAM. Many threads and a lot of memory, that should work for hosting VMs. I bought it secondhand and then swapped the 512GB hard drive for a 6TB one to store those virtual machines. 6TB is not required for running LLMs, and therefore I did not include it in the breakdown. But if you plan to collect many models, 512GB might not be enough.</p><p>I have come to like this workstation. It feels all very solid, and I haven't had any problems with it. At least, until I started this project. It turns out that HP does not like competition, and I encountered some difficulties when swapping components.</p><p>This is the magic ingredient. GPUs are expensive. But, as with the HP Z440, often one can find older equipment, that used to be top of the line and is still very capable, second-hand, for relatively little money. These Teslas were meant to run in server farms, for things like 3D rendering and other graphic processing. They come equipped with 24GB of VRAM. Nice. They fit in a PCI-Express 3.0 x16 slot. The Z440 has two of those, so we buy two. Now we have 48GB of VRAM. Double nice.</p><p>The catch is the part about that they were meant for servers. They will work fine in the PCIe slots of a normal workstation, but in servers the cooling is managed differently. Beefy GPUs consume a lot of power and can run very hot. That is the reason consumer GPUs always come equipped with big fans. The cards need to take care of their own cooling. The Teslas, however, have no fans whatsoever. They get just as hot, but expect the server to supply a steady flow of air to cool them. The enclosure of the card is somewhat shaped like a pipe, and you have two options: blow in air from one side or blow it in from the other side. How is that for flexibility? You absolutely must blow some air into it, though, or you will damage it as soon as you put it to work.</p><p>The solution is simple: just mount a fan on one end of the pipe. And indeed, it seems a whole cottage industry has grown of people that sell 3D-printed shrouds that hold a standard 60mm fan in just the right place. The problem is, the cards themselves are already quite bulky, and it is not easy to find a configuration that fits two cards and two fan mounts in the computer case. The seller who sold me my two Teslas was kind enough to include two fans with shrouds, but there was no way I could fit all of those into the case. So what do we do? We buy more parts.</p><p>This is where things got annoying. The HP Z440 had a 700 Watt PSU, which might have been enough. But I wasn't sure, and I needed to buy a new PSU anyway because it did not have the right connectors to power the Teslas. Using <a href=\"https://www.whatpsu.com/\">this</a> handy website, I deduced that 850 Watt would be sufficient, and I bought the NZXT C850. It is a modular PSU, meaning that you only need to plug in the cables that you actually need. It came with a neat bag to store the spare cables. One day, I might give it a good cleaning and use it as a toiletry bag.</p><p>Unfortunately, HP does not like things that are not HP, so they made it difficult to swap the PSU. It does not fit physically, and they also changed the main board and CPU connectors. All PSUs I have ever seen in my life are rectangular boxes. The HP PSU also is a rectangular box, but with a cutout, making sure that none of the normal PSUs will fit. For no technical reason at all. This is just to mess with you.</p><p>The mounting was eventually solved by using two random holes in the grill that I somehow managed to align with the screw holes on the NZXT. It sort of hangs stable now, and I feel lucky that this worked. I have seen Youtube videos where people resorted to double-sided tape.</p><p>The connector required... another purchase.</p><p>There is another issue with using server GPUs in this consumer workstation. The Teslas are intended to crunch numbers, not to play video games with. Consequently, they don't have any ports to connect a monitor to. The BIOS of the HP Z440 does not like this. It refuses to boot if there is no way to output a video signal. This computer will run headless, but we have no other choice. We have to get a third video card, that we don't to intent to use ever, just to keep the BIOS happy.</p><p>This can be the most scrappy card that you can find, of course, but there is a requirement: we must make it fit on the main board. The Teslas are bulky and fill the two PCIe 3.0 x16 slots. The only slots left that can physically hold a card are one PCIe x4 slot and one PCIe x8 slot. See <a href=\"https://duropc.com/the-difference-between-pcie-x1-x4-x8-x16-and-x32\">this website</a> for some background on what those names mean. One cannot buy any x8 card, though, because often even when a GPU is advertised as x8, the actual connector on it might be just as wide as an x16. Electronically it is an x8, physically it is an x16. That won't work on this main board, we really need the small connector.</p><h3>Nvidia Tesla Cooling Fan Kit</h3><p>As said, the challenge is to find a fan shroud that fits in the case. After some searching, I found <a href=\"https://www.ebay.com/itm/313527520178\">this kit</a> on Ebay an bought two of them. They came delivered complete with a 40mm fan, and it all fits perfectly.</p><p>Be warned that they make an awful lot of noise. You don't want to keep a computer with these fans under your desk.</p><p>To keep an eye on the temperature, I whipped up this quick script and put it in a cron job. It periodically reads out the temperature on the GPUs and sends that to my Homeassistant server:</p><pre data-lang=\"bash\"><code data-lang=\"bash\"></code></pre><p>In Homeassistant I added a graph to the dashboard that displays the values over time:</p><p>As one can see, the fans were noisy, but not particularly effective. 90 degrees is far too hot. I searched the internet for a reasonable upper limit but could not find anything specific. The documentation on the Nvidia site mentions a temperature of 47 degrees Celsius. But, what they mean by that is the temperature of the ambient air surrounding the GPU, not the measured value on the chip. You know, the number that actually is reported. Thanks, Nvidia. That was helpful.</p><p>After some further searching and reading the opinions of my fellow internet citizens, my guess is that things will be fine, provided that we keep it in the lower 70s. But don't quote me on that.</p><p>My first attempt to remedy the situation was by setting a maximum to the power consumption of the GPUs. According to <a href=\"https://old.reddit.com/r/LocalLLaMA/comments/1anh0vi/nvidia_p40_save_50_power_for_only_15_less/\">this Reddit thread</a>, one can lower the power consumption of the cards by 45% at the cost of only 15% of the performance. I tried it and... did not notice any difference at all. I wasn't sure about the drop in performance, having only a couple of minutes of experience with this configuration at that point, but the temperature characteristics were definitely unchanged.</p><p>And then a light bulb flashed on in my head. You see, just before the GPU fans, there is a fan in the HP Z440 case. In the photo above, it is in the right corner, inside the black box. This is a fan that sucks air into the case, and I figured this would work in tandem with the GPU fans that blow air into the Teslas. But this case fan was not spinning at all, because the rest of the computer did not need any cooling. Looking into the BIOS, I found a setting for the  of the case fans. It ranged from 0 to 6 stars and was currently set to 0. Putting it at a higher setting did wonders for the temperature. It also made more noise.</p><p>I'll reluctantly admit that the third video card was helpful when adjusting the BIOS setting.</p><h3>MODDIY Main Power Adaptor Cable and Akasa Multifan Adaptor</h3><p>Fortunately, sometimes things just work. These two items were plug and play. The MODDIY adaptor cable connected the PSU to the main board and CPU power sockets.</p><p>I used the Akasa to power the GPU fans from a 4-pin Molex. It has the nice feature that it can power two fans with 12V and two with 5V. The latter obviously reduces the speed and thus the cooling power of the fan. But it also reduces noise. Fiddling a bit with this and the case fan setting, I found an acceptable tradeoff between noise and temperature. For now at least. Maybe I will need to revisit this in the summer.</p><p>Inference speed. I collected these numbers by running  with the  flag and asking it five times to write a story and averaging the result:</p><pre><code></code></pre><p>Performancewise,  is configured with:</p><pre><code></code></pre><p>All models have the default quantization that  will pull for you if you don't specify anything.</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table><p>Another important finding: Terry is by far the most popular name for a tortoise, followed by Turbo and Toby. Harry is a favorite for hares. All LLMs are loving alliteration.</p><p>Over the days I kept an eye on the power consumption of the workstation:</p><table><tbody></tbody></table><p>Note that these numbers were taken with the 140W power cap active.</p><p>As one can see, there is another tradeoff to be made. Keeping the model on the card improves latency, but consumes more power. My current setup is to have two models loaded, one for coding, the other for generic text processing, and keep them on the GPU for up to an hour after last use.</p><p>After all that, am I happy that I started this project? Yes, I think I am.</p><p>I spent a bit more money than planned, but I got what I wanted: a way of locally running medium-sized models, completely under my own control.</p><p>It was a good choice to start with the workstation I already owned, and see how far I could come with that. If I had started with a new machine from scratch, it definitely would have cost me more. It would have taken me much longer too, as there would have been many more options to choose from. I would also have been very tempted to follow the hype and buy the latest and greatest of everything. New and shiny toys are fun. But if I buy something new, I want it to last for years. Confidently predicting where AI will go in 5 years time is impossible right now, so having a cheaper machine, that will last at least some while, feels satisfactory to me.</p><p>I wish you good luck on your own AI journey. I'll report back if I find something new or interesting.</p>","contentLength":14191,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42999297"},{"title":"Smoke in the cabin of two 737 MAX caused by Load Reduction Device system [video]","url":"https://www.youtube.com/watch?v=swlVkYVSlIE","date":1739139121,"author":"dz0ny","guid":227,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42994590"}],"tags":["dev","hn"]}